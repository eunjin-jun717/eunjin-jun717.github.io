{"meta":{"title":"Eunjin's Blog","subtitle":"","description":"","author":"Eunjin Jun","url":"https://eunjin-jun717.github.io","root":"/"},"pages":[],"posts":[{"title":"[Deep Learning] Computer Vision 용어 정리 (2)","slug":"Computer Vision/Object Detection/DL_basic(2)","date":"2021-06-06T15:00:00.000Z","updated":"2021-06-07T01:34:53.013Z","comments":true,"path":"2021/06/07/Computer Vision/Object Detection/DL_basic(2)/","link":"","permalink":"https://eunjin-jun717.github.io/2021/06/07/Computer%20Vision/Object%20Detection/DL_basic(2)/","excerpt":"","text":"references 아래의 자료들을 참고하였습니다. 감사합니다. https://eehoeskrap.tistory.com/430 https://deep-learning-study.tistory.com/635 https://arxiv.org/abs/2002.05712 [DL] Computer Vision 기초 용어 정리 (2)computer vision 용어 정리 1편에 이어서 YOLOv4 논문 리뷰를 하는데 필요한 용어들을 공부하였습니다. 1. Batch 만약 데이터가 10만개 있을 경우, 데이터를 1개씩 입력 받아 총 10만 번의 연산을 진행하는 것 보다 한번에 큰 묶음으로 데이터를 입력 받아(Batch) n번의 연산을 진행하는 것이 빠르다. 즉, 느린 I/O를 통해 데이터를 읽는 횟수를 줄이고 CPU나 GPU로 순수 계산을 하는 비율을 높여 속도를 빠르게 할 수 있다는 뜻 1-1. mini-batch 우선 데이터를 하나씩 학습시키는 방법/ 전체를 학습시키는 방법의 장단점을 소개한다. 하나씩 학습: 장점- 신경망을 한번 학습시키는데 소요시간이 매우 짧다. but 데이터를 하나씩 입력받으면 계산 속도가 사실상 늦어짐 GPU의 병렬처리를 사용하지 않으니 그만큼 자원낭비됨 -&gt; loss function에서 최적의 파라미터를 설정하는데 힘들다. 전체데이터 입력: 한번에 여러개의 데이터에 대해 신경망을 학습시킬 수 있으니 오차를 줄일 수 있는 cost funtion의 최적 param를 빠르게 알아낼 수 있음 but 신경망 한 번 학습시키는데 소요시간 매우 길다. 이 모든 장점들을 섞은 것이 mini-batch이다. mini-batch는 SGD(Stochastic Gradient Descent:확률적 경사 하강법)와 Batch를 섞은 것으로 전체 데이터를 N등분해서 각각의 학습 데이터를 배치 방식으로 학습시킨다. 따라서 최대한 신경망을 한 번 학습시키는데(iteration) 걸리는 시간을 줄이면서 전체 데이터를 반영할 수 있게 되며 효율적으로 CPU와 GPU를 활용함 1-2. small batch, large batch 비교 Batch size가 작을수록 generalization performance 측면에서 긍정적 영향을 끼친다. Batch size가 클수록 gradient는 정확해지지만 한 반복에 대해서는 계산량이 늘어나게 된다. 그러나 한 반복에서 각 example에 대한 gradient는 병렬계산이 가능하므로, 큰 batch를 사용하면 멀티 GPU 등 병렬계산의 활용도를 높여 전체 학습시간을 단축할 수 있다. small batch size의 장점 Generalization performance가 좋아짐 Training stability가 좋아짐. 즉, 보다 넓은 범위의 learning rate로 학습이 가능하다. 1-3. SGD(확률적 경사 하강법) SGD는 신경망을 학습할 때 마다 가중치를 갱신함. 학습할 때마다 가중치를 갱신하기 때문에 학습할 때마다 신경망의 성능이 들쑥날쑥하 변하면서 정답에 가까워지기 때문에 ‘무작위적’으로 보이기 때문 1-4. Normalization(정규화) 정규화하는 이유: 학습을 더 빨리 하기위해서 or local optimum 문제에 빠지는 가능성을 줄이기 위해 사용 위의 그림을 보면 왼쪽에서 global optimum을 찾지 못하고 local optimum에 머물러있는 문제가 생긴다. 해결하기 위해 정규화 하면 local optimum에 빠질 수 있는 가능성을 낮춘다. 1-5. Internal Covariance Shift 학습에서 불안정화가 일어나는 이유라한다. 네트워크의 각 레이어나 activation마다 입력값의 분산이 달라지는 현상을 뜻한다. covariate shift: 이전 레이어의 파라미터 변화로 인해 현재 레이어의 입력분포가 바뀌는 현상 internal covariate shift: 레이어를 통과할 때 마다 covariate shift가 일어나면서 입력의 분포가 약간씩 변하는 현상 1-6. Batch Normalization 배치 정규화는 평균과 분산을 조정하는 과정이 별도의 과정으로 떼어진 것이 아니라, 신경망 안에 포함되어 학습 시 평균과 분산을 조정하는 과정 역시 같이 조절됨 즉, 각 레이어마다 정규화하는 레이어를 두어, 변형된 분포가 나오지 않도록 조절하게 하는 것 즉, 간단하게 말하면 미니배치의 평균과 분산을 이용해 정규화 한 뒤, scale 및 shift를 감마값, 베타값을 통해 실행한다. 이때, 감마,베타는 학습 가능한 변수 역전파를 통해 학습 1-7. CNN구조에서의 배치 정규화 conv 레이어에서 활성화 함수가 입력되기 전에 wX+b 로 가중치가 적용되었을 때, b의 역할은 베타가 완전히 대신할 수 있기 때문에 b를 삭제함 결국 배치 정규화를 사용 시 장점은 기존 방법에서 learning rate를 높게 잡으면 gradient가 vanish/explode거나 local minima에 빠지는 경향이 있는데 이는 scale 때문이었으며 배치 정규화를 사용할 경우 propagation시 파라미터의 scale에 영향을 받지 않게 되기 때문에 learning rate를 높게 설정할 수 있다. regularization 효과가 있기 때문에 dropout 등의 기법 쓰지 않아도됨(효과는 같기 때문) 2. CBN(Cross-iteration Batch Normalization) CBN은 small batch size에서 발생하는 BN의 문제점을 개선하기 위해 이전 iteration에서 사용한 sample 데이터로 평균과 분산을 계산한다. CBN은 테일러 시리즈를 사용해 이전 가중치와 현재 가중치의 차이만큼 compensation하여 근사화 한다. 매 반복마다 변화하는 가중치 값이 매우 작다고 가정하기 때문에 테일러 시리즈 사용 가능 BN의 문제점은 미니배치 사이즈가 작은 경우에 발생, 적은 sample로 평균, 분산을 추정하므로 추정된 값은 전체 training set과 동일하지 않다. 따라서 많은 노이즈를 포함하게된다. 그렇다면 large batch size를 사용하면 BN은 문제가 될까? 되지는 않지만, 많은 연산량과 메모리 점유율이 필요한 OD, segmentation task에서는 GPU한계 때문에 mini-batch-size를 사용할 수 밖에 없다. 배치 사이즈가 4,2,1 인경우에 모델 정확도가 급격히 떨어짐","categories":[],"tags":[{"name":"DL","slug":"DL","permalink":"https://eunjin-jun717.github.io/tags/DL/"}]},{"title":"[Deep Learning] Computer Vision 용어 정리 (1)","slug":"Computer Vision/Object Detection/DL_basic(1)","date":"2021-06-05T15:00:00.000Z","updated":"2021-06-07T01:08:19.371Z","comments":true,"path":"2021/06/06/Computer Vision/Object Detection/DL_basic(1)/","link":"","permalink":"https://eunjin-jun717.github.io/2021/06/06/Computer%20Vision/Object%20Detection/DL_basic(1)/","excerpt":"","text":"references 글을 쓰는데 도움을 준 자료들 입니다. 감사합니다. https://velog.io/@hewas1230/ObjectDetection-Architecture https://eehoeskrap.tistory.com/186 https://eehoeskrap.tistory.com/300 [DL] Computer Vision 기초 용어 정리 (1)YOLOv4 논문 리뷰를 하는데 필요한 용어들을 정리해보았습니다. 이것저것 용어들을 찾아보며 공부를 하다보니 두서없이 정리되었네요😂 1. 딥러닝 네트워크 판단1-1. mAP Mean Average Precision -&gt; AP의 평균이며 CNN을 평가할 때 사용하는 지표 1-2. FPS Frame Per Second -&gt; 1초당 몇개의 이미지 Frame이 보여지는지 즉, 1초당 보여지는 frame 수가 많을 수록 영상은 끊김없이 부드럽게 보여진다. 참고로 인간의 눈은 보통 25-30 FPS를 보면 영상이 끊기지 않는다고 판단을 한다고 한다. -&gt; 즉, 실시간성이 있다라고 판단 그러나 예를 들어 우리가 40 FPS의 영상에 익숙해져 있다가 25-30 FPS를 보면 연속적이지 않은데? 라는 생각을 가질 수도 있다. 2. Object Detection Object Detection이란 객체 감지를 뜻하는 용어이며, 과정은 아래와 같다. object regognition(인식) -&gt; object classification(분류) -&gt; object Localization(좌표찍기) 이때, Object classification + object localization 을 “object detection”이라 한다. object detection은 한 이미지 내에서 진행되는 것이다. 예를 들어 아래의 사진을 보게되면 car, truck, umbrella, person 등 여러 개의 object가 있는 것을 확인할 수 있다. 이때 한 이미지 내에서 서로 다른 객체들을 분류하고 있는데 이것이 바로 object detection 이다. object classification이라 생각할 수도 있지만 그것과는 조금 다른 개념이다. object detection의 구조인 backbone-neck-head 구조를 보게 되면 알게 될 것이다. 알아보기 전에 pre-training과 fine tuning의 개념을 알아보겠다. 2-1. Pre-training 사전훈련, 선행학습, 전처리과정 이라고도 한다. Multi layered perceptron(MLP)에서 weight와 bias를 잘 초기화 시키는 방법이다. 이러한 pre-training을 통해서 효과적으로 layer를 쌓아서 여러 개의 hidden layer도 효율적으로 훈련할 수 있다 비지도학습이 가능하기 때문에(물론 이러한 가중치, 편향 초기화가 끝나면 label된 데이터로 지도학습해야한다-&gt;fine tuning) 레이블되지 않은 큰 데이터를 넣어 훈련시킬 수 있다는 장점이 있다. 또한, Drop-out, mini-batch 방식을 사용해서 pre-training 생략하기도 한다. 2-2. Fine Tuning 기존에 학습되어져 있는 모델을 기반으로 아키텍처를 새로운 목적(나의 이미지 데이터에 맞게) 변형을 하고 이미 학습된 모델 weights로부터 학습을 업데이트하는 방법을 말한다. 딥러닝에서는 이미 존재하는 모델에 추가 데이터를 투입하여 파라미터를 업데이트하는 것 예를 들어, cat과 dog 분류기를 만드는데 다른 데이터로 학습된 모델(VGG16, ResNet)을 가져다 쓰는 경우가 있다. VGG16의 모델 경우 1000개의 카테고리를 학습시켰기 때문에 고양이, 개, 2개의 카테고리만 필요한 우리 문제를 해결하는데 모든 레이어를 그대로 쓸 수는 없다. 따라서 가장 쉽게 이용하려면 내 데이터를 해당모델로 예측하여 보틀텍 피쳐만 뽑아내고, 이를 이용하여 fully-connected-layer만 학습시켜서 사용하는 방법을 취하게 한다. 하지만 이 경우에는 fine-tuning이라고 부르지 않는다. Feature를 추출해내는 layer의 파라미터를 업데이트하지 않기 때문이다. 따라서 fine-tuning을 했다고 말하려면 기존에 학습된 레이어에 내 데이터를 추가로 학습시켜 파라미터를 업데이트 했다고 해야한다. 주의할 점은 완전히 랜덤한 초기 파라미터를 쓴다거나 가장 아래쪽의 레이어(일반적으로 피쳐를 학습한 덜 추상화된 레이어)의 파라미터를 학습해버리면 과적합이 일어나거나 전체 파라미터가 망가지는 문제가 생긴다. 2-3. object detection architecture Backbone 이것들은 ImageNet과 같은 ‘image classification datasets’에 pre-training된 모델이다. Detection dataset에 fine-tune된다. 깊어질수록 higher semantics와 함께 다양한 level의 feature를 생성하는 구조는 OD 네트워크의 후반부에 유용하다. Backbone의 가장 밑에 부분은 input layer이고, 가운데 두개(실제로는 적거나 더 많음)는 conv layer이고, 가장 위의 부분은 feature map이다. 결론적으로 backbone 파트는 일반적인 CNN cell을 거쳐 평범하게 feature map을 생성하는 파트가 된 것이다. 게다가 image를 다루게 되니 이러한 상황에 최적화된 CNN 구조인 ImageNet을 주로, ResNet, VGG를 사용하는 경우가 많은 것이다. Neck 이름 그대로 Backbone과 Head를 이어주는 연결부이다. Neck에서는 backbone의 다른 stages에서 서로 다른 feature maps를 추출하게 되는데 이때 FPN, PANet, BI-FPN등이 사용될 수 있다. 이때 FPN이란 Feature Pyramid Network인데 위의 설명을 다시 봅시다. Head Bounding box들의 classification이나 regression 과 같은 검출이 이루어지는 실질적인 부분이다. Output은 x,y,h,w와 k classes +1 의 확률(1은 배경을 위한 것) 형태이다. 2-4. FPN(Feature Pyramid Network) Top-down 방식과 측면 연결을 통해서 기존의 convolutional network를 증강시킨다. 이는 network가 single resolution input image로부터 풍부하고 다양한 scale의 feature pyramid를 구성할 수 있도록 한다. 각 추출된 결과들인 low-resolution및 high-resolution들을 묶는 방식이다. 각 레벨에서 독립적으로 특징을 추출하여 객체를 탐지하는데 상위 레벨의 이미 계산된 특징을 재사용하므로 멀티 스케일 특징들을 효율적으로 사용할 수 있다. CNN 자체가 레이어를 거치면서 피라미드를 만들고 forward를 거치면서 더 많은 의미(semantic)을 가지게 된다. 각 레이어마다 예측과정을 넣어서 scale변화에 더 강한 모델이 되는 것이다. FPN은 skip connection, top-down, cnn forward에서 생성되는 피라미드 구조를 합친 형태이다. forward에서 추출된 의미정보들은 top-down 과정에서 up sampling하여 해상도를 올리고 forward에서 손실된 지역적인 정보들은 skip connection으로 보충해서 스케일 변화에 강인하게 되는 것이다. FPN 모델을 입력으로 임의의 크기의 단일 스케일 영상을 다루며, 전체적으로 convolutional한 방식으로 비례된 크기의 feature map을 다중 레벨로 출력하게 된다. 이 프로세스는 backbone convolutional architecture와는 독립적이며 ResNet을 사용하여 결과를 보여주게 된다. 결국 피라미드 형태를 통해 검출 객체에 다양한 스케일 변화를 준다 라는 아이디어이다. 2-5. NMS(Non-max Suppressed) 같은 물체를 한 번이 아닌 여러 번 검출 될 수 있는데 이때 NMS 알고리즘은 각 물체를 한번만 감지하게 한다. NMS란 확률의 최댓값을 도출하고 최댓값 아닌 것을 억제하는 것이다. 2-6. Letter Box Image 보통 이미지는 4:3 16:9 사이즈이다. 그러나 네트워크 input은 608*608 사각형이다. 그러므로 원본을 그대로 넣으면 찌그러질 것이다. 따라서 여분을 매꿔주는 letter box image 기법을 사용한다.","categories":[],"tags":[{"name":"DL","slug":"DL","permalink":"https://eunjin-jun717.github.io/tags/DL/"}]},{"title":"[Deep Learning] CNN(Convolution Neural Network)","slug":"Computer Vision/Object Detection/cnn","date":"2021-06-04T15:00:00.000Z","updated":"2021-06-07T01:52:52.665Z","comments":true,"path":"2021/06/05/Computer Vision/Object Detection/cnn/","link":"","permalink":"https://eunjin-jun717.github.io/2021/06/05/Computer%20Vision/Object%20Detection/cnn/","excerpt":"","text":"references 아래의 자료를 참고하였습니다. 감사합니다. https://22-22.tistory.com/26 [DL] CNN(Convolution Neural Network) CNN이란 합성곱(convolution)을 이용해 Feature를 추출하는 layer이다. Conv 특성상 근처의 데이터끼리만 묶어서 처리한다는 특징이 있다. 1. 합성곱 계층 이미지의 경우(w,h,color)형상으로 되어있으므로, 3차원 입력 받아 3차원을 출력한다. 합성곱 계층의 입출력 데이터를 feature map이라한다. 입력 데이터는 input feature map, 출력 데이터는 output feature map이라 한다. 1-1. Conv 연산 필터의 윈도우를 이동해가며, 각각의 범위 내 데이터만 적용된다. 단일 곱셈-누산(FMA): 입력과 필터에 각각 대응하는 원소끼리 곱한 후 총합을 구한다 위 과정을 모든 장소에서 수행 필터를 적용한 원소에 고정값(bias)를 더한다. input feature map이 filter와 패턴이 일치할수록 output feature map의 값 또한 크다. 즉, 입력값이 filter와 얼마나 일치하는지 나타낸다. 이미지나 음성 등에서 특정 패턴을 추출할 때 많이 사용한다. 계층이 얕으면 CNN은 둥근 모양, 날카로운 모양 등 저수준의 패턴 정보를 추출한다. 계층이 깊으면 추상적인 정보를 추출한다. 1-2. Filter 특징을 가진 값으로 입력 데이터와 “동일 차원”을 가진다. = kernel이라고 한다. 예를 들면, 필터는 이미지에 있는 특징 등을 나타낸다고 볼 수 있다. Filter를 여러 개 사용하여 한 번에 여러 feature를 추출할 수 있다. 2. Padding 패딩이란, 입력 데이터 주위에 0을 채우는 것이다. CNN의 출력 크기를 조절하기 위해 사용 Padding이 없으면 CNN layer를 통과할 때 마다 가로,세로 크기가 감소하지만, padding을 적용하면 conv을 수행해도 데이터의 크기가 동일하다. 3. Stride 필터를 적용하는 위치의 간격을 의미한다. 예를 들어 필터가 오른쪽, 아래쪽으로 한 칸 씩 움직이면 stride는 1이다. Padding, stride의 출력크기는 아래와 같다. 4. 3차원 데이터 이미지 데이터는 3차원 즉, h,w,channel이 3개의 차원으로 나눠서 생각할 수 있다. h,w크기의 데이터에 대해서 각 channel 별로 각각 다른 filter를 이용해 conv를 수행한다. 5. Pooling 데이터의 크기를 줄이는 연산을 의미한다. 즉, 데이터의 여러 값을 묶어서 하나의 데이터로 줄이는 과정 pooling에는 max,avg,global avg pooling 등이 있다. max pooling: 영역 내 최댓값을 취해 pooling하는 방법\\ Average pooling: 영역 내 값을 평균내서 pooling하는 것\\ global avg pooling: feature map 전체에 대해서 avg pooling을 취하는 방법 즉, 영역 크기가 H*W로 feature map의 크기와 동일하다. 즉, HWC 크기를 가지는 feature map을 11C 크기의 하나의 뉴런으로 mapping하는 것이다. 6. Epoch - 에폭 인공신경망에서 전체 데이터 셋에 대해 한 번 학습을 완료한 상태를 의미 순방향과 역방향을 모두 완료하면 한 번의 epoch이 완료된다. 1 epoch은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수이다. 예를 들어, 훈련 데이터 10000개를 100개의 미니배치로 학습할 경우, 최적의 가중치를 구하기 위해 SGD를 100번 반복해야 모든 훈련 데이터를 학습에 활용하기 때문에 100회가 1 epoch이 된다. 손실함수는 오차제곱합(Sum of Squares for error), 교차 엔트로피 오차(Cross Entropy Error)가 있으며, 오차제곱합은 회귀에 주로 사용, 교차 엔트로피오차는 분류에 주로 사용함 7. CNN layer의 응용7-1. FC(Fully Connected) Conv와 max pooling을 반복하면 반복할수록 추상적인 특징이 남는다. 제일 마지막 feature map을 fully connected layer에 연결하는 경우가 많다. 7-2. One-by-One(1*1) conv Channel 수를 조절하기 위한 conv이다. 높은 차원을 낮은 차원으로 축소하는 dimension reduction에 사용한다. Dimension reduction은 아래와 같은 특징이 있다. 차원 수를 줄여 불필요한 feature를 줄임/ 파라미터를 줄여 연산량 줄임/ 비선형성 증가","categories":[],"tags":[{"name":"DL","slug":"DL","permalink":"https://eunjin-jun717.github.io/tags/DL/"}]},{"title":"[Deep Learning] YOLO, YOLOv2 and YOLOv3 비교","slug":"Computer Vision/Object Detection/yolo","date":"2021-05-31T15:00:00.000Z","updated":"2021-06-01T08:01:57.596Z","comments":true,"path":"2021/06/01/Computer Vision/Object Detection/yolo/","link":"","permalink":"https://eunjin-jun717.github.io/2021/06/01/Computer%20Vision/Object%20Detection/yolo/","excerpt":"","text":"references https://amrokamal-47691.medium.com/yolo-yolov2-and-yolov3-all-you-want-to-know-7e3e92dc4899 YOLO, YOLOv2 and YOLOv3 Object Detection : Deep Learning의 Computer vision 분야에서 가장 주목받는 주제 중 하나이다. EX) RCNN, RetinaNet and YOLO(you only look once) 1. Object Detection 이미지 혹은 비디오 스트림이 주어지면, object detection 모델은 객체들의 모임 중에서 어떤 것이 존재할 수 있는지 식별하며 이미지 안에서 그들의 위치들에 대해 정보를 제공할 수 있다. Object Detection은 single object를 분류하고 이미지에서 그 object의 위치를 결정하는 classification with localization과는 다르다. 1-1. IOU(Intersect Over Union) IOU는 교차 영역을 두 box의 전체 영역으로 나누어 계산할 수 있다. IOU는 0과 1사이 아래의 그림에서 왼쪽 사진은 교차영역이 오른쪽 사진보다 작다.오른쪽 사진은 거의 완벽하게 교차하였기 때문에 IOU는 1에 가까워지는 것이다. 1-2. Precision(정밀도) Precision(정밀도)는 진양성을 양성예측으로 나눈 것으로 정의할 수 있다. 예를 들어 20 개의 이미지가 있고 20개의 이미지에서 120대의 자동차가 있다는 것을 알고 있다고 가정 이미지를 모델에 입력하고 -&gt; 100대의 자동차를 감지했다고 가정 20개가 잘못된 경우 정밀도는 80/100 = 0.8 이다. 만약 예측된 box와 ground truth box 사이의 IOU가 임계값(0.5,0.75,..)보다 작으면 부정확한 예측이 될 수 있다. 1-3. Recall(재현율) 위의 정밀도를 구하는 예제를 봤을 때, 자동차의 total 갯수를 고려하지 않았다. 그래서 120대 대신에 1000대의 자동차들을 사용한다고 가정하고, 그 모델 output이 80대가 있는 100개의 box가 맞다면 Precision은 다시 0.8이 된다. 그렇다면 이 문제를 해결하기 위해 우리는 재현율이라는 개념을 사용해야 한다. 아래의 식과 같으며 Recall은 진양성에서 실제 양성으로 나눈 것으로 정의할 수 있다. recall = 80/120 = 0.667 재현율이 데이터에서 object들을 detect하는데 좋다는 것을 확인하였다. 1-4. Average Precision and Mean Average Precision(mAP) Average Precision을 간략하게 정의하면 Precision-Recall Curve의 아래에 있는 영역을 의미한다. AP는 Precision과 Recall을 결합한 것이다. AP는 0과 1사이에 존재하며, 높을 수록 더 좋은 것이다. AP=1을 얻기위해서는 Precision과 Recall이 1로 같아야한다. mAP는 모든 class에 대해 계산된 AP의 평균을 의미한다. 2. YOLO YOLO란?? 많은 object detection은 이미지의 모든 object를 detect하기 위해 이미지를 한 번 이상 통과해야하거나 object를 detect하기 위해 2-step을 거쳐야 한다. YOLO는 이런 과정을 거칠 필요가 없다 모든 object를 detect하기 위해 이미지를 한 번만 보면 된다. 즉, YOLO(You Only Look Once) 해석 그대로 받아드리면 된다. 실제로 YOLO가 매우 빠른 모델인 이유이다. 3.YOLO (YOLOv1) YOLO는 이미지를 S*S 그리드로 나눈다. EX) 아래의 이미지는 5*5로 나누었으며, 만약에 Grid Cell안에 object가 위치해 있을 때, 그 grid cell은 object를 detect한다. YOLO는 7*7 = 49 grid cell 각각에 대해 동시에 Classification과 Localization 문제를 run한다. Classification과 Localization network는 오직 하나의 object만 detect할 수 있기 때문에 그 말은 즉, 어떤 grid cell은 오직 하나의 object만 detect할 수 있다. 따라서 아래와 같은 문제들을 직면하게 된다. 7*7 grid 를 사용하기 때문에 어느 grid는 하나의 object만 detect할 수 있고, 그 최대 object의 갯수는 49개만 가능하다는 것이다. 만약 grid cell이 하나의 object보다 더 많은 object를 포함하게 된다면?그 모델은 모든 object에 대하여 detect를 할 수 없을 것이다.따라서 Close Object Detection는 YOLO가 겪고 있는 문제이다. Object가 하나의 grid의 면적을 넘어 차지하고 있다면(위의 택시처럼), 그 모델은 한 번이 아니라 다른 grid에서도 taxi라 detect 할 수 있다.따라서 이 문제는 “Non-Max Suppresion”를 사용하여 해결한다. 49개 cell 모두 동시에 detect되어지며, 그것이 바로 YOLO가 매우 빠른 모델인 이유이다. 7*7 gric cell의 각각은 B Bounding boxe들을 예측하고(YOLO chose B =2), 각각의 박스, 모델의 output은 confidence score이다. confidence score 상자가 모델을 포함하는 모델을 얼마나 확신하는지를 반영한다. confidence score을 사용하여 모델이 배경을 감지하지 못하도록 막을 수 있으며 cell에 object가 없으면 confidence score은 0이 되어야한다. 만약 그렇지 않으면, confidence scroe가 예측된 box와 ground truth 사이의 union을 통한 IOU가 같기를 원한다. 왜 C= IOU 인가요? ground truth box는 손으로 그려졌기 때문에 ground truth box 안에 object가 있다는 것을 100% 확신한다. 따라서 truth box안에 있는 높은 IOU를 가진 box는 또한 같은 object를 둘러싸고, IOU가 높을 수록 predict box 내부에 object가 있다는 가능성이 높아진다. 7*7 =49 개의 cell들이 있고 각 cell들에 대해 2개의 상자(total 98개)를 예측함 box의 대부분은 매우 낮은 신뢰도(confidence)를 가지므로 제거할 수 있다. 추가로 confidence score(C)에 모델은 4개의 숫자를 출력한다. ((x,y),w,h)는 predeict bounding box의 위치를 나타낸다. (x,y)는 grid cell의 bound를 기준으로 box의 중심을 나타낸다. width와 height은 전체 이미지를 기준으로 예측된다.\\ YOLO는 20개의 다른 object들의 class들을 detect하도록 훈련되어 진다. 어떤 grid cell에서든지, 그 모델은 20개의 conditional class probabilities를 출력한다. 반면에 각각의 grid cell은 우리에게 2개의 bounding box들 사이의 choice를 주며, 우리는 오직 하나의 class probalility vector를 가질 수 있다. 우리는 가장 낮은 신뢰도를 가진 box를 제거 할 수 있다. 3-1. Output Shape 예측은 SxSx(Bx5 + classes) tensor로 암호화되어진다. EX) S=7, B=2, Classes = 20 총 7x7x30 tensor 이다. 3-2. Network Design YOLO는 하나의 Convolutional network를 사용하여 여러개의 bounding box들과 그 box들에 대한 class probabilities를 예측한다. 이 네트워크는 이미지 분류 모델인 GoogleNet에 의해 영향을 받았지만, GoogleNet에 사용되어진 inception 모듈 대신에 YOLO는 단순하게 3x3 convolutional layer(컨볼루션 레이어)와 1x1 reduction layer(축소 레이어)을 사용한다. 24개의 convolution layer와 2개의 완전히 연결된 layer들이 있다. 위에서 언급한 대로, 네트워크의 최종 output은 7x7x30 tensor로 예측한다. 3-3. Loss Function YOLO는 손실함수로서 SSE(Sum-squared Error)를 사용한다. optimize하게에 쉽기 때문이다. 위의 식에서 첫번째, 두번째 식은 localization loss이고,3번째, 4번째 식은 confidence loss이며마지막 식은 classification loss를 나타낸다. 3-4. Training 먼저, ImageNet 1000-class 대회 데이터 셋에서 분류에 대한 네트워크의 convolutional layer들을 사전훈련 시킨다. 사전훈련을 위해 첫 20개 convolutional layer들을 사용했고, 그 다음에 평균 pooling layer와 224x224의 input 사이즈를 가진 1x1000 인 fully connected layer를 사용했다. 이 network는 top-5의 정확도인 88%에 도달했다. 그 다음 1x1000 fully connected layer와 추가적으로 4개의 convolutional layer 그리고 2개의 fully connected layer를 랜덤하게 초기화한 가중치를 더했다. 그리고 224x224 에서 448x448 로부터 네트워크의 input resolution을 증가했다. 그 후 detect를 위해 모델을 훈련시켰다. 3-5. Non-Maximal Suppression YOLO는 7x7 grid를 사용하므로 만약 object가 하나의 grid 보다 더 차지한다면 이 object는 다른 grid에서도 detect될 수 있다. 한번에 하나만 detect되어야 하기 때문에, 예를 들어 아래의 이미지에서 택시의 yellow box가 3개나 dectect 되었다. 실제로 1개만 detect되어야 하는데 세 번 이상 detect 된 격이다. 그렇다면 이 yellow box 들 중 하나만 선택하는 방법은?\\ 각 class에 대해 수행하자\\ C &lt; C 임계 값(ex.C &lt; 0.5)를 사용하여 모든 box를 제거한다.\\ 가장 높은 신뢰도 C에서 시작하여 예측을 정렬한다.\\ C가 가장 높은 상자를 선택하고 예측을 출력한다.\\ 이전 단계의 box와 함께 IOU&gt;IOU-treshold 가 있는 box들을 모두 제거한다.\\ 모든 나머지 예측을 확인할 때 까지 4단계부터 다시 시작한다. Non-Maximal Suppresion 에서 2-3%를 추가한다. 3-6. Fast YOLO and YOLO VGG-16 Fast YOLO는 YOLO의 빠른 버전이다. fast YOLO는 24개 대신에 9개의 convolutional layer을 사용한다. YOLO보다 더 빠르지만 더 낮은 mAP를 가진다. YOLO VGG-16은 오리지널 YOLO 네트워크 대신에 그것의 backbone을 VGG-16을 사용한다. 좀 더 정확하지만 실시간보다 좀 더 느리다. 3-7. Limitations of YOLO 각각의 grid cell은 오직 2개의 box들을 사용하여 예측하고 하나의 class만 예측할 수 있기 때문에 YOLO가 예측할 수있는 nearby object의 수를 제한한다. 특히 새의 무리와 같이 그룹을 나타내는 작은 object의 경우에 그러하다. YOLO는 7x7 = 49개의 물체만 감지할 수 있다. 비교적 높은 localization 오류 4. YOLOv2 YOLO ver1은 상당히 많은 localization error들을 만든다. 게다가, YOLO는 비교적 낮은 재현율을 보인다. 그러므로 YOLO의 두번째 버전은 classification accuracy를 유지하면서 localization(위치)과 재현율을 향상시키는 것이 목표이다. 아래의 방법이 YOLOv2의 성능을 좋게 만드는 idea이다. BatchNormalization: YOLO의 모든 컨볼루션 레이어들에 batch normalizaition을 더함으로써 mAP가 2% 향상되었다. High Resolution Classifier: YOLO ver1은 아래와 같이 훈련되어졌다. 224x224에 classifier network를 훈련시켰다. detection을 위해 resolution(해상도)를 448로 증가시켰다. 이 뜻은 detection으로 전환할 때 네트워크가 동시에 학습 object detection으로 전환하고, 새로운 input resolution으로 조정한다. YOLOv2 를 위해 처음에 224x224 인 이미지에 모델을 훈련시키는 동안, full 448x448 이미지에 classification network를 fine tune한다 (dectection을 하기 위한 훈련 전에 ImageNet의 10 epochs 에 대해). 더 높은 resolution input에 더 나은 동작을 하도록 그것의 filter들을 조절하기 위한 network 시간을 제공한다. high-resolution classification network는 거의 4% 증가된 mAP를 제공한다. Convolutional With Anchor Boxes(한 cell당 여러개의 object 예측) YOLOv1은 object의 중심을 포함하는 grid cell에 object를 할당하려한다. 이 아이디어를 사용하여 위의 이미지에 빨간 셀에는 man과 necktie를 감지해야한다. 그러나 grid cell은 오직 하나의 cell에 하나의 object만 detect할 수 있다. 이것을 해결하기 위해 저자는 k bounding box를 이용해 하나의 object보다 더 많은 object를 detect하도록 했다. &gt; k bounding box를 예측하기 위해 YOLOv2는 Anchor boxes의 아이디어를 사용했다. 4-1. Anchor Box 그렇다면 Anchor Box는 무엇일까? YOLO는 컨볼루션 feature extractor의 위에서 완전히 연결된 layer들을 사용해서 바로 bounding box들의 좌표를 예측한다. 좌표를 바로 예측하는 대신에 Fater R-CNN인 다른 object detection 모델은 내가 직접 고른 anchor box들을 사용해서 bounding box를 예측한다. 우리는 이미지를 기준으로 박스의 예측하는 것 대신에 bounding box를 기준으로 bounding box를 예측할 수 있다. 이 아이디어를 사용해서 network가 더 쉽게 학습할 수 있다. 오직 컨볼루션 레이어들(완전히 연결된 layer들 제외)들인 Faster R-CNN은 offset과 anchor box들의 신뢰도를 예측한다. 아래의 이미지는 Grid cell(빨간 박스)과 5 anchor box들(노란 박스)과 함께 다른 모양을 가진다. YOLOv2는 손으로 직접 k anchor box를 고르는 것 대신에 anchor box들의 아이디어를 사용하려 한다. detection을 훈련하려는 네트워트를 더 쉽게 만들기 위해 가장 best인 anchor box를 찾으려 한다. 아래의 이미지는 5개의 red box들은 평균 dimension들과 VOC 2007 데이터셋의 object들의 위치를 나타낸다. 왜 5개의 박스들을 가지는지 궁금할 것이다! k-means clustering을 수행하는데 k개의 다양한 값에 대해 훈련 데이터셋의 bounding box에서 실행한다. 가장 가까운 중심으로 평균 IOU를 plot하지만, 유클리디안 거리 방식을 사용하는 대신에 bounding box와 중심 사이의 IOU를 사용한다. 모델의 복잡성과 높은 재현율 사이의 가장 좋은 트레이드오프로써 k=5로 선택된 것이다! YOLOv2는 grid cell의 위치를 기준으로 location coordinate를 예측한다. 이 예측은 ground truth를 0과 1 사이로 제한한다. 네트워크는 각 cell 당 5개의 bounding box들을 예측한다. 즉, 각 bounding box들에 대해 5개의 좌표를 예측한다는 것이다. tx,ty,tw,th 그리고 to 만약 그 cell이 왼쪽 위쪽 코너로 부터 (cx,cy)만큼 오프셋되고, 그리고 bounding box prior(anchor box)에 width, height pw,ph를 가진다면 아래의 예측과 일치한다. 예를 들어, 만약 2개의 anchor box를 gird cell(2,2)에 사용하는데 아래의 이미지에서 2개의 box들(blue, yellow)를 output할 것이다. cell에 해당하는 2 anchor box들을 검정 점 박스가 대표한다. 위의 그림에서 파란 박스만 고려해보자, YOLO ver1에서는 오직 하나의 grid cell로서 예측된 파란 박스를 할당하는 대신에! YOLOv2는 그 grid cell 뿐만 아니라 anchor box들 중 하나까지 할당한다. 그리고 그것은 ground truth box와 함께 가장 높은 IOU를 가진 것이 될 것이다. YOLOv2는 grid와 anchor box에 파란 박스를 할당하기 위해 위의 방정식을 사용한다. 4-2. Network Architecture1) Darknet-19 복잡성과 정확도 문제를 해결하기 위해 저자는 YOLOv2의 backbone으로 사용할 Darknet-19라는 새로운 classification model을 제한했다. Darknet-19는 19개의 컨볼루션 layer들과 5개의 max-pooling 레이어들을 가진다. ImageNet의 top-5의 정확도인 91.2%를 달성했고 그것은 VGG(90%)보다 나으며 YOLO network(88%)보다 높다. 2) Output Shape YOLOv2의 output shape은 13x13x(k.(1+4+20)) 이며 k는 anchor box들의 수이다. 20은 class들의 수 이다. k=5에 대한 output shape은 13x13x125가 될 것이다. 3) Training 그 모델은 classification에 대한 훈련이 처음되고 그 다음 detection에 대한 룬련이 되어진다. (1) Classification: 160 epoch과 224x224인 input shape의 ImageNet 1000 클래스 분류 데이터셋에서 Darknet-19 네트워크를 훈련했고, 그 후 10 epoch동안 448x448인 큰 input 크기로 네트워크를 미세조정하게 된다. 이것은 76.5%의 top-1정확도와 93.3%의 top-5 정확도를 제공한다.(2) Detection: classification 에 대한 훈련 후에 Darknet-19로 부터 마지막 컨볼루션 레이어를 제거한다. 그리고 대신에 3x3 컨볼루션 레이어와 output의 수인 1x1 컨볼루션 레이어를 더한다.(detection에 필요한 13x13x125) 또한, passthrough 레이어는 이전의 레이어로부터 모델에 더 좋은 grain feature들을 사용하기 위하여 추가된다. 그런 다음 detection 데이터셋(VOC, COCO datasets)에 160번의 epoch을 한 network를 훈련시킨다. 4) Multi-scale Training YOLOv2가 서로 다른 크기의 이미지에서 수행되도록 서로 다른 input size에 대해 모델을 훈련시킨다. input size를 수정하는 대신 몇 번의 반복마다 네트워크를 변경한다. 매 10 batches 후에 네트워크는 랜덤하게 새로운 차원 사이즈를 고르는데 dimensions set(320,352,384,…,608)로 부터 구한다. 해당 dimension에 맞게 네트워크 size를 조정하고 훈련을 계속한다. 동일한 네트워크가 다른 resolution에서 물체를 예측할 수있음을 의미한다. 5) Comparision to Other Dectection Systems YOLOv2는 최신식이며 다른 detection 시스템 보다 더 빠르다. 게다가, 다양한 이미지 사이즈에 동작될 수 있다. 속도와 정확도 사이의 부드러운 트레이드 오프를 제공한다.위의 결과: PASCAL VOC2012 테스트 detection위의 결과: COCO test-dev2015 4-3. YOLO9000 20개 class 이상의 detect가 필요할 경우, YOLO9000을 사용하면 된다. 실시간 프레임워크이며 9000개 object 카테고리보다 더 많이 감지하기 위해 사용한다. 또한 detection과 classification을 함께 최적화한다. YOLOv2는 classfication에 대한 훈련 후에 detection 훈련을 한다. 이러한 이유는 하나의 object를 포함하는 classfication에 대한 dataset은 dectection에 대한 데이터셋과 다르다. YOLOv2에서 저자는 classification에 대한 훈련과 dectection data를 함께 훈련하는 매커니즘을 제안한다. 훈련하는 동안, dectection과 classification 데이터셋 둘로 부터 이미지를 섞는다. 네트워크가 dectection을 위한 label된 이미지를 발견하면, full YOLOv2의 손실함수를 기반으로 역전파할 수 있다. detection과 classification data를 섞는 아이디어는 새로운 도전을 직면한다! detection 데이터셋들은 classification 데이터셋들과 비교해서 작다. detection 데이터셋들은 오직 공통인 object들과 일반적인 label만 가진다. 마치 dog 혹은 boat 처럼! 반면에 classification 데이터셋들은 더 넓고 깊은 범위의 label들을 가진다. 예를 들어, ImageNet 데이터셋은 “german shepherd”그리고 “Bedlington terrier”과 같은 개들의 백여가지 보다 더 많은 걸 가진다. 이러한 두 데이터셋들을 병합하기 위해 저자들은 *wordtree라고 부르는 것과 visual comcept들의 계층적 모델들을 만들었다. 모든 클래스들은 ROOT 아래에 위치해 있다. WordTree에서 DarkNet-19 모델을 훈련시켰다. WordTree로 부터 ImageNet 데이터셋의 1000개 class들을 추출했고, intermediate node들을 그것에 추가했다. 그리고 그것은 1000개에서 1369로 label space를 확장했다. 그리고 그것을 WordTree1k라고 불렀다. 지금은 Darknet-19의 output layer의 사이즈는 1000개 대신에 1369개가 되었다. 1369개의 예측들은, 하나의 softmax를 계산하지 않지만 동일한 개념인 분리된 softmax의 전반적인 synset들을 계산한다. Darknet-19에 369개의 추가적인 개념들을 더함에도 불구하고 71.9%의 top-1 정확도와 90.4%의 top-5의 정확도를 달성했다. detector은 bounding box와 tree의 probabilities를 예측하였지만, 둘 이상의 softmax를 사용하기 때문에 우리는 예측된 class들을 찾기위해 tree를 탐색해야 한다. tree의 위에서 아래까지 탐색하는데 threshold-probability보다 작아지는 확률에 도달하기 까지 매 split마다 가장 높은 신뢰도를 가져야 한다. 그리고 object class를 예측한다. 예를 들어, 만약 input image에 dog를 포함한다면, 그 트리의 확률은 아래와 같이 나타난다. 모든 이미지가 object를 가진다고 추정하는 대신에, YOLOv2의 objectness 예측기를 사용하여 root인 Pr(물리적 객체)의 값을 제공한다. 그 모델은 각 branch level에 대한 softmax를 출력한다. 우리가 위에서부터 아래로 움직이면서 가장 높은 확률(만약 threshold value보다 높다면)을 가진 노드를 고른다. 예측은 우리가 멈춘 노드가 될 것이다. 위의 트리에서, 모델은 physical object=&gt; dog =&gt; hunting dog 순으로 겪는다. hunting dog에서 멈출 것이고 sighthound로 내려가지 않을 것이다. 왜냐하면 그것의 신뢰도는 threshold 값보다 더 낮은 신뢰도를 가지기 때문이다. 그래서 그 모델은 sighthound가 아닌 hunting dog를 예측하는 것이다. 5. YOLOv3 전 버전 보다 좀 더 크지만 더 정확하다. 5-1. Bounding Box Prediction YOLO9000과 같이, 네트워크는 각 bounding box마다 4개의 좌표를 예측한다. 만약 그 cell이 (cx,cy)의 위쪽왼쪽 코너로 부터 떨어져있다면 bounding box prior은 width, heigh, Pw,Ph를 가지고 그런 다음 예측은 아래와 같이 일치한다. YOLOv3는 또한 logistic regression을 사용하여 각 bounding box에 대한 신뢰도를 예측한다. 이것은 1이될 것이다. 만약 bounding box prior이 다른 bounding box prior보다 더 많은 것에 의해 ground truth object가 오버랩한다면! 예를 들어(prior 1), 이전의 다른 bounding box보다 더 많이 첫번째 ground truth object보다 겹치고, prior2가 두번째 ground truth object와 겹친다. 시스템은 각 ground truth object에 대해 하나의 bounding box만 우선 할당한다. bounding box prior이 ground truth object에 할당되지 않은 경우 좌표 또는 클래스 예측에 대한 손실은 발생되지 않으며 오직 objectness만 발생한다. 만약 box가 가장 높은 IOU를 가지진 않지만 thresold 값 이상으로 ground truth object와 더 많이 겹치면 우리는 예측을 무시해도된다. (이때, thresold값 0.5) 5-2. Multi labels predection Open Image dataset과 같이 몇몇의 데이터셋들은 여러개의 label을 가질 수도 있다. 예를 들어, 하나의 object는 woman, person으로 label될 수 있다. 이 데이터셋에서, 많은 overlapping label이 존재한다. 클래스 예측에 대한 softmax를 사용하는 것은 각 box가 정확하게 하나의 class만 가진다는 가정을 두는 것이고, 그리고 그것은 종종 그렇지 않다. 이런 이유로 YOLOv3는 softmax를 사용하지 않는다. 대신 모든 class에 대해 독립적인 logistic classifier을 사용한다. 훈련 중에 class 예측을 위해 binary cross-entropy loss를 사용한다. 독립적인 logistic classifier를 사용하면, 한 object가 동시에 person으로서 woman으로서 detect될수 있다. 5-3. Small Objects Detection YOLO는 작은 object에 문제가 있었다. 그러나 YOLOv3는 작은 object에 더 나은 성능을 보인다. 왜냐하면 short cut connection을 사용하기 때문이다! short cut connetion 방법을 사용하는 것은 earlier feature map으로 부터 finer-grained information을 얻을 수 있다. 그러나 이전의 버전과 비교해서, YOLOv3는 medium과 large 사이즈 object에 대해 좀 더 안좋은 성능을 가지고 있다. 5-4. Feature Extractor Network(Darknet-53) YOLOv3는 feature extraction을 수행하는데 새로운 네트워크를 사용한다. YOLOv2(Darknet-19)에서 사용된 네트워크와 residual 네트워크 사이의 하이브리드 접근법이다. 그래서 몇몇의 short cut connection을 가진다. 53개 컨벌루션 레이어들을 가진다. 그것을 Darknet-53라 부른다. Darknet-53은 최신식의 classifier와 동등한 성능을 제공하지만 floating 작업이 적고 속도도 더 빠르다. classification 훈련후에 완전이 연결된 레이어는 Darknet-53으로 부터 제거되었다. 5-5. Predictions Across Scales YOLO와 YOLOv2와 다르게, 마지막 레이어에서 output을 예측한다. YOLOv3는 아래의 이미지에서 3개의 다른 규모들인 box들을 예측한다. 이것은 네트워크에 대한 간단한 diagram이다. 각각의 scale YOLOv3는 3개의 anchor box들을 사용하고 어떤 grid cell에 대해 3개의 box들을 예측한다. 각 object는 여전히 하나의 detection tensor에서 하나의 grid에 할당되어진다. 5-6. Performance AP50에서 정확도 vs 속도를 볼때, YOLOv3는 다른 detection 시스템을 넘어 상당한 benefit을 가진다. 그러나, YOLOv3 성능은 IOU threshold 증가함에 따라 떨어진다. YOLOv3는 object와 box를 완벽하게 분리하지는 못하지만, 그래도 다른 방법들보다는 빠르다.","categories":[],"tags":[{"name":"DL","slug":"DL","permalink":"https://eunjin-jun717.github.io/tags/DL/"}]},{"title":"[ML] Ensemble 학습방법","slug":"Python/ML/ensemble_study","date":"2021-05-13T15:00:00.000Z","updated":"2021-05-14T00:50:46.346Z","comments":true,"path":"2021/05/14/Python/ML/ensemble_study/","link":"","permalink":"https://eunjin-jun717.github.io/2021/05/14/Python/ML/ensemble_study/","excerpt":"","text":"앙상블 학습 유형 가장 많이 알려진 앙상블 학습 유형: Voting, Bagging, Boosting, Stacking 등 1. 앙상블 학습1-1. Voting, Averaging 둘은 서로 다른 알고리즘을 가진 Classifier를 결합하는 방식 Voting(분류/Classifier), Averaging(회귀/Regressor) Categorical인 경우에는 VotingNumerical인 경우에는 Averaging 이기도 함 방법-&gt; 일정 수의 base model과 predict를 만든다. -&gt; 훈련 데이터를 나누어 같은 알고리즘을 사용 or -&gt; 훈련 데이터는 같지만 다른 알고리즘을 사용 -&gt; 여기서 여러가지 방법으로 voting을 진행 1-2. Hard Voting = magjority voting = max voting = plurality voting 각 모델은 test data set의 결과를 예측 예측값들의 다수결로 예측값을 정함 즉, 이진분류에 있어서는 과반수 이상이 선택한 예측값을 최종 예측으로 선택 1-3. Soft Voting = weighted voting hard voting 방법과는 다르게, 좀 더 유연한 보팅 방법 Test dataset의 결과 가능성을 예측함 이 가중치를 특정 연산을 하여 분류 label의 확률값을 계산 가중치의 연산 방법: 보통 평균사용, 원하는 방식으로 사용 가능 Hard voting보다 유연한 결과 얻을 수 있으며, 예측 성능이 좋아 더 사용 2. Bagging 평균을 통해 분산 값을 줄여 모델을 더 일반화 시킴 최종적으로는 Voting을 사용-&gt; 일정 수의 Base model을 만듬 -&gt; 모델들의 알고리즘은 모두 같음 -&gt; 각각의 모델은 훈련 데이터셋에서 랜덤으로 만든 서브 데이터셋을 각각 사용 -&gt; 서브 데이터셋을 만드는 과정: Boot Straping -&gt; 배깅의 대표적인 알고리즘: RandomForest 3. Stacking &amp; Blending3-1. Stacking 머신러닝 알고리즘으로 훈련 데이터셋을 통해 새로운 데이터 셋을 만들고, 이를 데이터셋으로 사용하여 다시 머신러닝 알고리즘을 돌리는 것 보통은 서로 다른 타입의 모델들을 결합- 개별적인 기반 모델: 성능이 비슷한 여러 개의 모델 - 최종 메타 모델: Base model들이 만든 예측 데이터를 학습 데이터로 사용할 최종 모델 결론 여러개의 개별 모델들이 생성한 예측 데이터를 기반으로 최종 메타 모델이 학습할 별도의 학습 데이터 셋과 예측할 테스트 데이터 셋을 재생성하는 기법 모델을 통해 input을 만들고, 다시 모델에 넣는 구조 때문에 meta-model 이라고도 부름 3-2. Blending Blending은 Stacking과 비슷하지만 다른 점이 있다면 Blending은 Validation set을 활용한다. 방법step1) training/validation/test set을 나눈다 step2) training set에 모델 피팅을 한다 step3) validation/test set에 대하여 예측을 한다 step4) validation set과 그에 대한 예측이 새로운 모델을 만드는데 사용한다. step5) 4의 모델을 최종 test set에 대한 예측을 하는데 사용된다. 단점- 사용하게 되는 데이터의 양이 적어짐 - 최종 모델이 holdout set에 대해 과적합될 수 있음 Stacking보다는 간단하며 정보 누설의 위험을 줄임 참고 : Hold out이란 ? Hold out은 데이터셋을 train과 test set으로 나누는 과정을 의미 train이 작으면 모델 정확도의 분산이 커짐(과소적합의 가능성) train이 커지면 test로부터 측정한 정확도의 신뢰도 하락(과대적합) 3-3. Stacking &amp; Blending 차이점 Stacking에서는 cross-fold-validation을 사용하고, blending에서는 holdout validation을 사용함 그렇기 때문에 blending의 결과는 holdout set에 과대적합이 된 결과를 얻을 가능성이 높음 blending은 valid set에 대한 예측값을 training set에 이용하지만, stacking은 training set에 대한 예측값을 활용함 blending은 예측값 뿐만 아니라 원래 feature도 활용하는 반면, stacking은 예측값만 활용 references https://subinium.github.io/introduction-to-ensemble-1/ https://3months.tistory.com/486 https://m.blog.naver.com/sjc02183/221739648990","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[ML] Imbalanced Data 처리(2)","slug":"Python/ML/imbalanced_data_response_variables","date":"2021-05-12T15:00:00.000Z","updated":"2021-05-14T03:41:31.307Z","comments":true,"path":"2021/05/13/Python/ML/imbalanced_data_response_variables/","link":"","permalink":"https://eunjin-jun717.github.io/2021/05/13/Python/ML/imbalanced_data_response_variables/","excerpt":"","text":"불균형 데이터(Imbalanced Data) 처리(2) 2. 독립변수에 따른 불균형 데이터 High Cardinality에 대한 Encoding 방법 sklearn의 Category Encoders package 사용 category encoder은 nominal 혹은 ordinal 데이터일 때 사용 nominal(명목) data 통계에서 quantitative 값을 제공하지 않고 변수에 label을 지정하는데 사용되는 데이터이다. ordinal 데이터와 다르게 순서를 매길 수도 측정할 수도 없다. ex) gender(성별), 혈액형- Encoding: Onehot, Hashing, LeaveOneOut, Target - high cardinality 인 열과 decision tree-based 알고리즘은 Onehot encoding을 피해야한다. ordinal(서수) data 각각의 값들이 이산적이며 그 값들 사이에 순서가 존재 가장 큰 특징은 데이터 값 간의 차이를 알 수 없거나 의미가 없는 것 inteval 혹은 ratio 데이터와 달리 수학적인 연산자를 사용할 수 없다. 따라서 Ordinal data를 포함하는 dataset의 중심 경향을 측정할 수 있는 유일한 척도는 ‘중앙값’이다.- Encoding: Ordinal(Integer), Binary, OneHot, LeaveOneOut, Target - Helmert, Sum, BackwardDifference, Polynomial 은 시간적 여유가 있거나 이것을 시도할 이론적인 여유가 있다면 시도해보도록 한다. &gt;&gt; regression 문제에서는 Target, LeaveOneOut은 아마 잘 적용되지 않을 것이다. 2-1. Ordinal Encoder 각 문자열 값을 정수로 변환 열의 첫번째 unique한 값은 1, 두번째는 2,... Encoding 이전의 실제 값은 OrdinalEncoder로 fit_transform할 때 값에 영향을 주지는 않는다. 열에 Nominal data(명목형)가 포함된 경우 OrdinalEncoder를 사용하는 것은 좋지 않다. 머신러닝 알고리즘은 변수를 연속형으로 처리하고 값이 의미있는 척도에 있다고 가정한다. 반대로 열이 Ordinal data(서수형)이면 각 값에 할당된 정수가 의미가 있음을 의미한다. 1!pip install category_encoders 1234567891011121314import numpy as npimport pandas as pdimport category_encoders as cefrom sklearn.preprocessing import LabelEncoderpd.options.display.float_format = &#x27;&#123;:.2f&#125;&#x27;.formatdf = pd.DataFrame(&#123; &#x27;color&#x27;:[&#x27;a&#x27;,&#x27;c&#x27;,&#x27;a&#x27;,&#x27;a&#x27;,&#x27;b&#x27;,&#x27;b&#x27;], &#x27;outcome&#x27;:[1,2,0,0,0,1]&#125;)X= df.drop(&#x27;outcome&#x27;,axis=1)y= df.drop(&#x27;color&#x27;, axis=1) 1X .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color 0 a 1 c 2 a 3 a 4 b 5 b 12ce_ord = ce.OrdinalEncoder(cols=[&#x27;color&#x27;])ce_ord.fit_transform(X,y[&#x27;outcome&#x27;]) /usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version. Use is_categorical_dtype instead elif pd.api.types.is_categorical(cols): .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color 0 1 1 2 2 1 3 1 4 3 5 3 문자열의 unique값이 나오는 순서대로 정수를 mapping함 ‘a’ -&gt; 1, ‘c’-&gt; 2, ‘b’ -&gt; 3 2-2. Onehot Encoding Nominal data(명목형)를 처리하는 고전적인 접근 방식 Dummy Encoding, Indicator Encoding, Binary Encoding 이라고도 함 다른 모든 값과 비교할 각 값에 대해 하나의 열을 생성함각각의 새 열에 대해 행에 해당 열의 값이 포함되어 있으면 1을, 그렇지 않으면 0을 할당 12ce_one_hot=ce.OneHotEncoder(cols=[&#x27;color&#x27;])ce_one_hot.fit_transform(X,y) /usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version. Use is_categorical_dtype instead elif pd.api.types.is_categorical(cols): .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color_1 color_2 color_3 0 1 0 0 1 0 1 0 2 1 0 0 3 1 0 0 4 0 0 1 5 0 0 1 Onehot Encoder은 High Cardinality인 경우에는 심각한 메모리 문제를 일으킬 수 있다. why? High Cardinality는 많은 범주가 있기 때문 또한 decision tree-based에서도 문제를 일으킬 수 있다. 2-3. Binary Encoding Onehot encoder와 Hashing Encoder의 하이브리드라 생각할 수 있다. Binary Encoding은 열에 있는 값의 일부 고유성은 유지하면서 Onehot encoder보다 적은 features를 생성한다. 다시말해, Onehot encoding할 때의 차원보다 더 적은 차원을 생성한다는 뜻 메모리 효율성이 더 좋다. 많은 차원이 있는 Ordinal data에서 더 적합함 Binary Encoding 알고리즘 category가 숫자형식이 아니면 Ordinal Encoder에 의해 인코딩된다. Integer들은 Binary code로 변환된다. 5-&gt; 101, 10-&gt; 1010 그런 다음, 해당 이진 문자열의 숫자가 별도의 열로 분할 된다. Ordinal열에 4~7개의 값이 있는 경우 -&gt; 3개의 새 열에 생성하나는 첫번째 비트, 하나는 두번째 비트, 다른 하나는 세번째 비트 12ce_bin= ce.BinaryEncoder(cols=[&#x27;color&#x27;])ce_bin.fit_transform(X,y) /usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version. Use is_categorical_dtype instead elif pd.api.types.is_categorical(cols): .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color_0 color_1 color_2 0 0 0 1 1 0 1 0 2 0 0 1 3 0 0 1 4 0 1 1 5 0 1 1 첫번째 열에는 variance가 없으므로 모델에 아무런 도움이 되지 않는다. 세가지 level만 있으면 포함된 정보는 혼란스러워진다. 따라서, 값이 몇개인 경우에는 열을 OneHot Encoding한다. 결론적으로, Binary Encoding은 High Cardinality인 Ordinal data에 사용하자! 2-4. BaseN BaseN의 base=1이면 Onehot Encoding과 같은 동작을 한다. base=2 일 때는 Binary Encoding과 같은 역할을 한다. 따라서 base가 3~8일 때 적합하다. BaseN의 주요 특징은 Grid Search를 더 쉽게 만드는 것이다. 12ce_basen= ce.BaseNEncoder(cols =[&#x27;color&#x27;])ce_basen.fit_transform(X,y) /usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version. Use is_categorical_dtype instead elif pd.api.types.is_categorical(cols): .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color_0 color_1 color_2 0 0 0 1 1 0 1 0 2 0 0 1 3 0 0 1 4 0 1 1 5 0 1 1 BaseN Encoder의 Default base는 BinaryEncoder와 동일한 2이다. 2-5. Hashing Onehot Encoding과 유사하지만 새로운 feature가 더 적고 충돌로 인해 정보가 손실될 가능성이 있다. 겹치는 부분이 많지 않으면 충돌이 성능에 큰 영향을 주지는 않는다. 1X .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color 0 a 1 c 2 a 3 a 4 b 5 b 12ce_hash = ce.HashingEncoder(cols=[&#x27;color&#x27;])ce_hash.fit_transform(X,y) /usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version. Use is_categorical_dtype instead elif pd.api.types.is_categorical(cols): .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; col_0 col_1 col_2 col_3 col_4 col_5 col_6 col_7 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 2 0 1 0 0 0 0 0 0 3 0 1 0 0 0 0 0 0 4 0 0 0 0 0 0 0 1 5 0 0 0 0 0 0 0 1 n_components: 확장 열 수를 제어하는 매개 변수- 기본 값: 3개의 열 - 3개 값이 있는 예제 열에는 기본값은 0으로 가득 찬 5개의 열이 된다. n_components를 k보다 적게 설정하면 encoding된 데이터에서 제공하는 값이 약간 감소한다. 혹은 더 작은 feature 수를 가지게 된다. 결론적으로, HashingEncoder은 High Cardinality가 있는 경우에 Nominal, Ordinal 데이터에 대해 사용할 가치가 있다. references https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159 https://contrib.scikit-learn.org/category_encoders/#","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Statistic] Basic Data Types","slug":"Python/Statistic/nominal_ordinal_study_ej","date":"2021-05-05T06:08:52.000Z","updated":"2021-05-14T02:29:52.799Z","comments":true,"path":"2021/05/05/Python/Statistic/nominal_ordinal_study_ej/","link":"","permalink":"https://eunjin-jun717.github.io/2021/05/05/Python/Statistic/nominal_ordinal_study_ej/","excerpt":"","text":"Basic Data Types Nominal Data(명목 자료) Ordinal Data(순서 자료) Interval Data(구간 자료) Ratio Data(비율 자료) Discrete and Continuous (이산형, 연속형) 1. Nominal Data nominal(Named)이란 여러 catergories 들 중 하나의 이름에 데이터를 분류할 수 있을 때 사용된다. ex) 청팀, 홍팀, 백팀 순서를 매길 수 없고, 셀 수만 있다. 수학적인 계산을 하는 것은 의미가 없다. 그러나 percentage로는 표현해도 된다. ex) 청팀(33%), 홍팀(40%), 백팀(27%) nominal data가 두개의 범주 중 하나에 속하는 경우에는 Dichotomous data(이분 자료)라고도 한다. ex) gender(female, male) 2. Ordinal Data 데이터가 속하는 categories에 순서가 있는 경우 ordinal data라 한다. 즉, 순서가 있는 명목형 자료ex) 만족도: 1(낮음)/ 2(보통)/ 3(높음) nominal data 처럼 세는 것은 가능하고, percentage로 표현도 가능하다. 단, 평균에 대해서는 신중해야한다. 위의 예시와 같이 만족도의 1,2,3에 대한 숫자에 수학적/과학적 의미가 있지 않기 때문이다. 3. Interval Data 데이터의 연속된 측정 구간 사이의 간격이 동일한 경우 Interval data라고 부른다.ex) 온도, 시간 Interval data는 Numeric value를 가지므로 다양한 연산이 수행 가능하다.평균, 중앙값 등을 이용하여 척도의 중심 경향 계산 가능 단점: 미리 결정된 시작점이나 절대적 원점(zero point)이 없는 것00:00 이라는 자료의 값이 측정한 시간의 값이 없다는 것이 아니라 그냥 자정에 시간을 측정했다는 것! Ordinal data의 속성들을 모두 포함한다. 4. Ratio Data 비율척도라 한다. ex) 나이, 돈, 몸무게 현재 시각이 10시인데 시계를 보고 10시부터 10시 30분까지 계산해서 30분 기다렸네 하면 이것이 ratio data이다. Interval data와 다르게 절대적 원점(zero point)가 존재한다. 00:00 이라는 값은 기다린 시간이 0초라는 것이다. 5. Discrete, Continuous Interval, ratio 자료는 discrete이나 continuous 둘 중 하나의 속성을 가진다. discrete: 측정값이 정수로 떨어지는 것ex) 사탕 갯수(1개,2개,3개,...) continuous: 측정값이 연속된 무수히 많은 값으로 이루어진 것ex) 몸무게(72.6kg) ‘나이’라는 데이터는 본질적으로는 Ratio data이지만 Ordinal data로도 수집될 수 있다.ex) 나이가 속한 그룹: 21~29, 31~39 등 반면, nominal이나 ordinal(둘다 categorical data)은 interval이나 ratio data로 수집될 수 없다. ex) 청팀, 홍팀, 백팀을 interval, ratio로 수집할 수 없음 - 보편적으로 얘기하면, 데이터 측정은 주어진 데이터의 본질적 속성보다 더 낮은 수준으로 내려갈수는 있어도 더 정교한 수준으로 올라갈 수는 없다. - 다시 말해, Interval/ratio -&gt; nominal/ordinal (O) - nominal/ordinal -&gt; Interval/ratio (X) 6. Summary of Data types references http://blog.heartcount.io/dd nominal, ordinal, interval, ration data types","categories":[],"tags":[{"name":"statistic","slug":"statistic","permalink":"https://eunjin-jun717.github.io/tags/statistic/"}]},{"title":"[Python] Kaggle TPS-Playground-April","slug":"Python/Kaggle/tps-april-eda-feature-engineering","date":"2021-04-25T15:00:00.000Z","updated":"2021-05-14T00:13:42.470Z","comments":true,"path":"2021/04/26/Python/Kaggle/tps-april-eda-feature-engineering/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/26/Python/Kaggle/tps-april-eda-feature-engineering/","excerpt":"","text":"First look (1) Check data for NA EDA(Exploratory Data Analysis) (1) Survived (2) Age (3) Cabin (3) Family (3) Pclass (3) Sex (3) Embarked (3) Fare EDA(Exploratory Data Analysis) referenceshttps://www.kaggle.com/demidova/titanic-eda-tutorial[https://www.kaggle.com/demidova/titanic-logistic-regression-random-forest-xgboost?scriptVersionId=46567425] https://namu.wiki/jump/9AGb4mj%2Bgar2D116rRySHULPcuF9aQA9dU1%2FKaQlJabHnX1Bwo7dW3QKZZU5EDX7tyS7%2BeKInzFlBX0PyH2gvmr0xlEeT19AQhYRU4yv8erx25eqVyS5NlWU2pDAk3mhBaO4i%2BaABck5vAWwFaAE0g%3D%3D (1) Data Import12345678910111213import pandas as pdimport matplotlibimport matplotlib.pyplot as pltimport numpy as npimport seaborn as sbimport osprint(&quot;Version Pandas&quot;, pd.__version__)print(&quot;Version Matplotlib&quot;, matplotlib.__version__)print(&quot;Version Numpy&quot;, np.__version__)print(&quot;Version Seaborn&quot;, sb.__version__)os.listdir(&#x27;../input/tabular-playground-series-apr-2021/&#x27;) Version Pandas 1.2.3 Version Matplotlib 3.4.1 Version Numpy 1.19.5 Version Seaborn 0.11.1 [&#39;sample_submission.csv&#39;, &#39;train.csv&#39;, &#39;test.csv&#39;] 123456BASE_DIR = &#x27;../input/tabular-playground-series-apr-2021/&#x27;train = pd.read_csv(BASE_DIR + &#x27;train.csv&#x27;)test = pd.read_csv(BASE_DIR + &#x27;test.csv&#x27;)sample_submission = pd.read_csv(BASE_DIR + &#x27;sample_submission.csv&#x27;)train.shape, test.shape, sample_submission.shape ((100000, 12), (100000, 11), (100000, 2)) 1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 0 1 1 Oconnor, Frankie male NaN 2 0 209245 27.14 C12239 S 1 1 0 3 Bryan, Drew male NaN 0 0 27323 13.35 NaN S 2 2 0 3 Owens, Kenneth male 0.33 1 2 CA 457703 71.29 NaN S 3 3 0 3 Kramer, James male 19.00 0 0 A. 10866 13.04 NaN S 4 4 1 3 Bond, Michael male 25.00 0 0 427635 7.76 NaN S 1test.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 100000 3 Holliday, Daniel male 19.0 0 0 24745 63.01 NaN S 1 100001 3 Nguyen, Lorraine female 53.0 0 0 13264 5.81 NaN S 2 100002 1 Harris, Heather female 19.0 0 0 25990 38.91 B15315 C 3 100003 2 Larsen, Eric male 25.0 0 0 314011 12.93 NaN S 4 100004 1 Cleary, Sarah female 17.0 0 2 26203 26.89 B22515 C 1sample_submission.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived 0 100000 1 1 100001 1 2 100002 1 3 100003 1 4 100004 1 1234frames= [train, test]total_df=pd.concat(frames, sort=False)print(&#x27;total data shape: &#x27;, total_df.shape)total_df.head() total data shape: (200000, 12) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 0 1.0 1 Oconnor, Frankie male NaN 2 0 209245 27.14 C12239 S 1 1 0.0 3 Bryan, Drew male NaN 0 0 27323 13.35 NaN S 2 2 0.0 3 Owens, Kenneth male 0.33 1 2 CA 457703 71.29 NaN S 3 3 0.0 3 Kramer, James male 19.00 0 0 A. 10866 13.04 NaN S 4 4 1.0 3 Bond, Michael male 25.00 0 0 427635 7.76 NaN S 1total_df.describe(include=[object]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Name Sex Ticket Cabin Embarked count 200000 200000 190196 61303 199473 unique 174854 2 132613 45442 3 top Smith, James male A/5 C11139 S freq 61 125871 646 7 140981 1total_df.describe(include=[object]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Name Sex Ticket Cabin Embarked count 200000 200000 190196 61303 199473 unique 174854 2 132613 45442 3 top Smith, James male A/5 C11139 S freq 61 125871 646 7 140981 (2) Check data for NA dataset의 feature들을 살펴보고, null data의 여부를 체크해보자 종속변수 Survived(생존여부): target label (1,0) -&gt; integer 독립변수 PassengerId: 10000명 Pclass(티켓의 클래스): Upper(1), Middle(2), Lower(3) -&gt; categorical -&gt; integer Name(이름): 탑승자 성명들 Sex(성별): Male, Female -&gt; binary -&gt; string Age(나이): continuous -&gt; integer SibSp(함께 탑승한 형제와 배우자의 수): quantitative -&gt; integer Parch(함께 탑승한 부모, 아이의 수): quantitative -&gt; integer Ticket(티켓 번호): alphabet + integer -&gt; string Fare(탑승료): continous -&gt; float Cabin(객실 번호): alphabet + integer -&gt; string Embarked(탑승항구): C(Cherbourg), Q(Queenstown), S(Southhampton) -&gt; string references https://kaggle-kr.tistory.com/17 1total_df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 200000 entries, 0 to 99999 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 200000 non-null int64 1 Survived 100000 non-null float64 2 Pclass 200000 non-null int64 3 Name 200000 non-null object 4 Sex 200000 non-null object 5 Age 193221 non-null float64 6 SibSp 200000 non-null int64 7 Parch 200000 non-null int64 8 Ticket 190196 non-null object 9 Fare 199733 non-null float64 10 Cabin 61303 non-null object 11 Embarked 199473 non-null object dtypes: float64(3), int64(4), object(5) memory usage: 19.8+ MB Age, Fare -&gt; numeric variablesPclass -&gt; integer but in fact ‘categorical variable’ 12345total_df_na=total_df.isna().sum()train_na=train.isna().sum()test_na=test.isna().sum()pd.concat([train_na, test_na, total_df_na], axis=1, sort=False, keys=[&#x27;Train NA&#x27;,&#x27;Test NA&#x27;,&#x27;Total NA&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Train NA Test NA Total NA PassengerId 0 0.0 0 Survived 0 NaN 100000 Pclass 0 0.0 0 Name 0 0.0 0 Sex 0 0.0 0 Age 3292 3487.0 6779 SibSp 0 0.0 0 Parch 0 0.0 0 Ticket 4623 5181.0 9804 Fare 134 133.0 267 Cabin 67866 70831.0 138697 Embarked 250 277.0 527 missing data를 handling하기 위해서 EDA에서는 dataset을 합쳤지만, ML에서는 ‘data leakage’를 피하기 위해서 오직 train data set만 사용할 것이다. 1total_df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Age SibSp Parch Fare count 200000.000000 100000.000000 200000.000000 193221.000000 200000.000000 200000.000000 199733.000000 mean 99999.500000 0.427740 2.237920 34.464565 0.442120 0.473695 44.652071 std 57735.171256 0.494753 0.868273 16.783847 0.819392 0.937125 67.436104 min 0.000000 0.000000 1.000000 0.080000 0.000000 0.000000 0.050000 25% 49999.750000 0.000000 1.000000 22.000000 0.000000 0.000000 10.080000 50% 99999.500000 0.000000 3.000000 31.000000 0.000000 0.000000 20.250000 75% 149999.250000 1.000000 3.000000 48.000000 1.000000 1.000000 34.850000 max 199999.000000 1.000000 3.000000 87.000000 8.000000 9.000000 744.660000 (1) Survived train set에서 survived의 0,1 분포가 어떤지 확인해보겠습니다. 분포에 따라 모델의 평가 방법이 달라질 수 있습니다. 123456789101112131415plt.figure(figsize=(6, 4.5))ax= sb.countplot(x=&#x27;Survived&#x27;, data=total_df, palette=[&#x27;#4287f5&#x27;,&#x27;#7cd91e&#x27;])plt.xticks(np.arange(2), [&#x27;Drowned&#x27;,&#x27;Survived&#x27;])plt.title(&#x27;Overall survival&#x27;, fontsize=14)plt.xlabel(&#x27;Survived vs Drowned&#x27;)plt.ylabel(&#x27;Number of Passendgers&#x27;)labels=(total_df[&#x27;Survived&#x27;].value_counts())for i,v in enumerate(labels): ax.text(i, v-40, str(v), horizontalalignment=&#x27;center&#x27;, size=14, color=&#x27;w&#x27;, fontweight=&#x27;bold&#x27;) plt.show() 1total_df[&#x27;Survived&#x27;].value_counts(normalize=True) 0.0 0.57226 1.0 0.42774 Name: Survived, dtype: float64 (2) Independent Variables references https://wikidocs.net/75068 1) Age6779 : age missing values 3292 : train dataset 3487 : test dataset 123456789plt.figure(figsize=(15,3))sb.distplot(total_df[(total_df[&#x27;Age&#x27;]&gt;0)].Age, kde_kws=&#123;&#x27;lw&#x27;:3&#125;, bins=50)plt.title(&#x27;Distribution of passengers age (total data)&#x27;, fontsize=14)plt.xlabel(&#x27;Age&#x27;)plt.ylabel(&#x27;Frequency&#x27;)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) 12age_distr= pd.DataFrame(total_df[&#x27;Age&#x27;].describe())age_distr.transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Age 193221.0 34.464565 16.783847 0.08 22.0 31.0 48.0 87.0 0.08세 ~ 87세까지 다양하게 나이대가 있으며 mean=34.46세 이다. 1-1) Age by surviving status123456789plt.figure(figsize=(15,3))sb.boxplot(y=&#x27;Survived&#x27;, x=&#x27;Age&#x27;, data=train, palette=[&#x27;#4287f5&#x27;,&#x27;#7cd91e&#x27;], fliersize=0, orient=&#x27;h&#x27;)sb.stripplot(y=&#x27;Survived&#x27;,x=&#x27;Age&#x27;, data=train, linewidth=0.6, palette=[&#x27;#4287f5&#x27;,&#x27;#7cd91e&#x27;], orient=&#x27;h&#x27;)plt.yticks(np.arange(2), [&#x27;Drowned&#x27;,&#x27;Survived&#x27;])plt.title(&#x27;Age distribution grouped by survivng status (train data)&#x27;, fontsize=14)plt.ylabel(&#x27;Passengers status after the tragedy&#x27;)plt.tight_layout() 1pd.DataFrame(total_df.groupby(&#x27;Survived&#x27;)[&#x27;Age&#x27;].describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Survived 0.0 55290.0 36.708695 17.809058 0.08 24.0 36.0 52.0 83.0 1.0 41418.0 40.553799 18.742172 0.08 27.0 43.0 55.0 87.0 1-2) Age by Pclass1234567891011121314151617181920212223242526plt.figure(figsize=(20,6))palette=sb.cubehelix_palette(5, start=3)plt.subplot(1,2,1)sb.boxplot(x=&#x27;Pclass&#x27;, y=&#x27;Age&#x27;, data=total_df, palette=palette, fliersize=0)plt.xticks(np.arange(3), [&#x27;1st class&#x27;,&#x27;2nd class&#x27;,&#x27;3rd class&#x27;])plt.title(&#x27;Age distribution grouped by ticket class (total data)&#x27;, fontsize=16)plt.xlabel(&#x27;Ticket class&#x27;)plt.subplot(1,2,2)age_1_class = total_df[(total_df[&#x27;Age&#x27;]&gt;0)&amp;(total_df[&#x27;Pclass&#x27;]==1)]age_2_class = total_df[(total_df[&#x27;Age&#x27;]&gt;0)&amp;(total_df[&#x27;Pclass&#x27;]==2)]age_3_class = total_df[(total_df[&#x27;Age&#x27;]&gt;0)&amp;(total_df[&#x27;Pclass&#x27;]==3)]# Ploting the 3 variables that we createsb.kdeplot(age_1_class[&quot;Age&quot;], shade=True, color=&#x27;#eed4d0&#x27;, label = &#x27;1st class&#x27;)sb.kdeplot(age_2_class[&quot;Age&quot;], shade=True, color=&#x27;#cda0aa&#x27;, label = &#x27;2nd class&#x27;)sb.kdeplot(age_3_class[&quot;Age&quot;], shade=True,color=&#x27;#a2708e&#x27;, label = &#x27;3rd class&#x27;)plt.title(&#x27;Age distribution grouped by ticket class (total data)&#x27;,fontsize= 16)plt.xlabel(&#x27;Age&#x27;)plt.xlim(0, 90)plt.tight_layout()plt.show() 1pd.DataFrame(total_df.groupby(&#x27;Pclass&#x27;)[&#x27;Age&#x27;].describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Pclass 1 55365.0 40.672757 14.715634 0.08 28.0 41.0 52.0 83.0 2 36606.0 36.855067 18.470623 0.08 23.0 35.0 52.0 87.0 3 101250.0 30.205570 15.954521 0.08 20.0 26.0 41.0 85.0 2nd 클래스는 1st, 3rd 클래스에 비해 더 넓은 분포를 가진다. 또한 거의 대칭 적이다.가장 나이가 적은 passenger은 1,2,3 등급 동일한 나이인 0.08세이다. 가장 나이가 많은 passenger은 2nd 클래스의 87세이다. 3rd 클래스 mean age= 30.2세2nd 클래스 mean age= 36.9세1st 클래스 mean age= 40.7세 1-3) Age vs Pclass vs Sex12345678910111213141516171819202122232425plt.figure(figsize=(20, 5))palette = &quot;Set3&quot;plt.subplot(1, 3, 1)sb.boxplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_1_class, palette = palette, fliersize = 0)#sb.stripplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_1_class,linewidth = 0.6, palette = palette)plt.title(&#x27;1st class Age distribution by Sex&#x27;,fontsize= 14)plt.ylim(-5, 80)plt.subplot(1, 3, 2)sb.boxplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_2_class, palette = palette, fliersize = 0)#sb.stripplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_2_class,linewidth = 0.6, palette = palette)plt.title(&#x27;2nd class Age distribution by Sex&#x27;,fontsize= 14)plt.ylim(-5, 80)plt.subplot(1, 3, 3)sb.boxplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_3_class, order = [&#x27;female&#x27;, &#x27;male&#x27;], palette = palette, fliersize = 0)#sb.stripplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_3_class,order = [&#x27;female&#x27;, &#x27;male&#x27;], linewidth = 0.6, palette = palette)plt.title(&#x27;3rd class Age distribution by Sex&#x27;,fontsize= 14)plt.ylim(-5, 80)plt.show() 12345age_1_class_stat = pd.DataFrame(age_1_class.groupby(&#x27;Sex&#x27;)[&#x27;Age&#x27;].describe())age_2_class_stat = pd.DataFrame(age_2_class.groupby(&#x27;Sex&#x27;)[&#x27;Age&#x27;].describe())age_3_class_stat = pd.DataFrame(age_3_class.groupby(&#x27;Sex&#x27;)[&#x27;Age&#x27;].describe())pd.concat([age_1_class_stat, age_2_class_stat, age_3_class_stat], axis=0, sort = False, keys = [&#x27;1st&#x27;, &#x27;2nd&#x27;, &#x27;3rd&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Sex 1st female 25790.0 41.974173 15.253798 0.08 29.0 43.0 54.0 83.0 male 29575.0 39.537896 14.132517 0.08 28.0 39.0 51.0 80.0 2nd female 17554.0 37.283031 20.558883 0.08 22.0 36.0 55.0 87.0 male 19052.0 36.460753 16.302224 0.08 24.0 34.0 50.0 85.0 3rd female 28349.0 27.222629 17.974800 0.08 15.0 24.0 39.0 85.0 male 72901.0 31.365545 14.936175 0.08 22.0 27.0 41.0 83.0 2) Cabin 첫번째 코드만 추출함 A: lst class B C: 3rd class D: walking area E: 1st and 2nd class F: 2nd class, 2rd class G: boiler room T: boat deck U: Unknown 12total_df[&#x27;Cabin&#x27;]=total_df[&#x27;Cabin&#x27;].str.split(&#x27;&#x27;,expand=True)[1]total_df.loc[total_df[&#x27;Cabin&#x27;].isna(), &#x27;Cabin&#x27;]=&#x27;X&#x27; 12345678910111213141516171819202122232425fig = plt.figure(figsize=(20, 5))ax1 = fig.add_subplot(131)sb.countplot(x = &#x27;Cabin&#x27;, data = total_df, palette = &quot;hls&quot;, order = total_df[&#x27;Cabin&#x27;].value_counts().index, ax = ax1)plt.title(&#x27;Passengers distribution by Cabin&#x27;,fontsize= 16)plt.ylabel(&#x27;Number of passengers&#x27;)ax2 = fig.add_subplot(132)Cabin_by_class = total_df.groupby(&#x27;Cabin&#x27;)[&#x27;Pclass&#x27;].value_counts(normalize = True).unstack()Cabin_by_class.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;,color = [&#x27;#eed4d0&#x27;, &#x27;#cda0aa&#x27;, &#x27;#a2708e&#x27;], ax = ax2)plt.legend((&#x27;1st class&#x27;, &#x27;2nd class&#x27;, &#x27;3rd class&#x27;), loc=(1.04,0))plt.title(&#x27;Proportion of classes on each Cabin&#x27;,fontsize= 16)plt.xticks(rotation = False)ax3 = fig.add_subplot(133)Cabin_by_survived = total_df.groupby(&#x27;Cabin&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True).unstack()Cabin_by_survived = Cabin_by_survived.sort_values(by = 1, ascending = False)Cabin_by_survived.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], ax = ax3)plt.title(&#x27;Proportion of survived/drowned passengers by Cabin&#x27;,fontsize= 16)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))plt.xticks(rotation = False)plt.tight_layout()plt.show() /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): 대부분의 passengers는 Cabin code가 없다. Cabin code가 나와있는 승객들 중 가장 많은 수를 차지하는 deck은 ‘C’이며 lst class ticket이다. ‘C’ deck은 살아남은 승객들 중 4번째이다. 가장 많은 생존률을 가진 deck은 ‘F’이다. ‘A’ deck은 lifeboats와 가장 가까운 deck이였지만 생존률은 가장 낮은 확률을 보이고 있다. 3) Family Family size = Sib + Parch +1 1234total_df[&#x27;Family_size&#x27;]=total_df[&#x27;SibSp&#x27;]+total_df[&#x27;Parch&#x27;]+1family_size=total_df[&#x27;Family_size&#x27;].value_counts()print(&#x27;Family size and number of passengers:&#x27;)print(family_size) Family size and number of passengers: 1 116448 2 30139 3 25289 4 19724 5 4151 6 2021 7 1184 10 397 11 242 8 162 9 157 14 46 12 26 13 7 18 4 15 3 Name: Family_size, dtype: int64 12345678910111213141516171819202122fig = plt.figure(figsize = (12,4))ax1 = fig.add_subplot(121)ax = sb.countplot(total_df[&#x27;Family_size&#x27;], ax = ax1)# calculate passengers for each categorylabels = (total_df[&#x27;Family_size&#x27;].value_counts())# add result numbers on barchartfor i, v in enumerate(labels): ax.text(i, v+6, str(v), horizontalalignment = &#x27;center&#x27;, size = 10, color = &#x27;black&#x27;) plt.title(&#x27;Passengers distribution by family size&#x27;)plt.ylabel(&#x27;Number of passengers&#x27;)ax2 = fig.add_subplot(122)d = total_df.groupby(&#x27;Family_size&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True).unstack()d.plot(kind=&#x27;bar&#x27;, color=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], stacked=&#x27;True&#x27;, ax = ax2)plt.title(&#x27;Proportion of survived/drowned passengers by family size (train data)&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))plt.xticks(rotation = False)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): family size가 15명인 그룹은 모두 살아남지 못하였다. 대부분은 혼자 여행하는 사람들이였고, 생존율은 40% 정도 이다. 가장 높은 생존율을 보이는 family size는 2,3 정도이다. 4개의 category로 family size group을 나누어보겠다. single usual(sizes 2,3,4,5) big(6,7,8,9) large(all bigger then 10) 1234total_df[&#x27;Family_size_group&#x27;]=total_df[&#x27;Family_size&#x27;].map(lambda x: &#x27;f_single&#x27; if x ==1 else(&#x27;f_usual&#x27; if 6&gt;x&gt;=2 else(&#x27;f_big&#x27; if 10&gt;x&gt;=6 else(&#x27;f_large&#x27;)))) 1234567891011121314151617181920fig = plt.figure(figsize = (14,5))ax1 = fig.add_subplot(121)d = total_df.groupby(&#x27;Family_size_group&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True).unstack()d = d.sort_values(by = 1, ascending = False)d.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color = [&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], ax = ax1)plt.title(&#x27;Proportion of survived/drowned passengers by family size&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))_ = plt.xticks(rotation=False)ax2 = fig.add_subplot(122)d2 = total_df.groupby(&#x27;Family_size_group&#x27;)[&#x27;Pclass&#x27;].value_counts(normalize = True).unstack()d2 = d2.sort_values(by = 1, ascending = False)d2.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color = [&#x27;#eed4d0&#x27;, &#x27;#cda0aa&#x27;, &#x27;#a2708e&#x27;], ax = ax2)plt.legend((&#x27;1st class&#x27;, &#x27;2nd class&#x27;, &#x27;3rd class&#x27;), loc=(1.04,0))plt.title(&#x27;Proportion of 1st/2nd/3rd ticket class in family group size&#x27;)_ = plt.xticks(rotation=False)plt.tight_layout() 4) Pclass1234567891011ax = sb.countplot(total_df[&#x27;Pclass&#x27;], palette = [&#x27;#eed4d0&#x27;, &#x27;#cda0aa&#x27;, &#x27;#a2708e&#x27;])# calculate passengers for each categorylabels = (total_df[&#x27;Pclass&#x27;].value_counts(sort = False))# add result numbers on barchartfor i, v in enumerate(labels): ax.text(i, v+2, str(v), horizontalalignment = &#x27;center&#x27;, size = 12, color = &#x27;black&#x27;, fontweight = &#x27;bold&#x27;) plt.title(&#x27;Passengers distribution by Pclass&#x27;)plt.ylabel(&#x27;Number of passengers&#x27;)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning 1234567891011121314151617fig = plt.figure(figsize=(14, 5))ax1 = fig.add_subplot(121)sb.countplot(x = &#x27;Pclass&#x27;, hue = &#x27;Survived&#x27;, data = total_df, palette=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], ax = ax1)plt.title(&#x27;Number of survived/drowned passengers by class (train data)&#x27;)plt.ylabel(&#x27;Number of passengers&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))_ = plt.xticks(rotation=False)ax2 = fig.add_subplot(122)d = total_df.groupby(&#x27;Pclass&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True).unstack()d.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, ax = ax2, color =[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;])plt.title(&#x27;Proportion of survived/drowned passengers by class (train data)&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))_ = plt.xticks(rotation=False)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): 가장 많은 승객이 탄 3등급 임에도 불구하고 생존율은 가장 적은 승객이 탑승한 1등급에 비해 더 적은 생존율을 보인다. 4-1) Pclass vs Surviving vs Sex123sb.catplot(x = &#x27;Pclass&#x27;, hue = &#x27;Survived&#x27;, col = &#x27;Sex&#x27;, kind = &#x27;count&#x27;, data = total_df , palette=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;])plt.tight_layout() 1등급 클래스의 남성 승객들의 대부분은 살아남지 못하였고 여성들은 대부분 살아남았다. 3등급 클래스의 여성의 절반 이상은 살아남았다. 5) Embarked12345678910111213141516171819202122232425262728293031323334fig = plt.figure(figsize = (15,4))ax1 = fig.add_subplot(131)palette = sb.cubehelix_palette(5, start = 2)ax = sb.countplot(total_df[&#x27;Embarked&#x27;], palette = palette, order = [&#x27;C&#x27;, &#x27;Q&#x27;, &#x27;S&#x27;], ax = ax1)plt.title(&#x27;Number of passengers by Embarked&#x27;)plt.ylabel(&#x27;Number of passengers&#x27;)# calculate passengers for each categorylabels = (total_df[&#x27;Embarked&#x27;].value_counts())labels = labels.sort_index()# add result numbers on barchartfor i, v in enumerate(labels): ax.text(i, v+10, str(v), horizontalalignment = &#x27;center&#x27;, size = 10, color = &#x27;black&#x27;) ax2 = fig.add_subplot(132)surv_by_emb = total_df.groupby(&#x27;Embarked&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True)surv_by_emb = surv_by_emb.unstack().sort_index()surv_by_emb.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], ax = ax2)plt.title(&#x27;Proportion of survived/drowned passengers by Embarked (train data)&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))_ = plt.xticks(rotation=False)ax3 = fig.add_subplot(133)class_by_emb = total_df.groupby(&#x27;Embarked&#x27;)[&#x27;Pclass&#x27;].value_counts(normalize = True)class_by_emb = class_by_emb.unstack().sort_index()class_by_emb.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color = [&#x27;#eed4d0&#x27;, &#x27;#cda0aa&#x27;, &#x27;#a2708e&#x27;], ax = ax3)plt.legend((&#x27;1st class&#x27;, &#x27;2nd class&#x27;, &#x27;3rd class&#x27;), loc=(1.04,0))plt.title(&#x27;Proportion of clases by Embarked&#x27;)_ = plt.xticks(rotation=False)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): 대부분의 승객(140981)들은 S 항구에서 출발하였고 S 항구에서 출발한 승객들의 생존율은 가장 낮았다. 또한 3등급 클래스 사람들이 대부분이다. C항구에서 출발한 승객들은 75% 이상의 생존율을 보인다. 가장 적은 승객들이 탑승한 Q항구에는 가장 많은 l등급 클래스의 승객들이 탑승하였다. 1234sb.catplot(x=&quot;Embarked&quot;, y=&quot;Fare&quot;, kind=&quot;violin&quot;, inner=None, data=total_df, height = 6, palette = palette, order = [&#x27;C&#x27;, &#x27;Q&#x27;, &#x27;S&#x27;])plt.title(&#x27;Distribution of Fare by Embarked&#x27;)plt.tight_layout() 1pd.DataFrame(total_df.groupby(&#x27;Embarked&#x27;)[&#x27;Fare&#x27;].describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Embarked C 44440.0 73.673693 87.985577 1.51 13.54 31.42 89.5025 744.46 Q 13977.0 78.479737 92.941969 2.29 10.81 28.45 115.6700 744.66 S 140791.0 32.125407 50.934735 0.05 9.50 13.24 29.9900 727.65 6) Fare123fig, ax = plt.subplots(1, 1, figsize=(8, 8))g = sb.distplot(total_df[&#x27;Fare&#x27;], color=&#x27;r&#x27;, label=&#x27;Skewness : &#123;:.2f&#125;&#x27;.format(total_df[&#x27;Fare&#x27;].skew()), ax=ax)g = g.legend(loc=&#x27;best&#x27;) /opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) 1234567fare_map = total_df[[&#x27;Fare&#x27;, &#x27;Pclass&#x27;]].dropna().groupby(&#x27;Pclass&#x27;).median().to_dict()total_df[&#x27;Fare&#x27;] = total_df[&#x27;Fare&#x27;].fillna(total_df[&#x27;Pclass&#x27;].map(fare_map[&#x27;Fare&#x27;]))total_df[&#x27;Fare&#x27;] = total_df[&#x27;Fare&#x27;].map(lambda i: np.log(i) if i &gt; 0 else 0)fig, ax = plt.subplots(1, 1, figsize=(8, 8))g = sb.distplot(total_df[&#x27;Fare&#x27;], color=&#x27;b&#x27;, label=&#x27;Skewness : &#123;:.2f&#125;&#x27;.format(total_df[&#x27;Fare&#x27;].skew()), ax=ax)g = g.legend(loc=&#x27;best&#x27;) /opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) Feature Engineering Null values 확인 1total_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Family_size Family_size_group 0 0 1.0 1 Oconnor, Frankie male NaN 2 0 209245 3.301009 C S 3 f_usual 1 1 0.0 3 Bryan, Drew male NaN 0 0 27323 2.591516 X S 1 f_single 2 2 0.0 3 Owens, Kenneth male 0.33 1 2 CA 457703 4.266756 X S 4 f_usual 3 3 0.0 3 Kramer, James male 19.00 0 0 A. 10866 2.568022 X S 1 f_single 4 4 1.0 3 Bond, Michael male 25.00 0 0 427635 2.048982 X S 1 f_single ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 199995 NaN 3 Cash, Cheryle female 27.00 0 0 7686 2.314514 X Q 1 f_single 99996 199996 NaN 1 Brown, Howard male 59.00 1 0 13004 4.224056 X S 2 f_usual 99997 199997 NaN 3 Lightfoot, Cameron male 47.00 0 0 4383317 2.386007 X S 1 f_single 99998 199998 NaN 1 Jacobsen, Margaret female 49.00 1 2 PC 26988 3.390473 B C 4 f_usual 99999 199999 NaN 1 Fishback, Joanna female 41.00 0 2 PC 41824 5.275100 E C 3 f_usual 200000 rows × 14 columns 1total_df.isna().sum() PassengerId 0 Survived 100000 Pclass 0 Name 0 Sex 0 Age 6779 SibSp 0 Parch 0 Ticket 9804 Fare 0 Cabin 0 Embarked 527 Family_size 0 Family_size_group 0 dtype: int64 1) Data Correlation12345678910111213141516fig, ax=plt.subplots(1, 3, figsize=(17,5))feature_lst=[&#x27;Pclass&#x27;,&#x27;Age&#x27;,&#x27;Fare&#x27;,&#x27;Sex&#x27;,&#x27;Family_size&#x27;]corr=total_df[feature_lst].corr()mask=np.zeros_like(corr, dtype=np.bool)mask[np.triu_indices_from(mask)]=Truefor idx, method in enumerate([&#x27;pearson&#x27;,&#x27;kendall&#x27;,&#x27;spearman&#x27;]): sb.heatmap(total_df[feature_lst].corr(method=method), ax=ax[idx], square=True, annot=True, fmt=&#x27;.2f&#x27;, center=0, linewidth=2, cbar=False, cmap=sb.diverging_palette(240, 10, as_cmap=True), mask=mask) ax[idx].set_title(f&#x27;&#123;method.capitalize()&#125; Correlation&#x27;, loc=&#x27;left&#x27;, fontweight=&#x27;bold&#x27;) plt.show() 2) Age 각 클래스마다 나이의 평균을 각 클래스마다의 null 값에 넣어주었다. 12age_map= total_df[[&#x27;Age&#x27;,&#x27;Pclass&#x27;]].dropna().groupby(&#x27;Pclass&#x27;).median().to_dict()total_df[&#x27;Age&#x27;]=total_df[&#x27;Age&#x27;].fillna(total_df[&#x27;Pclass&#x27;].map(age_map[&#x27;Age&#x27;])) 3) Embarked1print(&#x27;Embarked has &#x27;, sum(total_df[&#x27;Embarked&#x27;].isnull()), &#x27; Null values&#x27;) Embarked has 527 Null values 1total_df[&#x27;Embarked&#x27;] = total_df[&#x27;Embarked&#x27;].fillna(&#x27;S&#x27;) 1total_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Family_size Family_size_group 0 0 1.0 1 Oconnor, Frankie male 41.00 2 0 209245 3.301009 C S 3 f_usual 1 1 0.0 3 Bryan, Drew male 26.00 0 0 27323 2.591516 X S 1 f_single 2 2 0.0 3 Owens, Kenneth male 0.33 1 2 CA 457703 4.266756 X S 4 f_usual 3 3 0.0 3 Kramer, James male 19.00 0 0 A. 10866 2.568022 X S 1 f_single 4 4 1.0 3 Bond, Michael male 25.00 0 0 427635 2.048982 X S 1 f_single ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 199995 NaN 3 Cash, Cheryle female 27.00 0 0 7686 2.314514 X Q 1 f_single 99996 199996 NaN 1 Brown, Howard male 59.00 1 0 13004 4.224056 X S 2 f_usual 99997 199997 NaN 3 Lightfoot, Cameron male 47.00 0 0 4383317 2.386007 X S 1 f_single 99998 199998 NaN 1 Jacobsen, Margaret female 49.00 1 2 PC 26988 3.390473 B C 4 f_usual 99999 199999 NaN 1 Fishback, Joanna female 41.00 0 2 PC 41824 5.275100 E C 3 f_usual 200000 rows × 14 columns 4) Name1total_df[&#x27;Name&#x27;] = total_df[&#x27;Name&#x27;].map(lambda x: x.split(&#x27;,&#x27;)[0]) 5) Ticket1total_df[&#x27;Ticket&#x27;] = total_df[&#x27;Ticket&#x27;].fillna(&#x27;X&#x27;).map(lambda x:str(x).split()[0] if len(str(x).split()) &gt; 1 else &#x27;X&#x27;) 6) Drop PassengerId, Name, SibSp, Parch, Cabin 1total_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Family_size Family_size_group 0 0 1.0 1 Oconnor male 41.00 2 0 X 3.301009 C S 3 f_usual 1 1 0.0 3 Bryan male 26.00 0 0 X 2.591516 X S 1 f_single 2 2 0.0 3 Owens male 0.33 1 2 CA 4.266756 X S 4 f_usual 3 3 0.0 3 Kramer male 19.00 0 0 A. 2.568022 X S 1 f_single 4 4 1.0 3 Bond male 25.00 0 0 X 2.048982 X S 1 f_single ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 199995 NaN 3 Cash female 27.00 0 0 X 2.314514 X Q 1 f_single 99996 199996 NaN 1 Brown male 59.00 1 0 X 4.224056 X S 2 f_usual 99997 199997 NaN 3 Lightfoot male 47.00 0 0 X 2.386007 X S 1 f_single 99998 199998 NaN 1 Jacobsen female 49.00 1 2 PC 3.390473 B C 4 f_usual 99999 199999 NaN 1 Fishback female 41.00 0 2 PC 5.275100 E C 3 f_usual 200000 rows × 14 columns 12total_df.drop([&#x27;PassengerId&#x27;,&#x27;Name&#x27;,&#x27;Family_size_group&#x27;,&#x27;Family_size&#x27;], axis=1, inplace=True)total_df.shape (200000, 10) 12345total_df[&#x27;Sex&#x27;]=total_df[&#x27;Sex&#x27;].map(&#123;&#x27;female&#x27;:0, &#x27;male&#x27;:1&#125;)total_df=pd.get_dummies(total_df, columns=[&#x27;Embarked&#x27;], prefix=&#x27;Embarked&#x27;)total_df=pd.get_dummies(total_df, columns=[&#x27;Cabin&#x27;], prefix=&#x27;Cabin&#x27;)total_df=pd.get_dummies(total_df, columns=[&#x27;Ticket&#x27;], prefix=&#x27;Ticket&#x27;)#total_df=pd.get_dummies(total_df, columns=[&#x27;Family_size_group&#x27;], prefix=&#x27;Family_size_group&#x27;) 1total_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Survived Pclass Sex Age SibSp Parch Fare Embarked_C Embarked_Q Embarked_S ... Ticket_SOTON/OQ Ticket_STON/O Ticket_STON/O2. Ticket_STON/OQ. Ticket_SW/PP Ticket_W./C. Ticket_W.E.P. Ticket_W/C Ticket_WE/P Ticket_X 0 1.0 1 1 41.00 2 0 3.301009 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 1 0.0 3 1 26.00 0 0 2.591516 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 2 0.0 3 1 0.33 1 2 4.266756 0 0 1 ... 0 0 0 0 0 0 0 0 0 0 3 0.0 3 1 19.00 0 0 2.568022 0 0 1 ... 0 0 0 0 0 0 0 0 0 0 4 1.0 3 1 25.00 0 0 2.048982 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 NaN 3 0 27.00 0 0 2.314514 0 1 0 ... 0 0 0 0 0 0 0 0 0 1 99996 NaN 1 1 59.00 1 0 4.224056 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 99997 NaN 3 1 47.00 0 0 2.386007 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 99998 NaN 1 0 49.00 1 2 3.390473 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 99999 NaN 1 0 41.00 0 2 5.275100 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 200000 rows × 69 columns Split data123456X = total_df[:train.shape[0]]print(&quot;X Shape is:&quot;, X.shape)y = X[&#x27;Survived&#x27;]X.drop([&#x27;Survived&#x27;], axis=1, inplace=True)test_data = total_df[train.shape[0]:].drop(columns=[&#x27;Survived&#x27;])test_data.info() X Shape is: (100000, 69) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 100000 entries, 0 to 99999 Data columns (total 68 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Pclass 100000 non-null int64 1 Sex 100000 non-null int64 2 Age 100000 non-null float64 3 SibSp 100000 non-null int64 4 Parch 100000 non-null int64 5 Fare 100000 non-null float64 6 Embarked_C 100000 non-null uint8 7 Embarked_Q 100000 non-null uint8 8 Embarked_S 100000 non-null uint8 9 Cabin_A 100000 non-null uint8 10 Cabin_B 100000 non-null uint8 11 Cabin_C 100000 non-null uint8 12 Cabin_D 100000 non-null uint8 13 Cabin_E 100000 non-null uint8 14 Cabin_F 100000 non-null uint8 15 Cabin_G 100000 non-null uint8 16 Cabin_T 100000 non-null uint8 17 Cabin_X 100000 non-null uint8 18 Ticket_A. 100000 non-null uint8 19 Ticket_A./5. 100000 non-null uint8 20 Ticket_A.5. 100000 non-null uint8 21 Ticket_A/4 100000 non-null uint8 22 Ticket_A/4. 100000 non-null uint8 23 Ticket_A/5 100000 non-null uint8 24 Ticket_A/5. 100000 non-null uint8 25 Ticket_A/S 100000 non-null uint8 26 Ticket_A4. 100000 non-null uint8 27 Ticket_AQ/3. 100000 non-null uint8 28 Ticket_AQ/4 100000 non-null uint8 29 Ticket_C 100000 non-null uint8 30 Ticket_C.A. 100000 non-null uint8 31 Ticket_C.A./SOTON 100000 non-null uint8 32 Ticket_CA 100000 non-null uint8 33 Ticket_CA. 100000 non-null uint8 34 Ticket_F.C. 100000 non-null uint8 35 Ticket_F.C.C. 100000 non-null uint8 36 Ticket_Fa 100000 non-null uint8 37 Ticket_LP 100000 non-null uint8 38 Ticket_P/PP 100000 non-null uint8 39 Ticket_PC 100000 non-null uint8 40 Ticket_PP 100000 non-null uint8 41 Ticket_S.C./A.4. 100000 non-null uint8 42 Ticket_S.C./PARIS 100000 non-null uint8 43 Ticket_S.O./P.P. 100000 non-null uint8 44 Ticket_S.O.C. 100000 non-null uint8 45 Ticket_S.O.P. 100000 non-null uint8 46 Ticket_S.P. 100000 non-null uint8 47 Ticket_S.W./PP 100000 non-null uint8 48 Ticket_SC 100000 non-null uint8 49 Ticket_SC/A.3 100000 non-null uint8 50 Ticket_SC/A4 100000 non-null uint8 51 Ticket_SC/AH 100000 non-null uint8 52 Ticket_SC/PARIS 100000 non-null uint8 53 Ticket_SC/Paris 100000 non-null uint8 54 Ticket_SCO/W 100000 non-null uint8 55 Ticket_SO/C 100000 non-null uint8 56 Ticket_SOTON/O.Q. 100000 non-null uint8 57 Ticket_SOTON/O2 100000 non-null uint8 58 Ticket_SOTON/OQ 100000 non-null uint8 59 Ticket_STON/O 100000 non-null uint8 60 Ticket_STON/O2. 100000 non-null uint8 61 Ticket_STON/OQ. 100000 non-null uint8 62 Ticket_SW/PP 100000 non-null uint8 63 Ticket_W./C. 100000 non-null uint8 64 Ticket_W.E.P. 100000 non-null uint8 65 Ticket_W/C 100000 non-null uint8 66 Ticket_WE/P 100000 non-null uint8 67 Ticket_X 100000 non-null uint8 dtypes: float64(2), int64(4), uint8(62) memory usage: 11.3 MB /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:4315: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, 12345from sklearn.model_selection import train_test_split#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state=42)X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, stratify = X[[&#x27;Pclass&#x27;]], random_state=42)X_train.shape, X_valid.shape, y_train.shape, y_valid.shape ((70000, 68), (30000, 68), (70000,), (30000,)) Modeling referenceshttps://www.kaggle.com/j2hoon85/tps-april-sklearn-pycaret-for-newbies XGBoost Shap123456789101112131415import xgboostimport shap# train an XGBoost modelxgb_model = xgboost.XGBClassifier().fit(X_train, y_train)# explain the model&#x27;s predictions using SHAP# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)explainer = shap.Explainer(xgb_model)shap_values = explainer(X)# visualize the first prediction&#x27;s explanationshap.plots.waterfall(shap_values[0])shap.plots.bar(shap_values) The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. [06:00:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ntree_limit is deprecated, use `iteration_range` or model slicing instead. Decision Tree123from sklearn.metrics import accuracy_scoredef acc_score(y_true, y_pred, **kwargs): return accuracy_score(y_true, (y_pred &gt; 0.5).astype(int), **kwargs) 123456789101112131415161718192021222324252627from sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import roc_curve, roc_auc_scorefrom matplotlib import pyplot as plttree_model = DecisionTreeClassifier(max_depth=7)tree_model.fit(X_train, y_train)predictions = tree_model.predict_proba(X_valid)AUC = roc_auc_score(y_valid, predictions[:,1])ACC = acc_score(y_valid, predictions[:,1])print(&quot;Model AUC:&quot;, AUC)print(&quot;Model Accurarcy:&quot;, ACC)print(&quot;\\n&quot;)fpr, tpr, _ = roc_curve(y_valid, predictions[:,1])fig, ax = plt.subplots(figsize=(10, 6))ax.plot(fpr, tpr)ax.text(x = 0.3, y = 0.4, s = &quot;Model AUC is &#123;&#125;\\n\\nModel Accuracy is &#123;&#125;&quot;.format(np.round(AUC, 2), np.round(ACC, 2)), fontsize=16, bbox=dict(facecolor=&#x27;gray&#x27;, alpha=0.3))ax.set_xlabel(&#x27;FPR&#x27;)ax.set_ylabel(&#x27;TPR&#x27;)ax.set_title(&#x27;ROC curve&#x27;)plt.show() Model AUC: 0.8505605139706627 Model Accurarcy: 0.7833333333333333 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn import treefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrixfrom matplotlib import pyplot as pltSEED = 0class sk_helper(object): def __init__(self, model, seed=0, params=&#123;&#125;): params[&#x27;random_state&#x27;]=seed self.model=model(**params) self.model_name=str(model).split(&#x27;.&#x27;)[-1][:-2] def train(self, X_train, y_train): self.model.fit(X_train, y_train) def predict(self, y_valid): return self.model.predict(y_valid) def fit(self, x, y): return self.model.fit(x,y) # feature importance def feature_importances(self, X_train, y_train): return self.model.fit(X_train, y_train).feature_importances_ # roc_curve def roc_curve_graph(self, X_train, y_train, X_valid, y_valid): self.model.fit(X_train, y_train) print(&quot;model_name:&quot;, self.model_name) model_name = self.model_name preds_proba = self.model.predict_proba(X_valid) preds = (preds_proba[:, 1] &gt; 0.5).astype(int) auc = roc_auc_score(y_valid, preds_proba[:, 1]) acc = accuracy_score(y_valid, preds) confusion = confusion_matrix(y_valid, preds) print(&#x27;Confusion Matrix&#x27;) print(confusion) print(&quot;Model AUC: &#123;0:.3f&#125;, Model Accuracy: &#123;1:.3f&#125;\\n&quot;.format(auc, acc)) fpr, tpr, _ = roc_curve(y_valid, preds_proba[:,1]) fig, ax = plt.subplots(figsize=(10, 6)) ax.plot(fpr, tpr) ax.text(x = 0.3, y = 0.4, s = &quot;Model AUC is &#123;&#125;\\n\\nModel Accuracy is &#123;&#125;&quot;.format(np.round(auc, 2), np.round(acc, 2)), fontsize=16, bbox=dict(facecolor=&#x27;gray&#x27;, alpha=0.3)) ax.set_xlabel(&#x27;FPR&#x27;) ax.set_ylabel(&#x27;TPR&#x27;) ax.set_title(&#x27;ROC curve of &#123;&#125;&#x27;.format(model_name), fontsize=16) plt.show() 1234567891011121314151617%%timefrom sklearn.ensemble import RandomForestClassifierrf_params = &#123; &#x27;n_jobs&#x27;: -1, &#x27;n_estimators&#x27;: 500, &#x27;warm_start&#x27;: True, #&#x27;max_features&#x27;: 0.2, &#x27;max_depth&#x27;: 10, &#x27;min_samples_leaf&#x27;: 2, &#x27;max_features&#x27; : &#x27;sqrt&#x27;, &#x27;verbose&#x27;: 1&#125;rf_model=sk_helper(model=RandomForestClassifier, seed=SEED, params=rf_params)rf_model.roc_curve_graph(X_train, y_train, X_valid, y_valid) [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 0.8s [Parallel(n_jobs=-1)]: Done 192 tasks | elapsed: 3.5s [Parallel(n_jobs=-1)]: Done 442 tasks | elapsed: 8.2s [Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 9.3s finished [Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 42 tasks | elapsed: 0.1s model_name: RandomForestClassifier [Parallel(n_jobs=4)]: Done 192 tasks | elapsed: 0.3s [Parallel(n_jobs=4)]: Done 442 tasks | elapsed: 0.6s [Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed: 0.7s finished Confusion Matrix [[14356 2879] [ 3582 9183]] Model AUC: 0.855, Model Accuracy: 0.785 CPU times: user 38.2 s, sys: 250 ms, total: 38.4 s Wall time: 10.5 s 1234567891011121314151617181920212223%%timeimport lightgbm as lgblgb_params = &#123; &#x27;metric&#x27;: &#x27;binary_logloss&#x27;, &#x27;n_estimators&#x27;: 1000, &#x27;objective&#x27;: &#x27;binary&#x27;, &#x27;random_state&#x27;: 2021, &#x27;learning_rate&#x27;: 0.01, &#x27;min_child_samples&#x27;: 150, &#x27;reg_alpha&#x27;: 3e-5, &#x27;reg_lambda&#x27;: 9e-2, &#x27;num_leaves&#x27;: 20, &#x27;max_depth&#x27;: 16, &#x27;colsample_bytree&#x27;: 0.8, &#x27;subsample&#x27;: 0.8, &#x27;subsample_freq&#x27;: 2, &#x27;max_bin&#x27;: 240&#125;lgb_model=sk_helper(model=lgb.LGBMClassifier, seed=SEED, params=rf_params)lgb_model.roc_curve_graph(X_train, y_train, X_valid, y_valid) .datatable table.frame { margin-bottom: 0; } .datatable table.frame thead { border-bottom: none; } .datatable table.frame tr.coltypes td { color: #FFFFFF; line-height: 6px; padding: 0 0.5em;} .datatable .bool { background: #DDDD99; } .datatable .object { background: #565656; } .datatable .int { background: #5D9E5D; } .datatable .float { background: #4040CC; } .datatable .str { background: #CC4040; } .datatable .row_index { background: var(--jp-border-color3); border-right: 1px solid var(--jp-border-color0); color: var(--jp-ui-font-color3); font-size: 9px;} .datatable .frame tr.coltypes .row_index { background: var(--jp-border-color0);} .datatable th:nth-child(2) { padding-left: 12px; } .datatable .hellipsis { color: var(--jp-cell-editor-border-color);} .datatable .vellipsis { background: var(--jp-layout-color0); color: var(--jp-cell-editor-border-color);} .datatable .na { color: var(--jp-cell-editor-border-color); font-size: 80%;} .datatable .footer { font-size: 9px; } .datatable .frame_dimensions { background: var(--jp-border-color3); border-top: 1px solid var(--jp-border-color0); color: var(--jp-ui-font-color3); display: inline-block; opacity: 0.6; padding: 1px 10px 1px 5px;} [LightGBM] [Warning] Unknown parameter: max_features [LightGBM] [Warning] Unknown parameter: min_samples_leaf [LightGBM] [Warning] Unknown parameter: warm_start [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Unknown parameter: max_features [LightGBM] [Warning] Unknown parameter: min_samples_leaf [LightGBM] [Warning] Unknown parameter: warm_start [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Info] Number of positive: 30009, number of negative: 39991 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020202 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 543 [LightGBM] [Info] Number of data points in the train set: 70000, number of used features: 65 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.428700 -&gt; initscore=-0.287157 [LightGBM] [Info] Start training from score -0.287157 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf model_name: LGBMClassifier Confusion Matrix [[14065 3170] [ 3286 9479]] Model AUC: 0.855, Model Accuracy: 0.785 CPU times: user 5.81 s, sys: 134 ms, total: 5.94 s Wall time: 3.82 s 1!pip install catboost Requirement already satisfied: catboost in /opt/conda/lib/python3.7/site-packages (0.25.1) Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from catboost) (3.4.1) Requirement already satisfied: plotly in /opt/conda/lib/python3.7/site-packages (from catboost) (4.14.3) Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from catboost) (1.5.4) Requirement already satisfied: numpy&gt;=1.16.0 in /opt/conda/lib/python3.7/site-packages (from catboost) (1.19.5) Requirement already satisfied: pandas&gt;=0.24.0 in /opt/conda/lib/python3.7/site-packages (from catboost) (1.2.3) Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from catboost) (1.15.0) Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from catboost) (0.8.4) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.1) Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas&gt;=0.24.0-&gt;catboost) (2021.1) Requirement already satisfied: pillow&gt;=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;catboost) (7.2.0) Requirement already satisfied: pyparsing&gt;=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;catboost) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;catboost) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;catboost) (0.10.0) Requirement already satisfied: retrying&gt;=1.3.3 in /opt/conda/lib/python3.7/site-packages (from plotly-&gt;catboost) (1.3.3) 12345678910111213141516%%timefrom catboost import CatBoostClassifiercb_params = &#123; &#x27;max_depth&#x27;: 8, &#x27;learning_rate&#x27;: 0.01, &#x27;n_estimators&#x27;: 1000, &#x27;max_bin&#x27;: 280, &#x27;min_data_in_leaf&#x27;: 64, &#x27;l2_leaf_reg&#x27;: 0.01, &#x27;subsample&#x27;: 0.8&#125;cb_model=sk_helper(model=CatBoostClassifier, seed=SEED, params=cb_params)cb_model.roc_curve_graph(X_train, y_train, X_valid, y_valid) 0: learn: 0.6884406 total: 80.5ms remaining: 1m 20s 1: learn: 0.6838170 total: 104ms remaining: 51.9s 2: learn: 0.6792866 total: 120ms remaining: 40s 3: learn: 0.6749307 total: 144ms remaining: 35.9s 4: learn: 0.6705501 total: 168ms remaining: 33.4s 5: learn: 0.6662601 total: 192ms remaining: 31.8s 6: learn: 0.6621476 total: 214ms remaining: 30.4s 7: learn: 0.6580493 total: 238ms remaining: 29.5s 8: learn: 0.6541011 total: 260ms remaining: 28.6s 9: learn: 0.6502757 total: 276ms remaining: 27.3s 10: learn: 0.6465377 total: 302ms remaining: 27.1s 11: learn: 0.6427935 total: 325ms remaining: 26.8s 12: learn: 0.6391145 total: 348ms remaining: 26.4s 13: learn: 0.6356544 total: 369ms remaining: 26s 14: learn: 0.6321140 total: 393ms remaining: 25.8s 15: learn: 0.6287126 total: 417ms remaining: 25.6s 16: learn: 0.6255104 total: 436ms remaining: 25.2s 17: learn: 0.6222667 total: 458ms remaining: 25s 18: learn: 0.6190328 total: 484ms remaining: 25s 19: learn: 0.6160637 total: 498ms remaining: 24.4s 20: learn: 0.6130081 total: 532ms remaining: 24.8s 21: learn: 0.6101029 total: 553ms remaining: 24.6s 22: learn: 0.6072655 total: 578ms remaining: 24.6s 23: learn: 0.6043829 total: 602ms remaining: 24.5s 24: learn: 0.6018266 total: 620ms remaining: 24.2s 25: learn: 0.5990827 total: 644ms remaining: 24.1s 26: learn: 0.5964782 total: 668ms remaining: 24.1s 27: learn: 0.5939595 total: 688ms remaining: 23.9s 28: learn: 0.5915331 total: 704ms remaining: 23.6s 29: learn: 0.5889863 total: 728ms remaining: 23.5s 30: learn: 0.5865869 total: 753ms remaining: 23.5s 31: learn: 0.5842859 total: 776ms remaining: 23.5s 32: learn: 0.5819020 total: 801ms remaining: 23.5s 33: learn: 0.5797240 total: 825ms remaining: 23.4s 34: learn: 0.5775389 total: 848ms remaining: 23.4s 35: learn: 0.5755076 total: 864ms remaining: 23.1s 36: learn: 0.5736566 total: 878ms remaining: 22.9s 37: learn: 0.5717165 total: 902ms remaining: 22.8s 38: learn: 0.5696527 total: 925ms remaining: 22.8s 39: learn: 0.5677411 total: 947ms remaining: 22.7s 40: learn: 0.5658559 total: 971ms remaining: 22.7s 41: learn: 0.5639679 total: 994ms remaining: 22.7s 42: learn: 0.5621573 total: 1.02s remaining: 22.6s 43: learn: 0.5603957 total: 1.04s remaining: 22.6s 44: learn: 0.5585892 total: 1.07s remaining: 22.7s 45: learn: 0.5569204 total: 1.09s remaining: 22.6s 46: learn: 0.5552463 total: 1.11s remaining: 22.6s 47: learn: 0.5536250 total: 1.14s remaining: 22.6s 48: learn: 0.5520718 total: 1.16s remaining: 22.5s 49: learn: 0.5504821 total: 1.19s remaining: 22.6s 50: learn: 0.5489653 total: 1.21s remaining: 22.6s 51: learn: 0.5474109 total: 1.24s remaining: 22.6s 52: learn: 0.5459763 total: 1.26s remaining: 22.6s 53: learn: 0.5445562 total: 1.29s remaining: 22.6s 54: learn: 0.5431438 total: 1.31s remaining: 22.6s 55: learn: 0.5417719 total: 1.34s remaining: 22.6s 56: learn: 0.5404477 total: 1.36s remaining: 22.6s 57: learn: 0.5391074 total: 1.39s remaining: 22.5s 58: learn: 0.5377986 total: 1.41s remaining: 22.5s 59: learn: 0.5365282 total: 1.44s remaining: 22.5s 60: learn: 0.5353029 total: 1.46s remaining: 22.5s 61: learn: 0.5340683 total: 1.48s remaining: 22.4s 62: learn: 0.5329104 total: 1.51s remaining: 22.4s 63: learn: 0.5317566 total: 1.53s remaining: 22.4s 64: learn: 0.5306147 total: 1.55s remaining: 22.4s 65: learn: 0.5294422 total: 1.58s remaining: 22.4s 66: learn: 0.5283472 total: 1.6s remaining: 22.4s 67: learn: 0.5272628 total: 1.63s remaining: 22.4s 68: learn: 0.5262269 total: 1.66s remaining: 22.4s 69: learn: 0.5252543 total: 1.68s remaining: 22.3s 70: learn: 0.5242217 total: 1.71s remaining: 22.3s 71: learn: 0.5232597 total: 1.73s remaining: 22.3s 72: learn: 0.5222463 total: 1.76s remaining: 22.3s 73: learn: 0.5213520 total: 1.78s remaining: 22.3s 74: learn: 0.5204568 total: 1.8s remaining: 22.3s 75: learn: 0.5196299 total: 1.83s remaining: 22.2s 76: learn: 0.5187603 total: 1.85s remaining: 22.2s 77: learn: 0.5179326 total: 1.88s remaining: 22.2s 78: learn: 0.5170868 total: 1.9s remaining: 22.2s 79: learn: 0.5162588 total: 1.92s remaining: 22.1s 80: learn: 0.5154756 total: 1.95s remaining: 22.1s 81: learn: 0.5146912 total: 1.97s remaining: 22.1s 82: learn: 0.5139581 total: 2s remaining: 22.1s 83: learn: 0.5131996 total: 2.02s remaining: 22.1s 84: learn: 0.5124481 total: 2.04s remaining: 22s 85: learn: 0.5116752 total: 2.07s remaining: 22s 86: learn: 0.5109482 total: 2.1s remaining: 22s 87: learn: 0.5103112 total: 2.12s remaining: 22s 88: learn: 0.5096581 total: 2.15s remaining: 22s 89: learn: 0.5089431 total: 2.18s remaining: 22s 90: learn: 0.5082522 total: 2.2s remaining: 22s 91: learn: 0.5076517 total: 2.23s remaining: 22s 92: learn: 0.5070308 total: 2.25s remaining: 21.9s 93: learn: 0.5063997 total: 2.27s remaining: 21.9s 94: learn: 0.5058001 total: 2.3s remaining: 21.9s 95: learn: 0.5051906 total: 2.33s remaining: 21.9s 96: learn: 0.5046129 total: 2.35s remaining: 21.9s 97: learn: 0.5040508 total: 2.38s remaining: 21.9s 98: learn: 0.5034705 total: 2.4s remaining: 21.9s 99: learn: 0.5029049 total: 2.42s remaining: 21.8s 100: learn: 0.5023410 total: 2.45s remaining: 21.8s 101: learn: 0.5018054 total: 2.47s remaining: 21.8s 102: learn: 0.5012927 total: 2.5s remaining: 21.8s 103: learn: 0.5007620 total: 2.53s remaining: 21.8s 104: learn: 0.5002364 total: 2.55s remaining: 21.8s 105: learn: 0.4997446 total: 2.58s remaining: 21.7s 106: learn: 0.4992382 total: 2.6s remaining: 21.7s 107: learn: 0.4987651 total: 2.63s remaining: 21.7s 108: learn: 0.4983881 total: 2.64s remaining: 21.6s 109: learn: 0.4979742 total: 2.67s remaining: 21.6s 110: learn: 0.4975065 total: 2.7s remaining: 21.6s 111: learn: 0.4970813 total: 2.73s remaining: 21.6s 112: learn: 0.4966415 total: 2.75s remaining: 21.6s 113: learn: 0.4962375 total: 2.77s remaining: 21.6s 114: learn: 0.4958272 total: 2.8s remaining: 21.5s 115: learn: 0.4954165 total: 2.82s remaining: 21.5s 116: learn: 0.4950403 total: 2.85s remaining: 21.5s 117: learn: 0.4946756 total: 2.87s remaining: 21.5s 118: learn: 0.4943074 total: 2.9s remaining: 21.4s 119: learn: 0.4939117 total: 2.92s remaining: 21.4s 120: learn: 0.4935388 total: 2.95s remaining: 21.4s 121: learn: 0.4932137 total: 2.97s remaining: 21.4s 122: learn: 0.4928860 total: 3s remaining: 21.4s 123: learn: 0.4925254 total: 3.02s remaining: 21.4s 124: learn: 0.4922541 total: 3.04s remaining: 21.3s 125: learn: 0.4919255 total: 3.06s remaining: 21.2s 126: learn: 0.4915802 total: 3.08s remaining: 21.2s 127: learn: 0.4912600 total: 3.11s remaining: 21.2s 128: learn: 0.4909621 total: 3.13s remaining: 21.2s 129: learn: 0.4906550 total: 3.16s remaining: 21.2s 130: learn: 0.4903320 total: 3.19s remaining: 21.1s 131: learn: 0.4900083 total: 3.21s remaining: 21.1s 132: learn: 0.4896774 total: 3.24s remaining: 21.1s 133: learn: 0.4893788 total: 3.26s remaining: 21.1s 134: learn: 0.4890677 total: 3.29s remaining: 21.1s 135: learn: 0.4887482 total: 3.31s remaining: 21s 136: learn: 0.4884599 total: 3.34s remaining: 21s 137: learn: 0.4881724 total: 3.37s remaining: 21s 138: learn: 0.4878845 total: 3.39s remaining: 21s 139: learn: 0.4876288 total: 3.42s remaining: 21s 140: learn: 0.4873738 total: 3.44s remaining: 21s 141: learn: 0.4871044 total: 3.47s remaining: 20.9s 142: learn: 0.4868465 total: 3.5s remaining: 21s 143: learn: 0.4865916 total: 3.53s remaining: 21s 144: learn: 0.4863548 total: 3.57s remaining: 21.1s 145: learn: 0.4860985 total: 3.6s remaining: 21.1s 146: learn: 0.4858511 total: 3.63s remaining: 21.1s 147: learn: 0.4856274 total: 3.65s remaining: 21s 148: learn: 0.4853693 total: 3.68s remaining: 21s 149: learn: 0.4851541 total: 3.7s remaining: 21s 150: learn: 0.4849915 total: 3.72s remaining: 20.9s 151: learn: 0.4847782 total: 3.74s remaining: 20.9s 152: learn: 0.4845270 total: 3.77s remaining: 20.9s 153: learn: 0.4842832 total: 3.8s remaining: 20.9s 154: learn: 0.4840638 total: 3.82s remaining: 20.8s 155: learn: 0.4838815 total: 3.85s remaining: 20.8s 156: learn: 0.4836920 total: 3.87s remaining: 20.8s 157: learn: 0.4835030 total: 3.9s remaining: 20.8s 158: learn: 0.4832918 total: 3.92s remaining: 20.7s 159: learn: 0.4831511 total: 3.93s remaining: 20.7s 160: learn: 0.4829497 total: 3.96s remaining: 20.6s 161: learn: 0.4827754 total: 3.98s remaining: 20.6s 162: learn: 0.4825804 total: 4.01s remaining: 20.6s 163: learn: 0.4823916 total: 4.04s remaining: 20.6s 164: learn: 0.4822085 total: 4.06s remaining: 20.5s 165: learn: 0.4820429 total: 4.08s remaining: 20.5s 166: learn: 0.4818731 total: 4.11s remaining: 20.5s 167: learn: 0.4817004 total: 4.13s remaining: 20.5s 168: learn: 0.4815285 total: 4.16s remaining: 20.4s 169: learn: 0.4813635 total: 4.18s remaining: 20.4s 170: learn: 0.4812079 total: 4.2s remaining: 20.4s 171: learn: 0.4810382 total: 4.23s remaining: 20.4s 172: learn: 0.4808926 total: 4.25s remaining: 20.3s 173: learn: 0.4807381 total: 4.28s remaining: 20.3s 174: learn: 0.4805700 total: 4.3s remaining: 20.3s 175: learn: 0.4804400 total: 4.32s remaining: 20.2s 176: learn: 0.4802812 total: 4.35s remaining: 20.2s 177: learn: 0.4801280 total: 4.37s remaining: 20.2s 178: learn: 0.4799842 total: 4.39s remaining: 20.2s 179: learn: 0.4798398 total: 4.42s remaining: 20.1s 180: learn: 0.4797125 total: 4.44s remaining: 20.1s 181: learn: 0.4795689 total: 4.47s remaining: 20.1s 182: learn: 0.4794479 total: 4.49s remaining: 20s 183: learn: 0.4792997 total: 4.51s remaining: 20s 184: learn: 0.4791614 total: 4.54s remaining: 20s 185: learn: 0.4790325 total: 4.56s remaining: 20s 186: learn: 0.4789103 total: 4.59s remaining: 19.9s 187: learn: 0.4787746 total: 4.61s remaining: 19.9s 188: learn: 0.4786240 total: 4.64s remaining: 19.9s 189: learn: 0.4784893 total: 4.66s remaining: 19.9s 190: learn: 0.4783515 total: 4.69s remaining: 19.9s 191: learn: 0.4782205 total: 4.71s remaining: 19.8s 192: learn: 0.4780959 total: 4.74s remaining: 19.8s 193: learn: 0.4779926 total: 4.76s remaining: 19.8s 194: learn: 0.4778871 total: 4.78s remaining: 19.7s 195: learn: 0.4777654 total: 4.81s remaining: 19.7s 196: learn: 0.4776578 total: 4.83s remaining: 19.7s 197: learn: 0.4775341 total: 4.85s remaining: 19.7s 198: learn: 0.4773985 total: 4.88s remaining: 19.6s 199: learn: 0.4772849 total: 4.91s remaining: 19.6s 200: learn: 0.4771735 total: 4.93s remaining: 19.6s 201: learn: 0.4770525 total: 4.95s remaining: 19.6s 202: learn: 0.4769315 total: 4.98s remaining: 19.5s 203: learn: 0.4768213 total: 5s remaining: 19.5s 204: learn: 0.4767196 total: 5.03s remaining: 19.5s 205: learn: 0.4766273 total: 5.05s remaining: 19.5s 206: learn: 0.4765335 total: 5.07s remaining: 19.4s 207: learn: 0.4764282 total: 5.1s remaining: 19.4s 208: learn: 0.4763482 total: 5.12s remaining: 19.4s 209: learn: 0.4762623 total: 5.15s remaining: 19.4s 210: learn: 0.4761735 total: 5.17s remaining: 19.3s 211: learn: 0.4760733 total: 5.19s remaining: 19.3s 212: learn: 0.4759979 total: 5.22s remaining: 19.3s 213: learn: 0.4758779 total: 5.24s remaining: 19.3s 214: learn: 0.4757827 total: 5.26s remaining: 19.2s 215: learn: 0.4757053 total: 5.29s remaining: 19.2s 216: learn: 0.4756008 total: 5.32s remaining: 19.2s 217: learn: 0.4755150 total: 5.34s remaining: 19.2s 218: learn: 0.4754208 total: 5.36s remaining: 19.1s 219: learn: 0.4753210 total: 5.39s remaining: 19.1s 220: learn: 0.4752463 total: 5.41s remaining: 19.1s 221: learn: 0.4751665 total: 5.44s remaining: 19.1s 222: learn: 0.4750788 total: 5.46s remaining: 19s 223: learn: 0.4749864 total: 5.49s remaining: 19s 224: learn: 0.4748725 total: 5.51s remaining: 19s 225: learn: 0.4747832 total: 5.54s remaining: 19s 226: learn: 0.4747065 total: 5.56s remaining: 18.9s 227: learn: 0.4746132 total: 5.59s remaining: 18.9s 228: learn: 0.4745160 total: 5.61s remaining: 18.9s 229: learn: 0.4744390 total: 5.64s remaining: 18.9s 230: learn: 0.4743604 total: 5.66s remaining: 18.9s 231: learn: 0.4742583 total: 5.69s remaining: 18.8s 232: learn: 0.4741847 total: 5.71s remaining: 18.8s 233: learn: 0.4740959 total: 5.74s remaining: 18.8s 234: learn: 0.4740094 total: 5.76s remaining: 18.8s 235: learn: 0.4739407 total: 5.79s remaining: 18.7s 236: learn: 0.4738660 total: 5.81s remaining: 18.7s 237: learn: 0.4737837 total: 5.84s remaining: 18.7s 238: learn: 0.4737104 total: 5.86s remaining: 18.7s 239: learn: 0.4736311 total: 5.89s remaining: 18.6s 240: learn: 0.4735539 total: 5.91s remaining: 18.6s 241: learn: 0.4734807 total: 5.94s remaining: 18.6s 242: learn: 0.4734141 total: 5.96s remaining: 18.6s 243: learn: 0.4733445 total: 5.99s remaining: 18.6s 244: learn: 0.4732736 total: 6.01s remaining: 18.5s 245: learn: 0.4731923 total: 6.04s remaining: 18.5s 246: learn: 0.4731118 total: 6.06s remaining: 18.5s 247: learn: 0.4730604 total: 6.08s remaining: 18.5s 248: learn: 0.4729942 total: 6.13s remaining: 18.5s 249: learn: 0.4729268 total: 6.23s remaining: 18.7s 250: learn: 0.4728592 total: 6.29s remaining: 18.8s 251: learn: 0.4727932 total: 6.32s remaining: 18.7s 252: learn: 0.4727464 total: 6.34s remaining: 18.7s 253: learn: 0.4726803 total: 6.37s remaining: 18.7s 254: learn: 0.4726170 total: 6.39s remaining: 18.7s 255: learn: 0.4725995 total: 6.4s remaining: 18.6s 256: learn: 0.4725542 total: 6.43s remaining: 18.6s 257: learn: 0.4724986 total: 6.45s remaining: 18.6s 258: learn: 0.4724363 total: 6.48s remaining: 18.5s 259: learn: 0.4723640 total: 6.5s remaining: 18.5s 260: learn: 0.4723009 total: 6.53s remaining: 18.5s 261: learn: 0.4722426 total: 6.55s remaining: 18.5s 262: learn: 0.4721912 total: 6.58s remaining: 18.4s 263: learn: 0.4721549 total: 6.6s remaining: 18.4s 264: learn: 0.4720826 total: 6.63s remaining: 18.4s 265: learn: 0.4720172 total: 6.65s remaining: 18.3s 266: learn: 0.4719535 total: 6.67s remaining: 18.3s 267: learn: 0.4719023 total: 6.7s remaining: 18.3s 268: learn: 0.4718534 total: 6.72s remaining: 18.3s 269: learn: 0.4717885 total: 6.74s remaining: 18.2s 270: learn: 0.4717176 total: 6.77s remaining: 18.2s 271: learn: 0.4716617 total: 6.79s remaining: 18.2s 272: learn: 0.4715830 total: 6.82s remaining: 18.2s 273: learn: 0.4715357 total: 6.84s remaining: 18.1s 274: learn: 0.4714868 total: 6.87s remaining: 18.1s 275: learn: 0.4714388 total: 6.89s remaining: 18.1s 276: learn: 0.4713894 total: 6.91s remaining: 18s 277: learn: 0.4713289 total: 6.94s remaining: 18s 278: learn: 0.4712782 total: 6.96s remaining: 18s 279: learn: 0.4712199 total: 7s remaining: 18s 280: learn: 0.4711798 total: 7.03s remaining: 18s 281: learn: 0.4711284 total: 7.06s remaining: 18s 282: learn: 0.4710726 total: 7.09s remaining: 18s 283: learn: 0.4710250 total: 7.13s remaining: 18s 284: learn: 0.4709614 total: 7.16s remaining: 18s 285: learn: 0.4709017 total: 7.19s remaining: 18s 286: learn: 0.4708453 total: 7.22s remaining: 17.9s 287: learn: 0.4707988 total: 7.25s remaining: 17.9s 288: learn: 0.4707513 total: 7.28s remaining: 17.9s 289: learn: 0.4706960 total: 7.32s remaining: 17.9s 290: learn: 0.4706629 total: 7.35s remaining: 17.9s 291: learn: 0.4706212 total: 7.38s remaining: 17.9s 292: learn: 0.4705679 total: 7.41s remaining: 17.9s 293: learn: 0.4705116 total: 7.45s remaining: 17.9s 294: learn: 0.4704656 total: 7.48s remaining: 17.9s 295: learn: 0.4704126 total: 7.51s remaining: 17.9s 296: learn: 0.4703708 total: 7.54s remaining: 17.9s 297: learn: 0.4703217 total: 7.57s remaining: 17.8s 298: learn: 0.4702804 total: 7.6s remaining: 17.8s 299: learn: 0.4702280 total: 7.63s remaining: 17.8s 300: learn: 0.4701886 total: 7.66s remaining: 17.8s 301: learn: 0.4701525 total: 7.69s remaining: 17.8s 302: learn: 0.4701154 total: 7.71s remaining: 17.7s 303: learn: 0.4700790 total: 7.75s remaining: 17.7s 304: learn: 0.4700324 total: 7.78s remaining: 17.7s 305: learn: 0.4699747 total: 7.81s remaining: 17.7s 306: learn: 0.4699302 total: 7.83s remaining: 17.7s 307: learn: 0.4698923 total: 7.86s remaining: 17.7s 308: learn: 0.4698518 total: 7.89s remaining: 17.6s 309: learn: 0.4698233 total: 7.92s remaining: 17.6s 310: learn: 0.4697744 total: 7.95s remaining: 17.6s 311: learn: 0.4697332 total: 7.98s remaining: 17.6s 312: learn: 0.4696965 total: 8.01s remaining: 17.6s 313: learn: 0.4696554 total: 8.03s remaining: 17.6s 314: learn: 0.4696063 total: 8.06s remaining: 17.5s 315: learn: 0.4695670 total: 8.09s remaining: 17.5s 316: learn: 0.4695291 total: 8.12s remaining: 17.5s 317: learn: 0.4694878 total: 8.15s remaining: 17.5s 318: learn: 0.4694437 total: 8.17s remaining: 17.4s 319: learn: 0.4694075 total: 8.2s remaining: 17.4s 320: learn: 0.4693653 total: 8.22s remaining: 17.4s 321: learn: 0.4693186 total: 8.24s remaining: 17.4s 322: learn: 0.4692856 total: 8.27s remaining: 17.3s 323: learn: 0.4692345 total: 8.29s remaining: 17.3s 324: learn: 0.4692009 total: 8.31s remaining: 17.3s 325: learn: 0.4691615 total: 8.34s remaining: 17.2s 326: learn: 0.4691261 total: 8.36s remaining: 17.2s 327: learn: 0.4690797 total: 8.38s remaining: 17.2s 328: learn: 0.4690424 total: 8.41s remaining: 17.1s 329: learn: 0.4689856 total: 8.43s remaining: 17.1s 330: learn: 0.4689386 total: 8.45s remaining: 17.1s 331: learn: 0.4689094 total: 8.47s remaining: 17.1s 332: learn: 0.4688729 total: 8.5s remaining: 17s 333: learn: 0.4688315 total: 8.52s remaining: 17s 334: learn: 0.4687920 total: 8.55s remaining: 17s 335: learn: 0.4687552 total: 8.57s remaining: 16.9s 336: learn: 0.4687154 total: 8.6s remaining: 16.9s 337: learn: 0.4686667 total: 8.62s remaining: 16.9s 338: learn: 0.4686390 total: 8.65s remaining: 16.9s 339: learn: 0.4685975 total: 8.67s remaining: 16.8s 340: learn: 0.4685549 total: 8.69s remaining: 16.8s 341: learn: 0.4685488 total: 8.7s remaining: 16.7s 342: learn: 0.4685116 total: 8.73s remaining: 16.7s 343: learn: 0.4684789 total: 8.75s remaining: 16.7s 344: learn: 0.4684390 total: 8.77s remaining: 16.7s 345: learn: 0.4683998 total: 8.8s remaining: 16.6s 346: learn: 0.4683660 total: 8.82s remaining: 16.6s 347: learn: 0.4683321 total: 8.85s remaining: 16.6s 348: learn: 0.4682982 total: 8.87s remaining: 16.5s 349: learn: 0.4682504 total: 8.89s remaining: 16.5s 350: learn: 0.4682185 total: 8.91s remaining: 16.5s 351: learn: 0.4681818 total: 8.94s remaining: 16.5s 352: learn: 0.4681308 total: 8.96s remaining: 16.4s 353: learn: 0.4680877 total: 8.99s remaining: 16.4s 354: learn: 0.4680679 total: 9.01s remaining: 16.4s 355: learn: 0.4680274 total: 9.03s remaining: 16.3s 356: learn: 0.4679898 total: 9.05s remaining: 16.3s 357: learn: 0.4679634 total: 9.08s remaining: 16.3s 358: learn: 0.4679364 total: 9.1s remaining: 16.3s 359: learn: 0.4679083 total: 9.13s remaining: 16.2s 360: learn: 0.4678759 total: 9.15s remaining: 16.2s 361: learn: 0.4678355 total: 9.17s remaining: 16.2s 362: learn: 0.4677858 total: 9.2s remaining: 16.1s 363: learn: 0.4677275 total: 9.22s remaining: 16.1s 364: learn: 0.4676922 total: 9.25s remaining: 16.1s 365: learn: 0.4676488 total: 9.27s remaining: 16.1s 366: learn: 0.4676148 total: 9.29s remaining: 16s 367: learn: 0.4675839 total: 9.32s remaining: 16s 368: learn: 0.4675372 total: 9.34s remaining: 16s 369: learn: 0.4675135 total: 9.36s remaining: 15.9s 370: learn: 0.4674817 total: 9.39s remaining: 15.9s 371: learn: 0.4674287 total: 9.41s remaining: 15.9s 372: learn: 0.4674151 total: 9.43s remaining: 15.9s 373: learn: 0.4673884 total: 9.45s remaining: 15.8s 374: learn: 0.4673525 total: 9.48s remaining: 15.8s 375: learn: 0.4673205 total: 9.5s remaining: 15.8s 376: learn: 0.4672887 total: 9.52s remaining: 15.7s 377: learn: 0.4672546 total: 9.55s remaining: 15.7s 378: learn: 0.4672294 total: 9.57s remaining: 15.7s 379: learn: 0.4671776 total: 9.6s remaining: 15.7s 380: learn: 0.4671438 total: 9.62s remaining: 15.6s 381: learn: 0.4670930 total: 9.65s remaining: 15.6s 382: learn: 0.4670640 total: 9.67s remaining: 15.6s 383: learn: 0.4670421 total: 9.69s remaining: 15.5s 384: learn: 0.4670226 total: 9.71s remaining: 15.5s 385: learn: 0.4669924 total: 9.74s remaining: 15.5s 386: learn: 0.4669590 total: 9.76s remaining: 15.5s 387: learn: 0.4669249 total: 9.78s remaining: 15.4s 388: learn: 0.4668914 total: 9.81s remaining: 15.4s 389: learn: 0.4668551 total: 9.84s remaining: 15.4s 390: learn: 0.4667978 total: 9.86s remaining: 15.4s 391: learn: 0.4667640 total: 9.88s remaining: 15.3s 392: learn: 0.4667328 total: 9.91s remaining: 15.3s 393: learn: 0.4667057 total: 9.93s remaining: 15.3s 394: learn: 0.4666729 total: 9.96s remaining: 15.2s 395: learn: 0.4666403 total: 9.98s remaining: 15.2s 396: learn: 0.4666114 total: 10s remaining: 15.2s 397: learn: 0.4665753 total: 10s remaining: 15.2s 398: learn: 0.4665430 total: 10.1s remaining: 15.1s 399: learn: 0.4665105 total: 10.1s remaining: 15.1s 400: learn: 0.4664574 total: 10.1s remaining: 15.1s 401: learn: 0.4664287 total: 10.1s remaining: 15.1s 402: learn: 0.4664050 total: 10.1s remaining: 15s 403: learn: 0.4663821 total: 10.2s remaining: 15s 404: learn: 0.4663643 total: 10.2s remaining: 15s 405: learn: 0.4663232 total: 10.2s remaining: 14.9s 406: learn: 0.4662958 total: 10.2s remaining: 14.9s 407: learn: 0.4662706 total: 10.3s remaining: 14.9s 408: learn: 0.4662425 total: 10.3s remaining: 14.9s 409: learn: 0.4661981 total: 10.3s remaining: 14.8s 410: learn: 0.4661695 total: 10.3s remaining: 14.8s 411: learn: 0.4661311 total: 10.4s remaining: 14.8s 412: learn: 0.4661092 total: 10.4s remaining: 14.7s 413: learn: 0.4660792 total: 10.4s remaining: 14.7s 414: learn: 0.4660495 total: 10.4s remaining: 14.7s 415: learn: 0.4660280 total: 10.4s remaining: 14.7s 416: learn: 0.4659799 total: 10.5s remaining: 14.6s 417: learn: 0.4659521 total: 10.5s remaining: 14.6s 418: learn: 0.4659136 total: 10.5s remaining: 14.6s 419: learn: 0.4658956 total: 10.5s remaining: 14.6s 420: learn: 0.4658665 total: 10.6s remaining: 14.5s 421: learn: 0.4658280 total: 10.6s remaining: 14.5s 422: learn: 0.4657990 total: 10.6s remaining: 14.5s 423: learn: 0.4657658 total: 10.6s remaining: 14.4s 424: learn: 0.4657396 total: 10.7s remaining: 14.4s 425: learn: 0.4657056 total: 10.7s remaining: 14.4s 426: learn: 0.4656819 total: 10.7s remaining: 14.4s 427: learn: 0.4656492 total: 10.7s remaining: 14.3s 428: learn: 0.4656146 total: 10.8s remaining: 14.3s 429: learn: 0.4656093 total: 10.8s remaining: 14.3s 430: learn: 0.4655778 total: 10.8s remaining: 14.2s 431: learn: 0.4655458 total: 10.8s remaining: 14.2s 432: learn: 0.4655261 total: 10.8s remaining: 14.2s 433: learn: 0.4655002 total: 10.9s remaining: 14.2s 434: learn: 0.4654702 total: 10.9s remaining: 14.1s 435: learn: 0.4654481 total: 10.9s remaining: 14.1s 436: learn: 0.4654259 total: 10.9s remaining: 14.1s 437: learn: 0.4654023 total: 10.9s remaining: 14s 438: learn: 0.4653630 total: 11s remaining: 14s 439: learn: 0.4653424 total: 11s remaining: 14s 440: learn: 0.4653026 total: 11s remaining: 14s 441: learn: 0.4652672 total: 11s remaining: 13.9s 442: learn: 0.4652472 total: 11.1s remaining: 13.9s 443: learn: 0.4652149 total: 11.1s remaining: 13.9s 444: learn: 0.4651717 total: 11.1s remaining: 13.9s 445: learn: 0.4651446 total: 11.1s remaining: 13.8s 446: learn: 0.4651118 total: 11.2s remaining: 13.8s 447: learn: 0.4650817 total: 11.2s remaining: 13.8s 448: learn: 0.4650518 total: 11.2s remaining: 13.7s 449: learn: 0.4650259 total: 11.2s remaining: 13.7s 450: learn: 0.4649924 total: 11.3s remaining: 13.7s 451: learn: 0.4649556 total: 11.3s remaining: 13.7s 452: learn: 0.4649371 total: 11.3s remaining: 13.6s 453: learn: 0.4649141 total: 11.3s remaining: 13.6s 454: learn: 0.4648867 total: 11.3s remaining: 13.6s 455: learn: 0.4648588 total: 11.4s remaining: 13.6s 456: learn: 0.4648250 total: 11.4s remaining: 13.5s 457: learn: 0.4647940 total: 11.4s remaining: 13.5s 458: learn: 0.4647546 total: 11.4s remaining: 13.5s 459: learn: 0.4647189 total: 11.5s remaining: 13.5s 460: learn: 0.4646901 total: 11.5s remaining: 13.4s 461: learn: 0.4646688 total: 11.5s remaining: 13.4s 462: learn: 0.4646478 total: 11.5s remaining: 13.4s 463: learn: 0.4646157 total: 11.6s remaining: 13.3s 464: learn: 0.4645898 total: 11.6s remaining: 13.3s 465: learn: 0.4645564 total: 11.6s remaining: 13.3s 466: learn: 0.4645328 total: 11.6s remaining: 13.3s 467: learn: 0.4645155 total: 11.6s remaining: 13.2s 468: learn: 0.4644996 total: 11.7s remaining: 13.2s 469: learn: 0.4644666 total: 11.7s remaining: 13.2s 470: learn: 0.4644513 total: 11.7s remaining: 13.2s 471: learn: 0.4644258 total: 11.7s remaining: 13.1s 472: learn: 0.4644093 total: 11.8s remaining: 13.1s 473: learn: 0.4643853 total: 11.8s remaining: 13.1s 474: learn: 0.4643599 total: 11.8s remaining: 13.1s 475: learn: 0.4643203 total: 11.8s remaining: 13s 476: learn: 0.4642956 total: 11.9s remaining: 13s 477: learn: 0.4642900 total: 11.9s remaining: 13s 478: learn: 0.4642673 total: 11.9s remaining: 12.9s 479: learn: 0.4642508 total: 11.9s remaining: 12.9s 480: learn: 0.4642142 total: 11.9s remaining: 12.9s 481: learn: 0.4641946 total: 12s remaining: 12.9s 482: learn: 0.4641754 total: 12s remaining: 12.8s 483: learn: 0.4641545 total: 12s remaining: 12.8s 484: learn: 0.4641318 total: 12s remaining: 12.8s 485: learn: 0.4641079 total: 12.1s remaining: 12.7s 486: learn: 0.4640660 total: 12.1s remaining: 12.7s 487: learn: 0.4640376 total: 12.1s remaining: 12.7s 488: learn: 0.4640081 total: 12.1s remaining: 12.7s 489: learn: 0.4639916 total: 12.1s remaining: 12.6s 490: learn: 0.4639589 total: 12.2s remaining: 12.6s 491: learn: 0.4639303 total: 12.2s remaining: 12.6s 492: learn: 0.4639081 total: 12.2s remaining: 12.6s 493: learn: 0.4638912 total: 12.2s remaining: 12.5s 494: learn: 0.4638710 total: 12.3s remaining: 12.5s 495: learn: 0.4638476 total: 12.3s remaining: 12.5s 496: learn: 0.4638259 total: 12.3s remaining: 12.5s 497: learn: 0.4637943 total: 12.3s remaining: 12.4s 498: learn: 0.4637668 total: 12.4s remaining: 12.4s 499: learn: 0.4637471 total: 12.4s remaining: 12.4s 500: learn: 0.4637232 total: 12.4s remaining: 12.4s 501: learn: 0.4637067 total: 12.4s remaining: 12.3s 502: learn: 0.4636740 total: 12.4s remaining: 12.3s 503: learn: 0.4636452 total: 12.5s remaining: 12.3s 504: learn: 0.4636246 total: 12.5s remaining: 12.2s 505: learn: 0.4636054 total: 12.5s remaining: 12.2s 506: learn: 0.4635891 total: 12.5s remaining: 12.2s 507: learn: 0.4635682 total: 12.6s remaining: 12.2s 508: learn: 0.4635449 total: 12.6s remaining: 12.1s 509: learn: 0.4635268 total: 12.6s remaining: 12.1s 510: learn: 0.4635089 total: 12.6s remaining: 12.1s 511: learn: 0.4634711 total: 12.6s remaining: 12.1s 512: learn: 0.4634321 total: 12.7s remaining: 12s 513: learn: 0.4633932 total: 12.7s remaining: 12s 514: learn: 0.4633642 total: 12.7s remaining: 12s 515: learn: 0.4633316 total: 12.7s remaining: 12s 516: learn: 0.4633065 total: 12.8s remaining: 11.9s 517: learn: 0.4632862 total: 12.8s remaining: 11.9s 518: learn: 0.4632698 total: 12.8s remaining: 11.9s 519: learn: 0.4632225 total: 12.8s remaining: 11.8s 520: learn: 0.4632012 total: 12.9s remaining: 11.8s 521: learn: 0.4631620 total: 12.9s remaining: 11.8s 522: learn: 0.4631414 total: 12.9s remaining: 11.8s 523: learn: 0.4631194 total: 12.9s remaining: 11.7s 524: learn: 0.4630930 total: 12.9s remaining: 11.7s 525: learn: 0.4630583 total: 13s remaining: 11.7s 526: learn: 0.4630285 total: 13s remaining: 11.7s 527: learn: 0.4630087 total: 13s remaining: 11.6s 528: learn: 0.4629701 total: 13s remaining: 11.6s 529: learn: 0.4629498 total: 13.1s remaining: 11.6s 530: learn: 0.4629261 total: 13.1s remaining: 11.6s 531: learn: 0.4629016 total: 13.1s remaining: 11.5s 532: learn: 0.4628997 total: 13.1s remaining: 11.5s 533: learn: 0.4628804 total: 13.1s remaining: 11.5s 534: learn: 0.4628440 total: 13.2s remaining: 11.4s 535: learn: 0.4628268 total: 13.2s remaining: 11.4s 536: learn: 0.4628064 total: 13.2s remaining: 11.4s 537: learn: 0.4627887 total: 13.2s remaining: 11.4s 538: learn: 0.4627678 total: 13.3s remaining: 11.3s 539: learn: 0.4627416 total: 13.3s remaining: 11.3s 540: learn: 0.4627175 total: 13.3s remaining: 11.3s 541: learn: 0.4626954 total: 13.3s remaining: 11.3s 542: learn: 0.4626756 total: 13.3s remaining: 11.2s 543: learn: 0.4626457 total: 13.4s remaining: 11.2s 544: learn: 0.4626263 total: 13.4s remaining: 11.2s 545: learn: 0.4625989 total: 13.4s remaining: 11.2s 546: learn: 0.4625737 total: 13.5s remaining: 11.1s 547: learn: 0.4625547 total: 13.5s remaining: 11.1s 548: learn: 0.4625313 total: 13.5s remaining: 11.1s 549: learn: 0.4625298 total: 13.5s remaining: 11.1s 550: learn: 0.4625013 total: 13.5s remaining: 11s 551: learn: 0.4624840 total: 13.6s remaining: 11s 552: learn: 0.4624610 total: 13.6s remaining: 11s 553: learn: 0.4624321 total: 13.6s remaining: 11s 554: learn: 0.4623955 total: 13.6s remaining: 10.9s 555: learn: 0.4623654 total: 13.7s remaining: 10.9s 556: learn: 0.4623306 total: 13.7s remaining: 10.9s 557: learn: 0.4623076 total: 13.7s remaining: 10.9s 558: learn: 0.4622805 total: 13.7s remaining: 10.8s 559: learn: 0.4622622 total: 13.8s remaining: 10.8s 560: learn: 0.4622486 total: 13.8s remaining: 10.8s 561: learn: 0.4622117 total: 13.8s remaining: 10.8s 562: learn: 0.4621923 total: 13.8s remaining: 10.7s 563: learn: 0.4621611 total: 13.8s remaining: 10.7s 564: learn: 0.4621252 total: 13.9s remaining: 10.7s 565: learn: 0.4621024 total: 13.9s remaining: 10.7s 566: learn: 0.4620826 total: 13.9s remaining: 10.6s 567: learn: 0.4620599 total: 13.9s remaining: 10.6s 568: learn: 0.4620290 total: 14s remaining: 10.6s 569: learn: 0.4620077 total: 14s remaining: 10.6s 570: learn: 0.4619892 total: 14s remaining: 10.5s 571: learn: 0.4619696 total: 14s remaining: 10.5s 572: learn: 0.4619295 total: 14.1s remaining: 10.5s 573: learn: 0.4619020 total: 14.1s remaining: 10.5s 574: learn: 0.4618889 total: 14.1s remaining: 10.4s 575: learn: 0.4618631 total: 14.1s remaining: 10.4s 576: learn: 0.4618511 total: 14.1s remaining: 10.4s 577: learn: 0.4618254 total: 14.2s remaining: 10.3s 578: learn: 0.4617986 total: 14.2s remaining: 10.3s 579: learn: 0.4617769 total: 14.2s remaining: 10.3s 580: learn: 0.4617501 total: 14.2s remaining: 10.3s 581: learn: 0.4617394 total: 14.3s remaining: 10.2s 582: learn: 0.4617182 total: 14.3s remaining: 10.2s 583: learn: 0.4616982 total: 14.3s remaining: 10.2s 584: learn: 0.4616772 total: 14.3s remaining: 10.2s 585: learn: 0.4616512 total: 14.4s remaining: 10.1s 586: learn: 0.4616288 total: 14.4s remaining: 10.1s 587: learn: 0.4616152 total: 14.4s remaining: 10.1s 588: learn: 0.4615934 total: 14.5s remaining: 10.1s 589: learn: 0.4615703 total: 14.5s remaining: 10.1s 590: learn: 0.4615505 total: 14.5s remaining: 10s 591: learn: 0.4615333 total: 14.5s remaining: 10s 592: learn: 0.4615128 total: 14.6s remaining: 10s 593: learn: 0.4614878 total: 14.6s remaining: 9.97s 594: learn: 0.4614633 total: 14.6s remaining: 9.95s 595: learn: 0.4614457 total: 14.6s remaining: 9.92s 596: learn: 0.4614247 total: 14.7s remaining: 9.89s 597: learn: 0.4614077 total: 14.7s remaining: 9.87s 598: learn: 0.4613902 total: 14.7s remaining: 9.84s 599: learn: 0.4613635 total: 14.7s remaining: 9.82s 600: learn: 0.4613463 total: 14.7s remaining: 9.79s 601: learn: 0.4613203 total: 14.8s remaining: 9.77s 602: learn: 0.4613012 total: 14.8s remaining: 9.74s 603: learn: 0.4612828 total: 14.8s remaining: 9.71s 604: learn: 0.4612645 total: 14.8s remaining: 9.69s 605: learn: 0.4612359 total: 14.9s remaining: 9.66s 606: learn: 0.4612195 total: 14.9s remaining: 9.63s 607: learn: 0.4612010 total: 14.9s remaining: 9.61s 608: learn: 0.4611876 total: 14.9s remaining: 9.58s 609: learn: 0.4611520 total: 15s remaining: 9.56s 610: learn: 0.4611268 total: 15s remaining: 9.53s 611: learn: 0.4611067 total: 15s remaining: 9.51s 612: learn: 0.4610767 total: 15s remaining: 9.48s 613: learn: 0.4610560 total: 15s remaining: 9.46s 614: learn: 0.4610269 total: 15.1s remaining: 9.43s 615: learn: 0.4610179 total: 15.1s remaining: 9.4s 616: learn: 0.4609943 total: 15.1s remaining: 9.38s 617: learn: 0.4609806 total: 15.1s remaining: 9.35s 618: learn: 0.4609726 total: 15.1s remaining: 9.32s 619: learn: 0.4609495 total: 15.2s remaining: 9.3s 620: learn: 0.4609341 total: 15.2s remaining: 9.27s 621: learn: 0.4609171 total: 15.2s remaining: 9.24s 622: learn: 0.4608912 total: 15.2s remaining: 9.22s 623: learn: 0.4608739 total: 15.3s remaining: 9.2s 624: learn: 0.4608477 total: 15.3s remaining: 9.17s 625: learn: 0.4608232 total: 15.3s remaining: 9.14s 626: learn: 0.4607958 total: 15.3s remaining: 9.12s 627: learn: 0.4607737 total: 15.4s remaining: 9.1s 628: learn: 0.4607536 total: 15.4s remaining: 9.07s 629: learn: 0.4607321 total: 15.4s remaining: 9.04s 630: learn: 0.4606962 total: 15.4s remaining: 9.02s 631: learn: 0.4606798 total: 15.4s remaining: 8.99s 632: learn: 0.4606626 total: 15.5s remaining: 8.97s 633: learn: 0.4606421 total: 15.5s remaining: 8.94s 634: learn: 0.4606160 total: 15.5s remaining: 8.92s 635: learn: 0.4606055 total: 15.5s remaining: 8.89s 636: learn: 0.4605804 total: 15.6s remaining: 8.87s 637: learn: 0.4605600 total: 15.6s remaining: 8.84s 638: learn: 0.4605430 total: 15.6s remaining: 8.81s 639: learn: 0.4604852 total: 15.6s remaining: 8.79s 640: learn: 0.4604722 total: 15.7s remaining: 8.77s 641: learn: 0.4604579 total: 15.7s remaining: 8.74s 642: learn: 0.4604315 total: 15.7s remaining: 8.72s 643: learn: 0.4604011 total: 15.7s remaining: 8.69s 644: learn: 0.4603772 total: 15.7s remaining: 8.67s 645: learn: 0.4603554 total: 15.8s remaining: 8.64s 646: learn: 0.4603449 total: 15.8s remaining: 8.62s 647: learn: 0.4603269 total: 15.8s remaining: 8.59s 648: learn: 0.4603107 total: 15.8s remaining: 8.56s 649: learn: 0.4602941 total: 15.9s remaining: 8.54s 650: learn: 0.4602784 total: 15.9s remaining: 8.51s 651: learn: 0.4602536 total: 15.9s remaining: 8.49s 652: learn: 0.4602419 total: 15.9s remaining: 8.46s 653: learn: 0.4602243 total: 15.9s remaining: 8.44s 654: learn: 0.4602097 total: 16s remaining: 8.41s 655: learn: 0.4601953 total: 16s remaining: 8.39s 656: learn: 0.4601696 total: 16s remaining: 8.36s 657: learn: 0.4601489 total: 16s remaining: 8.34s 658: learn: 0.4601240 total: 16.1s remaining: 8.31s 659: learn: 0.4601054 total: 16.1s remaining: 8.29s 660: learn: 0.4600876 total: 16.1s remaining: 8.26s 661: learn: 0.4600683 total: 16.1s remaining: 8.23s 662: learn: 0.4600450 total: 16.2s remaining: 8.21s 663: learn: 0.4600040 total: 16.2s remaining: 8.19s 664: learn: 0.4599875 total: 16.2s remaining: 8.16s 665: learn: 0.4599516 total: 16.2s remaining: 8.14s 666: learn: 0.4599331 total: 16.2s remaining: 8.11s 667: learn: 0.4599103 total: 16.3s remaining: 8.09s 668: learn: 0.4598994 total: 16.3s remaining: 8.06s 669: learn: 0.4598961 total: 16.3s remaining: 8.03s 670: learn: 0.4598790 total: 16.3s remaining: 8.01s 671: learn: 0.4598607 total: 16.4s remaining: 7.98s 672: learn: 0.4598472 total: 16.4s remaining: 7.96s 673: learn: 0.4598206 total: 16.4s remaining: 7.93s 674: learn: 0.4597874 total: 16.4s remaining: 7.91s 675: learn: 0.4597532 total: 16.4s remaining: 7.88s 676: learn: 0.4597350 total: 16.5s remaining: 7.86s 677: learn: 0.4597119 total: 16.5s remaining: 7.83s 678: learn: 0.4596926 total: 16.5s remaining: 7.81s 679: learn: 0.4596621 total: 16.5s remaining: 7.78s 680: learn: 0.4596408 total: 16.6s remaining: 7.76s 681: learn: 0.4596258 total: 16.6s remaining: 7.73s 682: learn: 0.4595994 total: 16.6s remaining: 7.71s 683: learn: 0.4595768 total: 16.6s remaining: 7.68s 684: learn: 0.4595568 total: 16.7s remaining: 7.66s 685: learn: 0.4595370 total: 16.7s remaining: 7.63s 686: learn: 0.4595153 total: 16.7s remaining: 7.61s 687: learn: 0.4594958 total: 16.7s remaining: 7.58s 688: learn: 0.4594774 total: 16.8s remaining: 7.56s 689: learn: 0.4594549 total: 16.8s remaining: 7.54s 690: learn: 0.4594402 total: 16.8s remaining: 7.51s 691: learn: 0.4594128 total: 16.8s remaining: 7.49s 692: learn: 0.4593982 total: 16.8s remaining: 7.46s 693: learn: 0.4593871 total: 16.9s remaining: 7.43s 694: learn: 0.4593710 total: 16.9s remaining: 7.41s 695: learn: 0.4593510 total: 16.9s remaining: 7.38s 696: learn: 0.4593407 total: 16.9s remaining: 7.36s 697: learn: 0.4593223 total: 17s remaining: 7.34s 698: learn: 0.4593092 total: 17s remaining: 7.31s 699: learn: 0.4592780 total: 17s remaining: 7.29s 700: learn: 0.4592626 total: 17s remaining: 7.26s 701: learn: 0.4592435 total: 17s remaining: 7.24s 702: learn: 0.4592295 total: 17.1s remaining: 7.21s 703: learn: 0.4592147 total: 17.1s remaining: 7.19s 704: learn: 0.4592007 total: 17.1s remaining: 7.16s 705: learn: 0.4591784 total: 17.1s remaining: 7.14s 706: learn: 0.4591636 total: 17.2s remaining: 7.11s 707: learn: 0.4591430 total: 17.2s remaining: 7.09s 708: learn: 0.4591243 total: 17.2s remaining: 7.06s 709: learn: 0.4590984 total: 17.2s remaining: 7.04s 710: learn: 0.4590806 total: 17.3s remaining: 7.01s 711: learn: 0.4590781 total: 17.3s remaining: 6.98s 712: learn: 0.4590593 total: 17.3s remaining: 6.96s 713: learn: 0.4590411 total: 17.3s remaining: 6.93s 714: learn: 0.4590188 total: 17.3s remaining: 6.91s 715: learn: 0.4590011 total: 17.4s remaining: 6.88s 716: learn: 0.4589881 total: 17.4s remaining: 6.86s 717: learn: 0.4589662 total: 17.4s remaining: 6.83s 718: learn: 0.4589442 total: 17.4s remaining: 6.81s 719: learn: 0.4589192 total: 17.5s remaining: 6.79s 720: learn: 0.4588769 total: 17.5s remaining: 6.76s 721: learn: 0.4588527 total: 17.5s remaining: 6.74s 722: learn: 0.4588199 total: 17.5s remaining: 6.71s 723: learn: 0.4587977 total: 17.5s remaining: 6.69s 724: learn: 0.4587864 total: 17.6s remaining: 6.67s 725: learn: 0.4587536 total: 17.6s remaining: 6.64s 726: learn: 0.4587309 total: 17.6s remaining: 6.62s 727: learn: 0.4587084 total: 17.6s remaining: 6.59s 728: learn: 0.4586970 total: 17.7s remaining: 6.57s 729: learn: 0.4586717 total: 17.7s remaining: 6.54s 730: learn: 0.4586615 total: 17.7s remaining: 6.51s 731: learn: 0.4586393 total: 17.7s remaining: 6.49s 732: learn: 0.4586023 total: 17.8s remaining: 6.47s 733: learn: 0.4585754 total: 17.8s remaining: 6.44s 734: learn: 0.4585648 total: 17.8s remaining: 6.42s 735: learn: 0.4585352 total: 17.8s remaining: 6.39s 736: learn: 0.4585120 total: 17.8s remaining: 6.37s 737: learn: 0.4584882 total: 17.9s remaining: 6.35s 738: learn: 0.4584717 total: 17.9s remaining: 6.32s 739: learn: 0.4584506 total: 17.9s remaining: 6.3s 740: learn: 0.4584400 total: 17.9s remaining: 6.27s 741: learn: 0.4584231 total: 18s remaining: 6.25s 742: learn: 0.4583861 total: 18s remaining: 6.22s 743: learn: 0.4583646 total: 18s remaining: 6.2s 744: learn: 0.4583315 total: 18s remaining: 6.17s 745: learn: 0.4583167 total: 18.1s remaining: 6.15s 746: learn: 0.4582962 total: 18.1s remaining: 6.13s 747: learn: 0.4582765 total: 18.1s remaining: 6.1s 748: learn: 0.4582646 total: 18.1s remaining: 6.08s 749: learn: 0.4582608 total: 18.1s remaining: 6.05s 750: learn: 0.4582458 total: 18.2s remaining: 6.02s 751: learn: 0.4582240 total: 18.2s remaining: 6s 752: learn: 0.4582099 total: 18.2s remaining: 5.97s 753: learn: 0.4581939 total: 18.2s remaining: 5.95s 754: learn: 0.4581842 total: 18.3s remaining: 5.92s 755: learn: 0.4581604 total: 18.3s remaining: 5.9s 756: learn: 0.4581397 total: 18.3s remaining: 5.87s 757: learn: 0.4581280 total: 18.3s remaining: 5.85s 758: learn: 0.4581143 total: 18.3s remaining: 5.82s 759: learn: 0.4580891 total: 18.4s remaining: 5.8s 760: learn: 0.4580627 total: 18.4s remaining: 5.77s 761: learn: 0.4580526 total: 18.4s remaining: 5.75s 762: learn: 0.4580375 total: 18.4s remaining: 5.72s 763: learn: 0.4580195 total: 18.5s remaining: 5.7s 764: learn: 0.4580057 total: 18.5s remaining: 5.67s 765: learn: 0.4579887 total: 18.5s remaining: 5.65s 766: learn: 0.4579696 total: 18.5s remaining: 5.63s 767: learn: 0.4579509 total: 18.5s remaining: 5.6s 768: learn: 0.4579495 total: 18.6s remaining: 5.57s 769: learn: 0.4579257 total: 18.6s remaining: 5.55s 770: learn: 0.4579099 total: 18.6s remaining: 5.53s 771: learn: 0.4578826 total: 18.6s remaining: 5.5s 772: learn: 0.4578682 total: 18.6s remaining: 5.48s 773: learn: 0.4578556 total: 18.7s remaining: 5.45s 774: learn: 0.4578421 total: 18.7s remaining: 5.43s 775: learn: 0.4578141 total: 18.7s remaining: 5.4s 776: learn: 0.4577988 total: 18.7s remaining: 5.38s 777: learn: 0.4577819 total: 18.8s remaining: 5.35s 778: learn: 0.4577701 total: 18.8s remaining: 5.33s 779: learn: 0.4577531 total: 18.8s remaining: 5.3s 780: learn: 0.4577352 total: 18.8s remaining: 5.28s 781: learn: 0.4577211 total: 18.8s remaining: 5.25s 782: learn: 0.4577111 total: 18.9s remaining: 5.23s 783: learn: 0.4576968 total: 18.9s remaining: 5.21s 784: learn: 0.4576787 total: 18.9s remaining: 5.18s 785: learn: 0.4576650 total: 18.9s remaining: 5.16s 786: learn: 0.4576469 total: 19s remaining: 5.13s 787: learn: 0.4576377 total: 19s remaining: 5.11s 788: learn: 0.4576243 total: 19s remaining: 5.08s 789: learn: 0.4576076 total: 19s remaining: 5.06s 790: learn: 0.4575942 total: 19s remaining: 5.03s 791: learn: 0.4575747 total: 19.1s remaining: 5.01s 792: learn: 0.4575623 total: 19.1s remaining: 4.98s 793: learn: 0.4575533 total: 19.1s remaining: 4.96s 794: learn: 0.4575277 total: 19.1s remaining: 4.94s 795: learn: 0.4575093 total: 19.2s remaining: 4.91s 796: learn: 0.4574888 total: 19.2s remaining: 4.89s 797: learn: 0.4574696 total: 19.2s remaining: 4.86s 798: learn: 0.4574404 total: 19.2s remaining: 4.84s 799: learn: 0.4574324 total: 19.3s remaining: 4.81s 800: learn: 0.4574140 total: 19.3s remaining: 4.79s 801: learn: 0.4573900 total: 19.3s remaining: 4.77s 802: learn: 0.4573717 total: 19.3s remaining: 4.74s 803: learn: 0.4573504 total: 19.4s remaining: 4.72s 804: learn: 0.4573377 total: 19.4s remaining: 4.69s 805: learn: 0.4573195 total: 19.4s remaining: 4.67s 806: learn: 0.4573076 total: 19.4s remaining: 4.64s 807: learn: 0.4572822 total: 19.4s remaining: 4.62s 808: learn: 0.4572714 total: 19.5s remaining: 4.6s 809: learn: 0.4572412 total: 19.5s remaining: 4.57s 810: learn: 0.4572213 total: 19.5s remaining: 4.55s 811: learn: 0.4572136 total: 19.5s remaining: 4.52s 812: learn: 0.4571862 total: 19.6s remaining: 4.5s 813: learn: 0.4571635 total: 19.6s remaining: 4.47s 814: learn: 0.4571452 total: 19.6s remaining: 4.45s 815: learn: 0.4571295 total: 19.6s remaining: 4.43s 816: learn: 0.4571178 total: 19.7s remaining: 4.4s 817: learn: 0.4571013 total: 19.7s remaining: 4.38s 818: learn: 0.4570851 total: 19.7s remaining: 4.35s 819: learn: 0.4570771 total: 19.7s remaining: 4.33s 820: learn: 0.4570603 total: 19.7s remaining: 4.3s 821: learn: 0.4570368 total: 19.8s remaining: 4.28s 822: learn: 0.4570364 total: 19.8s remaining: 4.25s 823: learn: 0.4570189 total: 19.8s remaining: 4.23s 824: learn: 0.4569980 total: 19.8s remaining: 4.21s 825: learn: 0.4569802 total: 19.9s remaining: 4.18s 826: learn: 0.4569542 total: 19.9s remaining: 4.16s 827: learn: 0.4569360 total: 19.9s remaining: 4.13s 828: learn: 0.4569153 total: 19.9s remaining: 4.11s 829: learn: 0.4568951 total: 19.9s remaining: 4.09s 830: learn: 0.4568824 total: 20s remaining: 4.06s 831: learn: 0.4568659 total: 20s remaining: 4.04s 832: learn: 0.4568395 total: 20s remaining: 4.01s 833: learn: 0.4568130 total: 20s remaining: 3.99s 834: learn: 0.4567972 total: 20.1s remaining: 3.96s 835: learn: 0.4567794 total: 20.1s remaining: 3.94s 836: learn: 0.4567642 total: 20.1s remaining: 3.92s 837: learn: 0.4567562 total: 20.1s remaining: 3.89s 838: learn: 0.4567364 total: 20.2s remaining: 3.87s 839: learn: 0.4567156 total: 20.2s remaining: 3.84s 840: learn: 0.4566981 total: 20.2s remaining: 3.82s 841: learn: 0.4566868 total: 20.2s remaining: 3.79s 842: learn: 0.4566664 total: 20.2s remaining: 3.77s 843: learn: 0.4566496 total: 20.3s remaining: 3.75s 844: learn: 0.4566306 total: 20.3s remaining: 3.72s 845: learn: 0.4566150 total: 20.3s remaining: 3.7s 846: learn: 0.4565976 total: 20.3s remaining: 3.67s 847: learn: 0.4565772 total: 20.4s remaining: 3.65s 848: learn: 0.4565593 total: 20.4s remaining: 3.63s 849: learn: 0.4565394 total: 20.4s remaining: 3.6s 850: learn: 0.4565181 total: 20.4s remaining: 3.58s 851: learn: 0.4565031 total: 20.5s remaining: 3.56s 852: learn: 0.4564851 total: 20.5s remaining: 3.53s 853: learn: 0.4564641 total: 20.5s remaining: 3.51s 854: learn: 0.4564482 total: 20.5s remaining: 3.48s 855: learn: 0.4564319 total: 20.6s remaining: 3.46s 856: learn: 0.4564187 total: 20.6s remaining: 3.44s 857: learn: 0.4564000 total: 20.6s remaining: 3.41s 858: learn: 0.4563870 total: 20.6s remaining: 3.39s 859: learn: 0.4563727 total: 20.7s remaining: 3.36s 860: learn: 0.4563622 total: 20.7s remaining: 3.34s 861: learn: 0.4563425 total: 20.7s remaining: 3.31s 862: learn: 0.4563246 total: 20.7s remaining: 3.29s 863: learn: 0.4562975 total: 20.7s remaining: 3.27s 864: learn: 0.4562817 total: 20.8s remaining: 3.24s 865: learn: 0.4562610 total: 20.8s remaining: 3.22s 866: learn: 0.4562399 total: 20.8s remaining: 3.19s 867: learn: 0.4562167 total: 20.8s remaining: 3.17s 868: learn: 0.4562029 total: 20.9s remaining: 3.15s 869: learn: 0.4561811 total: 20.9s remaining: 3.12s 870: learn: 0.4561683 total: 20.9s remaining: 3.1s 871: learn: 0.4561510 total: 20.9s remaining: 3.07s 872: learn: 0.4561413 total: 20.9s remaining: 3.05s 873: learn: 0.4561190 total: 21s remaining: 3.02s 874: learn: 0.4560982 total: 21s remaining: 3s 875: learn: 0.4560580 total: 21s remaining: 2.98s 876: learn: 0.4560453 total: 21s remaining: 2.95s 877: learn: 0.4560332 total: 21.1s remaining: 2.93s 878: learn: 0.4560155 total: 21.1s remaining: 2.9s 879: learn: 0.4559969 total: 21.1s remaining: 2.88s 880: learn: 0.4559668 total: 21.1s remaining: 2.85s 881: learn: 0.4559490 total: 21.2s remaining: 2.83s 882: learn: 0.4559280 total: 21.2s remaining: 2.81s 883: learn: 0.4559081 total: 21.2s remaining: 2.78s 884: learn: 0.4558973 total: 21.2s remaining: 2.76s 885: learn: 0.4558706 total: 21.3s remaining: 2.73s 886: learn: 0.4558539 total: 21.3s remaining: 2.71s 887: learn: 0.4558312 total: 21.3s remaining: 2.69s 888: learn: 0.4558055 total: 21.3s remaining: 2.66s 889: learn: 0.4557857 total: 21.3s remaining: 2.64s 890: learn: 0.4557686 total: 21.4s remaining: 2.61s 891: learn: 0.4557392 total: 21.4s remaining: 2.59s 892: learn: 0.4557197 total: 21.4s remaining: 2.57s 893: learn: 0.4557035 total: 21.4s remaining: 2.54s 894: learn: 0.4556835 total: 21.5s remaining: 2.52s 895: learn: 0.4556707 total: 21.5s remaining: 2.49s 896: learn: 0.4556560 total: 21.5s remaining: 2.47s 897: learn: 0.4556452 total: 21.5s remaining: 2.45s 898: learn: 0.4556277 total: 21.6s remaining: 2.42s 899: learn: 0.4556122 total: 21.6s remaining: 2.4s 900: learn: 0.4555786 total: 21.6s remaining: 2.37s 901: learn: 0.4555640 total: 21.6s remaining: 2.35s 902: learn: 0.4555446 total: 21.7s remaining: 2.33s 903: learn: 0.4555261 total: 21.7s remaining: 2.3s 904: learn: 0.4555105 total: 21.7s remaining: 2.28s 905: learn: 0.4554856 total: 21.7s remaining: 2.25s 906: learn: 0.4554663 total: 21.7s remaining: 2.23s 907: learn: 0.4554458 total: 21.8s remaining: 2.21s 908: learn: 0.4554283 total: 21.8s remaining: 2.18s 909: learn: 0.4554108 total: 21.8s remaining: 2.16s 910: learn: 0.4553899 total: 21.8s remaining: 2.13s 911: learn: 0.4553896 total: 21.9s remaining: 2.11s 912: learn: 0.4553777 total: 21.9s remaining: 2.08s 913: learn: 0.4553589 total: 21.9s remaining: 2.06s 914: learn: 0.4553303 total: 21.9s remaining: 2.04s 915: learn: 0.4553188 total: 21.9s remaining: 2.01s 916: learn: 0.4553021 total: 22s remaining: 1.99s 917: learn: 0.4552775 total: 22s remaining: 1.96s 918: learn: 0.4552582 total: 22s remaining: 1.94s 919: learn: 0.4552456 total: 22s remaining: 1.92s 920: learn: 0.4552353 total: 22.1s remaining: 1.89s 921: learn: 0.4552037 total: 22.1s remaining: 1.87s 922: learn: 0.4551853 total: 22.1s remaining: 1.84s 923: learn: 0.4551758 total: 22.1s remaining: 1.82s 924: learn: 0.4551646 total: 22.2s remaining: 1.8s 925: learn: 0.4551315 total: 22.2s remaining: 1.77s 926: learn: 0.4551300 total: 22.2s remaining: 1.75s 927: learn: 0.4551201 total: 22.2s remaining: 1.72s 928: learn: 0.4551077 total: 22.2s remaining: 1.7s 929: learn: 0.4550858 total: 22.3s remaining: 1.68s 930: learn: 0.4550616 total: 22.3s remaining: 1.65s 931: learn: 0.4550351 total: 22.3s remaining: 1.63s 932: learn: 0.4550163 total: 22.3s remaining: 1.6s 933: learn: 0.4550073 total: 22.4s remaining: 1.58s 934: learn: 0.4549719 total: 22.4s remaining: 1.55s 935: learn: 0.4549516 total: 22.4s remaining: 1.53s 936: learn: 0.4549308 total: 22.4s remaining: 1.51s 937: learn: 0.4549130 total: 22.4s remaining: 1.48s 938: learn: 0.4548999 total: 22.5s remaining: 1.46s 939: learn: 0.4548913 total: 22.5s remaining: 1.44s 940: learn: 0.4548726 total: 22.5s remaining: 1.41s 941: learn: 0.4548671 total: 22.5s remaining: 1.39s 942: learn: 0.4548387 total: 22.6s remaining: 1.36s 943: learn: 0.4548268 total: 22.6s remaining: 1.34s 944: learn: 0.4547866 total: 22.6s remaining: 1.31s 945: learn: 0.4547752 total: 22.6s remaining: 1.29s 946: learn: 0.4547542 total: 22.7s remaining: 1.27s 947: learn: 0.4547439 total: 22.7s remaining: 1.24s 948: learn: 0.4547325 total: 22.7s remaining: 1.22s 949: learn: 0.4547208 total: 22.7s remaining: 1.2s 950: learn: 0.4547003 total: 22.7s remaining: 1.17s 951: learn: 0.4546867 total: 22.8s remaining: 1.15s 952: learn: 0.4546711 total: 22.8s remaining: 1.12s 953: learn: 0.4546565 total: 22.8s remaining: 1.1s 954: learn: 0.4546364 total: 22.8s remaining: 1.07s 955: learn: 0.4546195 total: 22.9s remaining: 1.05s 956: learn: 0.4546043 total: 22.9s remaining: 1.03s 957: learn: 0.4545783 total: 22.9s remaining: 1s 958: learn: 0.4545592 total: 22.9s remaining: 980ms 959: learn: 0.4545341 total: 22.9s remaining: 956ms 960: learn: 0.4545170 total: 23s remaining: 932ms 961: learn: 0.4544988 total: 23s remaining: 908ms 962: learn: 0.4544688 total: 23s remaining: 884ms 963: learn: 0.4544563 total: 23s remaining: 860ms 964: learn: 0.4544383 total: 23.1s remaining: 836ms 965: learn: 0.4544221 total: 23.1s remaining: 813ms 966: learn: 0.4544077 total: 23.1s remaining: 789ms 967: learn: 0.4543857 total: 23.1s remaining: 765ms 968: learn: 0.4543678 total: 23.2s remaining: 741ms 969: learn: 0.4543516 total: 23.2s remaining: 717ms 970: learn: 0.4543406 total: 23.2s remaining: 693ms 971: learn: 0.4543111 total: 23.2s remaining: 669ms 972: learn: 0.4542974 total: 23.2s remaining: 645ms 973: learn: 0.4542779 total: 23.3s remaining: 621ms 974: learn: 0.4542674 total: 23.3s remaining: 597ms 975: learn: 0.4542575 total: 23.3s remaining: 573ms 976: learn: 0.4542426 total: 23.3s remaining: 549ms 977: learn: 0.4542294 total: 23.4s remaining: 526ms 978: learn: 0.4542146 total: 23.4s remaining: 502ms 979: learn: 0.4541863 total: 23.4s remaining: 478ms 980: learn: 0.4541696 total: 23.4s remaining: 454ms 981: learn: 0.4541501 total: 23.5s remaining: 430ms 982: learn: 0.4541334 total: 23.5s remaining: 406ms 983: learn: 0.4541171 total: 23.5s remaining: 382ms 984: learn: 0.4540999 total: 23.5s remaining: 358ms 985: learn: 0.4540737 total: 23.5s remaining: 334ms 986: learn: 0.4540537 total: 23.6s remaining: 310ms 987: learn: 0.4540358 total: 23.6s remaining: 287ms 988: learn: 0.4539991 total: 23.6s remaining: 263ms 989: learn: 0.4539805 total: 23.6s remaining: 239ms 990: learn: 0.4539649 total: 23.7s remaining: 215ms 991: learn: 0.4539472 total: 23.7s remaining: 191ms 992: learn: 0.4539314 total: 23.7s remaining: 167ms 993: learn: 0.4539172 total: 23.7s remaining: 143ms 994: learn: 0.4539031 total: 23.7s remaining: 119ms 995: learn: 0.4538919 total: 23.8s remaining: 95.5ms 996: learn: 0.4538690 total: 23.8s remaining: 71.6ms 997: learn: 0.4538524 total: 23.8s remaining: 47.7ms 998: learn: 0.4538285 total: 23.8s remaining: 23.9ms 999: learn: 0.4538154 total: 23.9s remaining: 0us model_name: CatBoostClassifier Confusion Matrix [[13973 3262] [ 3145 9620]] Model AUC: 0.858, Model Accuracy: 0.786 CPU times: user 1min 13s, sys: 8.63 s, total: 1min 22s Wall time: 24.8 s 1234567891011121314import numpy as npfrom datetime import datetimeversion = datetime.now().strftime(&quot;%d-%m-%Y %H-%M-%S&quot;)def final_submission(model, data, version): final_preds = model.predict(data) binarizer = np.vectorize(lambda x: 1 if x &gt;= .5 else 0) prediction_binarized = binarizer(final_preds) submission = pd.concat([sample_submission,pd.DataFrame(prediction_binarized)], axis=1).drop(columns=[&#x27;Survived&#x27;]) submission.columns = [&#x27;PassengerId&#x27;, &#x27;Survived&#x27;] submission.to_csv(&#x27;Sklearn of Submission.csv&#x27;.format(version), index=False) final_submission(cb_model, test_data, version) 1","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[ML]텍스트 마이닝","slug":"Python/ML/textmining","date":"2021-04-19T15:00:00.000Z","updated":"2021-05-14T00:15:21.186Z","comments":true,"path":"2021/04/20/Python/ML/textmining/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/20/Python/ML/textmining/","excerpt":"","text":"[ML] 텍스트 마이닝(Text Mining)이란?1. 개념 텍스트 마이닝은 비정형 및 반 정형 데이터에 대하여 자연어 처리 기술과 문서 처리 기술을 적용하여 유용한 정보를 추출, 가공하는 목적으로 하는 기술이다. 대부분의 텍스트 데이터베이스에 저장된 데이터는 반구조적 데이터이다. 이때, 반구조적 데이터란? 완전히 구조적이지도 않고 완전히 비구조적이지도 않은 데이터를 의미 2. 데이터 마이닝과 텍스트 마이닝의 비교 3. 텍스트 마이닝의 문제점 자연어에 영향을 많이 받는다. 분석 결과물 자체로 어떤 성과를 보기가 어렵다. 4. 텍스트 마이닝이 요구하는 기법 텍스트 마아닝은 “데이터 마이닝 기법” 이외에도 자연어 처리기술, 문서처리 기술을 추가로 요구한다. 5. 불용어의 정의 분석하는데 있어서 크게 의미가 없는 단어들, 예를 들면 조사, 접미사 등이 있다. References https://iamdaisy.tistory.com/29 https://wikidocs.net/book/2155","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://eunjin-jun717.github.io/tags/ML/"}]},{"title":"[ML] Catboost","slug":"Python/ML/catboost","date":"2021-04-14T15:00:00.000Z","updated":"2021-04-16T00:05:29.944Z","comments":true,"path":"2021/04/15/Python/ML/catboost/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/15/Python/ML/catboost/","excerpt":"","text":"[Machine Learning] CatboostCatboost의 개념 Catboost란 Gradient boosting를 기반으로한 부스팅이다. 간단하게 Gradient boosting을 설명하자면, 첫번째 정답의 오차에 대해 학습시키고 또 두번째 정답의 오차에 대해 학습시키면서 점진적으로 weaker learner을 strong learner로 만드는 방법이다. Catboost의 특징1) Level-wise tree 는 Best First Tree 같이 트리를 만들어나가는 형태 XGB, LightBGM과 다르게 대칭 트리의 구조를 사용한다. 이렇게 균형잡힌 depth-wise trees의 장점은 오버피팅을 방지할 수 있음(그러나 단점으로는 leaf wise보다는 느림) Root node에서 leaf node까지 만들어진 조건들을 binary된 백터로 변환하여 각 index에 값을 저장함으로써 메모리에 저장할 필요가 없기 때문에 효율적인 테스트 가능! 2) 기존의 부스팅 모델인 Gradient boosting같은 경우에는 모든 훈련 데이터를 대상으로 잔차 계산을 했다면, Catboost는 일부만가지고 잔차를 계산한 뒤, 이것을 모델로 만든다. X1의 잔차만 계산하고, 이를 기반으로 모델을 만든다. 이 모델을 이용하여 x2의 잔차를 예측한다. X1, x2 의 잔차를 가지고 모델을 만들고, 이를 기반으로 x3,x4의 잔차를 예측한다. 반복한다. 이를 Ordered Boosting이라 한다. 3) Random Permutation Ordered 부스팅할 때, 순서를 섞어 주지않으면 매번 같은 순서대로 잔차를 예측하는 모델을 만들 가능성이 있음 따라서 Catboost에서는 데이터를 셔플링해서 뽑아내야한다. 4) Ordered Target Encoding 범주형 변수를 수로 인코딩 시키는 방법 Time과 feature1을 가지고 class_labels를 예측할 것이다. Mean encoding을 할 것인데 , 우리가 예측해야하는 값이 훈련 셋 피쳐에 들어가는 문제(Data Leakage)를 일으키지 않는 해결책으로 아래처럼 인코딩 할 것이다. 예를 들면, Friday에 cloudy는 (15+14)/2 =15.5로 인코딩 Saturday에 cloudy는 (15+14+20)/3=16.3 으로 인코딩 즉, 현재 데이터의 target값을 사용하지 않고, 이전 데이터들의 타겟값만 사용하지 data leakage를 일으키지 않는 것! 5) Categorical Feature Combinations Country와 hair color로 class label을 예측하려는데, 위와 같이 두 feature를 하나의 feature로 묶을 수 있는 경우가 생긴다. catboost에서는 하나로 묶어버린다. 6) One-hot Encoding Caldinality가 3이하인 범주형 변수들은 Target encoding이 아닌 one-hot encoding으로된다. Target 인코딩보다 더 효율적이다. 3. Catboost의 한계 데이터 대부분이 수치형 변수일 경우에는 Light GBM보다 학습 속도가 느리다. 따라서 범주형 변수에 더 적합하다 4. Hyper Parameter Learning_rate Random_strength L2_regulariser Catboost는 기본 파라미터로도 충분히 최적화가 잘 되어있기 때문에, 굳이 다른 파라미터를 더 사용하지 않아도 된다. References https://dailyheumsi.tistory.com/136 https://hanishrohit.medium.com/whats-so-special-about-catboost-335d64d754ae","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[ML] Gradient Boosting","slug":"Python/ML/ml-gradient","date":"2021-04-14T15:00:00.000Z","updated":"2021-04-16T00:05:29.946Z","comments":true,"path":"2021/04/15/Python/ML/ml-gradient/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/15/Python/ML/ml-gradient/","excerpt":"","text":"[Machine Learning] Gradient Boosting Catboost, XGBoost, LightGBM의 기반이 되는 부스팅이다.1. 과정1) 최초, 평균 값으로 예측 Height, favorite color, gender로 weight을 ‘일반적’으로 예측하는 방법은, 먼저 weight의 평균값으로 예측하는 것이다. (avg weight = 71.2) 2) 예측 값과 실제 값의 오차 구하기 Residual(오차)들을 계산해보자. 당연히 클 수밖에 없다. 3) 오차 값을 예측하는 Tree 만들기 위에서 계산한 오차를, dataset의 feature들(height, favorite color, gender)을 가지고 트리를 만들어 예측한다. Leaf node에 분류되 값들이 여러 개 있는 경우에는 평균을 내서 하나의 값으로 만들자 4) Learning rate를 적용하여, 기존 예측 값 업데이트하기 오차 값(16.8)에 원래의 예측값(71.2)을 더해주어 업데이트한다. 그러나 이때 값은 88이다. 이 값은 원래의 weight와 동일하다. 그냥 더해주기만 하면 기존 dataset에는 완벽히 fitted하지만, new dataset에는 안맞을 가능성이 높다 따라서 그냥 더하지 않고 learning rate만큼 더해준다. 여기서는 learning rate를 0.1로 잡아준다. 0.1을 곱해서 더해주니 예측값은 72.9가 나왔다. 처음 예측값(71.2) -&gt; 현재 예측값(72.9) 원래의 weight는 88이니 이전보다 오차가 줄어들었다. 또 다시 원래의 데이터 값에 현재 예측값을 뺀 오차에 대한 트리를 만들며 일정 loop를 반복하다보면 점점 원래의 값인 88에 가까워 진다. 즉, High Variance를 피하면서 점진적으로 학습시키는 것이다. 5) 전체적인 구조 References https://www.youtube.com/watch?v=2xudPOBz-vs https://dailyheumsi.tistory.com/116?category=815369","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[ML] Decision Tree","slug":"Python/ML/ml-dtree","date":"2021-04-13T15:00:00.000Z","updated":"2021-04-16T00:05:29.945Z","comments":true,"path":"2021/04/14/Python/ML/ml-dtree/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/14/Python/ML/ml-dtree/","excerpt":"","text":"[Machine Learning] Decision Tree1. Decision Tree(결정트리) 개념 분류와 회귀 모두 가능한 학습 지도 모델 스무고개 하듯이 예/아니오 질문을 이어가며 학습 이렇게 특정 기준(질문)을 따라 데이터를 구분하는 모델 첫 질문: Root node / 맨 마지막 노드: Terminal node(Leaf node) 데이터를 지나치게 나누다 보면 과대적합(Overfitting) 됨 여기서 과대적합이란, 모든 데이터 셋이 모두 잘 분류되어 불순도가 0이 될 때 까지 분기해 나간다. 또한, Root에서부터 하위 노드가 많이 만들어질수록 모델이 복잡해져 과대적합이 발생할 수 있다. 따라서, 결정트리에 하이퍼파라미터를 주고 모델링하기 그렇다면 오버피팅 막는 전략은 무엇이 있을까? 1) 가지치기(Pruning): 가지치기란? 하부트리를 제거하여 일반화 성능을 높이는 것. 첫번째, 속성을 제한 한다. Max_depth를 통해 트리의 깊이를 제한한다. 두번째, 한 노드에 들어있는 최소 데이터 수를 정한다. Min_samples_split을 조정하면 된다. 왼쪽은 default, 오른쪽은 min_samples_split=4라는 규제를 주어 훈련 시킴. 리프 노드의 최소 샘플이 4개 이상이니, 과대적합을 피하였고 좀 더 일반화가 가능해진 것을 확인 할 수 있다. 2) 그리드서치(Grid Search): 격자탐색이라 하며, 모델 하이퍼 파라미터에 넣을 수 있는 값들을 순차적으로 입력한 뒤에 가장 높은 성능을 보이는 하이퍼 파라미터들을 찾는 탐색 방법이다. 즉, 쉽게 말하면 세부적인 규율인 하이퍼 파라미터를 일일히 다 적용해가면서 어떤 규율이 모델에 적합한지 판단하는 것! EX) grid_dtree = GridSearchCV(dtree, param_grid= parameters, cv=3, refit=True) parameters에는 하이퍼 파라미터 정보담기 딕셔너리 cv는 cross validation 교차 검증이다. 이 함수를 수행한 뒤 이후에 grid_dtree.best_estimator을 활용하면, 학습과정에서 하이퍼 파라미터 조정으로 가장 좋은 성능을 보인 모델이 반환된다. 3) 랜덤 서치(Random Search): 그리드서치는 말그대로 모든 경우를 테이블로 만든 뒤 격자로 탐색하는 방식이지만 랜덤 서치는 하이퍼 파라미터 값을 랜덤하게 넣어보고 그 중 우수한 값을 모인 하이퍼 파라미터를 활용해 모델을 생성하는 것! 2. 하이퍼 파라미터(Hyper Parameter) Max_depth: 최대 깊이 값이 클수록 모델의 복잡도는 증가 Min_samples_split: 분할되기 위해 노드가 가져야 하는 최소 샘플 수 (default: 2) Min_samples_leaf: 리프 노드가 가지고 있어야 할 최소 샘플 수(default: 1) Max_leaf_nodes: 리프 노드의 최대 수 Min_samples_leaf: 가지를 칠 최소 sample 수 Ex) min_samples_leaf=4 이면, 4보다 자식노드 개수가 작아지면 더 이상 분류하지 않고 끝낸다. 3. Criterion(분할 품질을 측정하는 기능)1) 불순도- Gini 계수(결정트리의 기본 criterion) 우선, 불순도가 무엇인지 간단하게 설명해보자. 위쪽: 순도가 높다 아래쪽: 불순도가 높음, 즉 순도가 낮다. 불순도 최대: 한 범주에 두 데이터가 딱 반반 그렇다면 불순도는 어떻게 구해지는가? 아래의 식을 Gini라고 한다. 6장의 예제를 보면서 간단하게 설명해보자. gini = 1- (i번째 노드에 들어있는 k의 비율) Versicolor의 Gini(불순도)를 위의 gini 식에 대입하면 아래와 같다. 1- ((0/54)^2 +(49/54)^2 +(5/54)^2 ) = 0.168 Gini 계수의 값이 0에 가까울수록 좋다. 2) 불순도- 엔트로피(Entropy) (Pi는 한 영역에 존재하는 데이터 가운데 범주 i에 속하는 데이터 비율) 위의 그림을 보며 엔트로피를 설명해보자. 전체 16개 중 빨간색(범주=1)는 10개, 파란색(범주=0)은 6개이다. 이때의 엔트로피는 다음과 같다. 주황색 영역의 엔트로피는 0.95이다. 그렇다면, 그림에서 빨간 선을 그은 뒤 두개의 영역으로 나눴을 때의 엔트로피는? 엔트로피가 0.95에서 0.75로 내려갔다. 즉, 불확실성 감소 -&gt; 순도 증가 -&gt; 정보 획득 이런식으로 엔트로피가 낮아지는 방향으로 분할하면 된다. 3) 정보 획득량(information gain) 정보획득량이란? 엔트로피(불확실성)의 감소량 각 특성들이 훈련 예제들을 얼만큼 잘 분류할 수 있는가를 측정 트리 구축과정에서 테스트할 후보 특성의 순서를 결정할 때 사용 Information gain = (분류 전) 처음 엔트로피 – (분류 후) 나중 엔트로피 4. DecisionTreeRegressor결정트리 문제는 회귀 문제에도 사용 가능하다. DecisionTreeClassifer 와 비슷하지만 DecisionTreeRegressor은 클래스를 예측하는것이 아니라 어떤 값을 예측한다. 예를 들면, x1=0.6인 샘플의 target값을 예측한다 하자 루트 노드부터 순회하면 결국 value=0.111인 리프 노드에 도달한다. 이 리프노드에 있는 110개 훈련 샘플의 평균 target값이 예측된다. 이 예측값을 사용해 110개 sample에 대한 MSE를 계산하면 0.015가 나온다. 즉, 각 영역의 예측값 = 그 영역에 있는 target 값의 평균 알고리즘은 예측값과 가능한 많은 샘플이 가까이 있도록 영역을 분할한다. References https://wikidocs.net/43627 https://ysyblog.tistory.com/76 https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html https://huidea.tistory.com/32 핸즈온 머신러닝","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Python] Series & Dataframe","slug":"Python/series_df","date":"2021-04-07T15:00:00.000Z","updated":"2021-04-26T08:13:14.034Z","comments":true,"path":"2021/04/08/Python/series_df/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/08/Python/series_df/","excerpt":"","text":"[Python] Series &amp; Dataframe Pandas의 데이터 구조: Series, Dataframe, Panel 3가지가 있다. 1. Series pandas의 1차원 자료구조 1234import pandas as pdser= pd.Series([10,20,30,40],index=list(&#x27;abcd&#x27;)) # 10,20,30,40의 리스트를 만드는데 이때의 index는 a,b,c,d로 지정ser a 10 b 20 c 30 d 40 dtype: int64 12print(ser[3],ser[2],ser[1],ser[0])print(ser[&#x27;d&#x27;],ser[&#x27;c&#x27;],ser[&#x27;b&#x27;],ser[&#x27;a&#x27;]) 40 30 20 10 40 30 20 10 1print(ser[&#x27;a&#x27;:&#x27;d&#x27;]) #index 문자로 slicing 하면 마지막까지 포함됨 a 10 b 20 c 30 d 40 dtype: int64 1print(ser[0:3]) #index 숫자로 slicing 했을 때는 마지막이 포함되지 않음 a 10 b 20 c 30 dtype: int64 2. Dataframe pandas의 2차원 자료구조 행과 열 1234567data=&#123;&#x27;name&#x27;:[&#x27;bella&#x27;,&#x27;charlie&#x27;], &#x27;age&#x27;:[25,27], &#x27;birthyear&#x27;:[97,95] &#125;df1 = pd.DataFrame(data, index=list(&#x27;bc&#x27;))print(df1)print(df1.info()) name age birthyear b bella 25 97 c charlie 27 95 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 2 entries, b to c Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 name 2 non-null object 1 age 2 non-null int64 2 birthyear 2 non-null int64 dtypes: int64(2), object(1) memory usage: 64.0+ bytes None 1print(df1.dtypes) # column의 datatype을 반환 name object age int64 birthyear int64 dtype: object 12print(df1.T) # dataframe을 치환함, 행과 열의 데이터를 치환한다. # 2*3 -&gt; 3*2 b c name bella charlie age 25 27 birthyear 97 95 1print(df1.values) # 값들을 반환 [[&#39;bella&#39; 25 97] [&#39;charlie&#39; 27 95]] 1print(df1.index) # index를 반환 Index([&#39;b&#39;, &#39;c&#39;], dtype=&#39;object&#39;) 1print(df1.columns) # column 명을 반환 Index([&#39;name&#39;, &#39;age&#39;, &#39;birthyear&#39;], dtype=&#39;object&#39;) 1234567data2=&#123;&#x27;color&#x27;:[&#x27;red&#x27;,&#x27;orange&#x27;,&#x27;yellow&#x27;,&#x27;green&#x27;,&#x27;blue&#x27;], &#x27;flower&#x27;:[&#x27;rose&#x27;,&#x27;daisy&#x27;,&#x27;lotus&#x27;,&#x27;tulips&#x27;,&#x27;calendula&#x27;], &#x27;season&#x27;:[&#x27;spring&#x27;,&#x27;spring&#x27;,&#x27;winter&#x27;,&#x27;summer&#x27;,&#x27;fall&#x27;], &#x27;month&#x27;:[10,5,6,7,11] &#125;df2=pd.DataFrame(data2,index=list(&#x27;abcde&#x27;))print(df2) color flower season month a red rose spring 10 b orange daisy spring 5 c yellow lotus winter 6 d green tulips summer 7 e blue calendula fall 11 1df2.head(3) # 상위데이터 3개 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower season month a red rose spring 10 b orange daisy spring 5 c yellow lotus winter 6 1df2.tail(2) # 하위데이터 2개 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower season month d green tulips summer 7 e blue calendula fall 11 1df2.sort_index(axis=0, ascending=False) #axis=0일 경우, index 이름 기준 정렬, 내림차순 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower season month e blue calendula fall 11 d green tulips summer 7 c yellow lotus winter 6 b orange daisy spring 5 a red rose spring 10 1df2.sort_index(axis=1) #axis=1일 경우, index의 column 이름 기준 정렬, 오름차순 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower month season a red rose 10 spring b orange daisy 5 spring c yellow lotus 6 winter d green tulips 7 summer e blue calendula 11 fall 1df2.sort_values(&#x27;color&#x27;) # index명이나 column명 입력하면 키값이 정렬됨 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower season month e blue calendula fall 11 d green tulips summer 7 b orange daisy spring 5 a red rose spring 10 c yellow lotus winter 6 참조 https://harryp.tistory.com/868","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Python]시각화 코드 실습","slug":"Python/Visualization_Python/spines_grids","date":"2021-04-06T15:00:00.000Z","updated":"2021-04-26T08:13:14.032Z","comments":true,"path":"2021/04/07/Python/Visualization_Python/spines_grids/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/07/Python/Visualization_Python/spines_grids/","excerpt":"","text":"블로그 실습 따라하기 Spines&amp;Grids practice 시각화 코드를 함수로 만들기1234567891011121314151617181920212223def plot_example(ax, zorder=0): # zorder(레이어의 위치) ax.bar(tips_day[&quot;day&quot;],tips_day[&quot;tip&quot;],color=&quot;lightgray&quot;, zorder=zorder) #x축: day, y축: tip ax.set_title(&quot;tip (mean)&quot;, fontsize=16, pad=12) # values h_pad=0.1 for i in range(len(ax.patches)): # axes에 들어오는 객체: 막대기 4개 즉, ax.patches 길이는 4 fontweight=&quot;normal&quot; #폰트 굵기 color=&quot;k&quot; # 폰트 색깔 if i == 3: # 일요일 fontweight=&quot;bold&quot; color=&quot;darkred&quot; ax.text(i, tips_day[&quot;tip&quot;].loc[i]+h_pad, f&quot;&#123;tips_day[&#x27;tip&#x27;].loc[i]:0.2f&#125;&quot;, # loc[i]: 요일별높이 설정해줌, f&quot;: 글자 출력 horizontalalignment=&#x27;center&#x27;,fontsize=12, fontweight=fontweight, color=color) #Sunday # 4번째가 sunday ax.patches[3].set_facecolor(&quot;darkred&quot;) # bar채우기색 ax.patches[3].set_edgecolor(&quot;black&quot;) # bar edge 색 # set Range ax.set_ylim(0,4) return ax seaborn의 tips 데이터 가져오기 12345import matplotlib.pyplot as pltimport seaborn as snstips = sns.load_dataset(&quot;tips&quot;)tips.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 12tips_day=tips.groupby(&quot;day&quot;).mean().reset_index() # 요일별 평균 데이터 tips_day .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; day total_bill tip size 0 Thur 17.682742 2.771452 2.451613 1 Fri 17.151579 2.734737 2.105263 2 Sat 20.441379 2.993103 2.517241 3 Sun 21.410000 3.255132 2.842105 12fig, ax =plt.subplots() # figure, axes 객체 생성ax=plot_example(ax) 1type(ax.spines) #dictionary의 일종 collections.OrderedDict 12for k,v in ax.spines.items(): print(f&quot;spines[&#123;k&#125;]=&#123;v&#125;&quot;) spines[left]=Spine spines[right]=Spine spines[bottom]=Spine spines[top]=Spine 1ax.spines.values() # spine은 patch의 subclass odict_values([&lt;matplotlib.spines.Spine object at 0x7f0fd2d9f890&gt;, &lt;matplotlib.spines.Spine object at 0x7f0fd2d9f050&gt;, &lt;matplotlib.spines.Spine object at 0x7f0fd2d9f1d0&gt;, &lt;matplotlib.spines.Spine object at 0x7f0fd2d9fcd0&gt;]) 1234567fig, ax=plt.subplots()ax = plot_example(ax)# set_visible(False)를 함으로써 bottom 축만 남기고 다른 축들은 숨김ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False) 1234567fig,ax = plt.subplots()ax = plot_example(ax)# spince의 일부 영역만 보고싶을 때ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_bounds(1,3) # set_bounds로 최소, 최대 영역 정해줌 1ax.spines[&#x27;left&#x27;].get_position() # left spine이 밖으로 0만큼 나가있음 (&#39;outward&#39;, 0.0) 123456fig, ax = plt.subplots()ax = plot_example(ax)ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_position((&#x27;outward&#x27;,10)) # left spine을 바깥쪽으로 10만큼 움직임 그래프 3개 한꺼번에 그리기123456789101112131415fig, ax = plt.subplots(ncols=3, figsize=(15,3))for i in range(3): #3개의 그래프 ax[i]=plot_example(ax[i]) ax[i].spines[&#x27;top&#x27;].set_visible(False) ax[i].spines[&#x27;right&#x27;].set_visible(False)# ax[0]: spine을 data영역에서 -50만큼 이동ax[0].spines[&#x27;left&#x27;].set_position((&quot;outward&quot;,-50))# ax[1]: spine을 axes의 0.3의 위치에 설정ax[1].spines[&#x27;left&#x27;].set_position((&#x27;axes&#x27;,0.3))# ax[2]: spine을 data의 2.5의 위치에 설정ax[2].spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;,2.5)) sin 그래프 그리기123456789101112131415161718192021222324252627282930313233import numpy as np## datax= np.linspace(-np.pi, np.pi,100)y=2*np.sin(x)fig, ax = plt.subplots(ncols=3, figsize=(12,4)) # 그래프 3개 만듦## normal plotax[0].plot(x,y)ax[0].set_title(&quot;normal plot&quot;, pad=12)## textbook(1)ax[1].plot(x,y)ax[1].set_title(&quot;textbook (1)&quot;, pad=12)ax[1].spines[&#x27;left&#x27;].set_visible(False)ax[1].spines[&#x27;top&#x27;].set_visible(False)ax[1].spines[&#x27;right&#x27;].set_position((&#x27;data&#x27;,0)) # (0,0) 지나가게 함ax[1].spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;,0))## textbook(2)ax[2].plot(x,y)ax[2].set_title(&quot;textbook (2)&quot;, pad=12)ax[2].spines[&#x27;left&#x27;].set_visible(False)ax[2].spines[&#x27;top&#x27;].set_visible(False)ax[2].spines[&#x27;right&#x27;].set_position((&#x27;data&#x27;,0))ax[2].spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;,0))ax[2].plot(1,0,&#x27;&gt;k&#x27;,transform=ax[2].get_yaxis_transform(),clip_on=False) # &gt;k 화살표 모양# get_yaxis_transform이면 x축 기준으로 1만큼 y기준 움직이지 않음ax[2].plot(0,1,&#x27;^k&#x27;,transform=ax[2].get_xaxis_transform(),clip_on=False) # ^k 위쪽 화살표 # get_xaxis_transform이면 y축 기준으로 x는 움직이지 않고 y는 1만큼 움직임plt.show() set_position((‘axes’,0.5)) = set_position(‘center’) set_position((‘data’,0)) = set_position(‘zero’) 12345678910111213141516fig, ax = plt.subplots(ncols=3, figsize=(15,3))for i in range(3): ax[i]=plot_example(ax[i]) ax[i].spines[&#x27;top&#x27;].set_visible(False) ax[i].spines[&#x27;right&#x27;].set_visible(False) ax[i].spines[&#x27;left&#x27;].set_position((&#x27;outward&#x27;,10))# ax[0]: x,y 둘다ax[0].grid(axis=&#x27;both&#x27;)# ax[1]: x축만ax[1].grid(axis=&#x27;x&#x27;)# ax[2]: y축만ax[2].grid(axis=&#x27;y&#x27;) 12345678910111213141516# major, minor tick 설정from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, StrMethodFormatter)fig, ax = plt.subplots()ax=plot_example(ax)ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_visible(False)# y축 tick 설정ax.yaxis.set_major_locator(MultipleLocator(1)) # y축 major tick을 1단위로 설정ax.yaxis.set_major_formatter(&#x27;&#123;x:0.5f&#125;&#x27;)ax.yaxis.set_minor_locator(MultipleLocator(0.5)) # 0.5마다 minor tick 그림plt.plot() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-180-84d5866b2f60&gt; in &lt;module&gt;() 11 12 ax.yaxis.set_major_locator(MultipleLocator(1)) # y축 major tick을 1단위로 설정 ---&gt; 13 ax.yaxis.set_major_formatter(&#39;&#123;x:0.5f&#125;&#39;) 14 ax.yaxis.set_minor_locator(MultipleLocator(0.5)) # 0.5마다 minor tick 그림 15 /usr/local/lib/python3.7/dist-packages/matplotlib/axis.py in set_major_formatter(self, formatter) 1626 formatter : `~matplotlib.ticker.Formatter` 1627 &quot;&quot;&quot; -&gt; 1628 cbook._check_isinstance(mticker.Formatter, formatter=formatter) 1629 self.isDefault_majfmt = False 1630 self.major.formatter = formatter /usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py in _check_isinstance(_types, **kwargs) 2126 &quot;, &quot;.join(names[:-1]) + &quot; or &quot; + names[-1] 2127 if len(names) &gt; 1 else names[0], -&gt; 2128 type_name(type(v)))) 2129 2130 TypeError: &#39;formatter&#39; must be an instance of matplotlib.ticker.Formatter, not a str 오류 상황: ax.yaxis.set_major_formatter(‘{x:0.5f}’) 에서 Type Error가 발생하였다. TypeError: ‘formatter’ must be an instance of matplotlib.ticker.Formatter, not a str 해결 방법: string을 입력했을 때, 내부적으로 StrMethodFormatter이 autogenerate되서 str로 교체된다.그러나 Error를 보면 자동으로 동작하지 않았기 때문에 Type Error가 뜬 것으로 보인다.따라서, StrMethodFormatter 의 함수를 입력하면 된다.from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, StrMethodFormatter) 처럼 StrMethodFormatter를 추가한 뒤,*ax.yaxis.set_major_formatter(StrMethodFormatter(‘{x:0.2f}’))*위의 코드로 변경하면 된다 . 12345678910111213141516# major, minor tick 설정from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, StrMethodFormatter)fig, ax = plt.subplots()ax=plot_example(ax)ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_visible(False)# y축 tick 설정ax.yaxis.set_major_locator(MultipleLocator(1)) # y축 major tick을 1단위로 설정ax.yaxis.set_major_formatter(StrMethodFormatter(&#x27;&#123;x:0.2f&#125;&#x27;)) # y축 단위 소수점 2자리까지 보임ax.yaxis.set_minor_locator(MultipleLocator(0.5)) # 0.5마다 minor tick 그림plt.plot() [] 1234567891011121314151617181920212223fig, ax = plt.subplots(ncols=3, figsize=(15,3))for i in range(3): ax[i]= plot_example(ax[i], zorder=2) # bar를 grid 앞으로 위치시킴 ax[i].spines[&#x27;top&#x27;].set_visible(False) ax[i].spines[&#x27;right&#x27;].set_visible(False) ax[i].spines[&#x27;left&#x27;].set_position((&#x27;outward&#x27;,10)) ax[i].yaxis.set_major_locator(MultipleLocator(1)) ax[i].yaxis.set_major_formatter(StrMethodFormatter(&#x27;&#123;x:0.2f&#125;&#x27;)) ax[i].yaxis.set_minor_locator(MultipleLocator(0.5))# ax[0]: major, minor 둘다ax[0].grid(axis=&#x27;y&#x27;,which=&#x27;both&#x27;)# ax[1]: major 만ax[1].grid(axis=&#x27;y&#x27;, which=&#x27;major&#x27;)# ax[2]: major만 &amp; optionax[2].grid(axis=&#x27;y&#x27;, which=&#x27;major&#x27;, color=&#x27;r&#x27;, lw=0.5, alpha=0.5)plt.show() 12345678910111213fig, ax = plt.subplots()ax=plot_example(ax, zorder=2)ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_visible(False)ax.yaxis.set_major_locator(MultipleLocator(1))ax.yaxis.set_major_formatter(StrMethodFormatter(&#x27;&#123;x:0.2f&#125;&#x27;))ax.yaxis.set_minor_locator(MultipleLocator(0.5))ax.grid(axis=&#x27;y&#x27;,which=&#x27;major&#x27;, color=&#x27;lightgray&#x27;)ax.grid(axis=&#x27;y&#x27;, which=&#x27;minor&#x27;, ls=&#x27;:&#x27;) # 점선으로 minor 부분 그리기 출처 Jehyunlee: spines and grids Formatter type error solution 참고","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Python] List vs Tuple","slug":"Python/speed_list_tuple","date":"2021-04-05T15:00:00.000Z","updated":"2021-04-26T08:13:14.035Z","comments":true,"path":"2021/04/06/Python/speed_list_tuple/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/06/Python/speed_list_tuple/","excerpt":"","text":"List vs Tuple in python차이점 list는 mutable 즉, 생성된 후에 변경 가능하다.tuple은 immutable 즉, 생성된 후에 변경이 불가능하다. list는 dictionary의 key값으로 쓸 수 없지만 tuple은 가능하다.\\ why? key값은 불변한 값만 올 수 있기 때문!만약, tuple에 list가 들어있다면?=&gt; key 값으로 사용 불가능함! 참고) 문자열 또한 불변한 값이므로 dictionary의 key값으로 사용 가능하다. list 대신 tuple을 사용하는 이유는?메모리 크기 비교12345list_1 = [&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;]tuple_1 = &#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;import sysprint(sys.getsizeof(list_1))print(sys.getsizeof(tuple_1)) 104 88 tuple이 메모리를 차이하는 것이 더 작은 것을 볼 수 있다. 생성 속도12%timeit list_2=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;]%timeit tuple_2=&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27; 10000000 loops, best of 5: 57.5 ns per loop 100000000 loops, best of 5: 16.4 ns per loop tuple의 생성속도가 더 빠른 것을 확인할 수 있다. 인덱싱 속도12345list_3=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;]%timeit list_3[0]tuple_3=&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;%timeit tuple_3[0] 10000000 loops, best of 5: 43 ns per loop 10000000 loops, best of 5: 42.8 ns per loop tuple이 list보다 indexing 으로 데이터에 접근하는 속도가 더 빠르다. 결과 메모리 크기비교, 생성속도, 인덱싱 속도에서 모두 tuple의 결과가 더 빠른것을 확인 할 수 있었다. 출처 https://codacoding.tistory.com/36 https://itholic.github.io/python-list-tuple/","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Python]시각화 블로그 연습","slug":"Python/Visualization_Python/hexo_python","date":"2021-04-05T15:00:00.000Z","updated":"2021-04-26T08:13:14.030Z","comments":true,"path":"2021/04/06/Python/Visualization_Python/hexo_python/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/06/Python/Visualization_Python/hexo_python/","excerpt":"","text":"파이썬 시각화 블로그 연습1234567891011121314import matplotlib.pyplot as pltdates = [ &#x27;2021-01-01&#x27;, &#x27;2021-01-02&#x27;, &#x27;2021-01-03&#x27;, &#x27;2021-01-04&#x27;, &#x27;2021-01-05&#x27;, &#x27;2021-01-06&#x27;, &#x27;2021-01-07&#x27;, &#x27;2021-01-08&#x27;, &#x27;2021-01-09&#x27;, &#x27;2021-01-10&#x27;]min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0]fig,axes = plt.subplots(nrows=1, ncols=1, figsize=(10,6))axes.plot(dates, min_temperature, label = &#x27;Min Temperature&#x27;)axes.plot(dates, max_temperature, label = &#x27;Max Temperature&#x27;)axes.legend()plt.show()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Statistic]기초 통계-상관분석","slug":"Python/Statistic/statistic","date":"2021-04-05T06:08:52.000Z","updated":"2021-04-26T08:13:14.036Z","comments":true,"path":"2021/04/05/Python/Statistic/statistic/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/05/Python/Statistic/statistic/","excerpt":"","text":"상관분석과 로지스틱회귀분석상관분석개념(정의) 연속 변수로 측정된 두 변수간의 선형 관계를 분석하는 기법 X가 증가함에 따라 B도 증가되는지 혹은 감소하는지를 분석기본 가정사항 두 변수 중 적어도 하나의 변수는 정규분포일 것 정규성 검사는 “shapiro.test()” 사용 만약, 두 변수 모두 정규성을 만족하지 못한다면? Spearman, Kendall 상관계수 사용! 정규성 검정에서 정규분포를 따르지 않거나 표본의 개수가 10개 미만일 때 사용 연속형 두 변수 간에는 선형적인 관계가 있어야함 분석을 실시하기 전, 반드시 두 변수의 산점도를 통해 확인해야함 공분산(Corvariance) 2개의 확률 변수의 상관정도를 나타내는 값 만약 하나의 값이 상승하는 경향을 보이면서 다른 값도 상승하면, 이때 공분산의 값은 양수! 하나의 값이 상승하는데 반대로 다른 값은 하강하면, 이때 공분산 값은 음수! 공분산 값만으로는 상승 or 하강의 경향을 알 수는 있으나 어느정도의 상관관계인지는 파악할 수 없음 따라서 공분산을 표준화 시킨 “상관계수”를 통해 파악하자! 상관관계 &amp; 상관계수(Correlation Coefficient) 상관관계 두 변수의 선형적인 관계 정도를 나타냄 일반적으로, 피어슨 상관계수를 의미 상관계수 피어슨 상관계수(Pearson) 상관계수(r)의 값: -1.0 &lt;= r &lt;= -0.7 (강한 음적 선형관계)-0.7&lt;= r &lt;= -0.3 (뚜렷한 음적 선형관계)-0.3&lt;= r &lt;= -0.1 (약한 음적 선형관계)-0.1&lt;= r &lt;= +0.1 (거의 무시될 수 있는 선형관계)+0.1&lt;= r &lt;= +0.3 (약한 양적 선형관계)+0.3&lt;= r &lt;= +0.73 (뚜렷한 양적 선형관계)+0.7&lt;= r &lt;= +1.0 (강한 양적 선형관계) 로지스틱 회귀분석로지스틱 회귀분석이 필요한 이유는?GLM(Generalized Linear Model) 비선형을 선형적으로 일반화 시킨 모형 이유는? 선형모델에서만 사용할 수 있는 모형의 해석, 확장, 수정 등을 이용하기 위해! 선형화 시키는 대상은? 관심 범주에 속할 확률 알코올 섭취량과 비만일 확률은 높아지지만 완전한 선형이라기 보다는 약간의 커브가 존재한다. 여기에 일반 선형회귀 라인을 넣으면 위와 같이 선형회귀선이 얼추 맞는다고 생각이 들 수 있다. 그렇지만 이 경우에는 음의 값이나 1을 초과하는 예측값을 제시할 수도 있는 가능성이 있다. 이러한 구조적 문제로 추가분석이 불가능하게 된다 그렇다면 비선형적인 모형을 넣어보면 어떨까? 훨씬 데이터에 잘 적용되는 것을 알 수 있다. 그렇지만 비선형모델은 여러 가지 추가 분석 제약이 있다! 따라서 이런 제약을 극복하기 위해 이를 선형화하는 것이 바로!! 로지스틱 회귀분석!! 이러한 특징을 만족하는 함수가 “시그모이드 함수”이다. 시그모이드 함수(로지스틱 반응함수)를 Odds에 넣는다. 확률p의 범위(0,1) -&gt; Odds(p)의 범위는(0,∞) 이다. Odds가 클수록 데이터가 해당 집단에 속할 확률이 큼 Odds에 log를 취하는데 이유는 선형으로 만들기 위해! 입력값(독립변수)의 범위가 –무한대에서 +무한대일 때, 출력값(종속변수)의 범위를 0에서 1로 변환시켜줌 X1, x2,…,xq가 어느 값을 가지더라도 항상 0과 1의 값을 가지게 된다. 1) 정의: 분류모델(기법) 일반적인 회귀분석과 동일하게 종속변수와 독립변수간의 관계를 함수로 나타내어 예측모델로 사용 특정기준(정답)에 의해 분석 대상을 특정개수의 집단으로 분류하는 예측모형 학습된 모델을 통해, 입력된 값을 미리 정해진 결과를 분류해주는 모델 이항형 로지스틱 회귀(Binary Logistic Regression): 2개의 범주Ex) 주택소유(있다/없다) 다항형 로지스틱 회귀(Multinomial Logistic Regression): 3개 이상의 범주Ex) 소유주택현황(아파트/단독주택/연립) 2) Odds(승산비) Odds란? 예측변수 1단위 변경 이후의 승산/ 원래 승산= 데이터가 어떤 집단에 속할 확률/ 속하지 않을 확률\\ 예를 들면, 나이의 Odds비가 0.969라면 나이가 한 단위 증가할수록 사망률이 0.969배 증가한다는 의미이다. 다시 말해, 생존율이 0.041배 감소한다. 3) 최대우도법 검정(Likelihood Ratio Test) 일반적인 회귀분석과 다르게 최대우도법 검정으로 추정을 한다!! 선형 회귀분석: 최소제곱법 로지스틱 회귀분석: 최대우도법 원하는 값이 나올 확률을 최대로 만드는 모수를 선택하는 방법 즉, 주어진 현상이 있을 때, 이 현상이 추출될 가능성을 가장 높게한다. 4) 이탈도(Deviance) 모형의 적합도의 측도 두 모형의 로그가능도 함수 값의 차이가 유의한지 보는 것 이때 포화모형이란 추정해야할 모수의 수가 데이터의 수와 동일한 상태 이탈도가 크면 그 모형은 적합하지 않다는 뜻 만약, p-value(&gt;0.05)이 클 때 모형 M이 의미있다는 뜻 5) AIC 이탈도와 다르게 로그가능도 함수 값과 모형에 사용된 모수의 수를 동시에 고려 로그가능도함수 값이 높으면 가산점을 준다!사용된 모수의 수가 많으면 패널티를 준다!따라서 AIC가 작을수록 바람직한 모형 Ex) 2개의 모형 모수를 이용한 모형의 로그가능도함수 값(LL)이 -16,4개의 모수만을 이용한 모형의 로그가능도함수 값(LL)이 -15 일 때,각 모형의 AIC =&gt; 36, 38 이다.비록 두번째 모형의 로그가능도함수 값이 -15로 더 컸지만,모형에 사용된 모수의 개수가 첫번째 모형보다 2개가 더 사용되었기 때문에 바람직하지 않다고 볼 수 있다. 6) 추가 Residual deviance(잔차이탈도) : 작을수록 좋고, 카이제곱분포를 따르기 때문에 카이제곱 적합도 검정을 통해 모형이 적합한지 확인가능 null deviance(0이탈도) : 아무런 변수 없이 상수항만 있을 때의 이탈도, 데이터가 전혀없는 최악의 상황 staturated model(포화모형) : 추정해야할 모수의 수가 데이터의 수와 동일한 상태 즉, 데이터가 10000개이면 추정할 모수의 수도 10000개 Null model(영 모델): 절편항만 가지는 모형으로 추정할 모수가 1개임 Proposed Model(제안모형): p개의 모수+ 절편항을 포함하는 모형으로, 추정할 모수가 p+1 개임. 참조 상관분석 상관분석 R 로지스틱회귀 이탈도 로그가능도, 이탈도","categories":[],"tags":[{"name":"statistic","slug":"statistic","permalink":"https://eunjin-jun717.github.io/tags/statistic/"}]},{"title":"[R] Encoding error in R","slug":"TroubleShooting/error2","date":"2021-03-30T12:06:12.000Z","updated":"2021-04-26T08:13:14.049Z","comments":true,"path":"2021/03/30/TroubleShooting/error2/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/30/TroubleShooting/error2/","excerpt":"","text":"오류 상황 12Not all characters in C:&#x2F;Users&#x2F;study&#x2F;Desktop&#x2F;analysis&#x2F;logistic_hd.R could be decoded using CP949. To try a different encoding, choose &quot;File | Reopen with Encoding...&quot; from the main menu. R 파일을 여는데 위와 같은 오류문구가 발생하면서 글자가 거의 모두 깨져있었다. 해결방안 File -&gt; Reopen with encoding -&gt; UTF-8로 설정 설정을 완료하면 아래와 같이 해결된 것을 볼 수 있다. 오류가 난 이유 Mac OS나 Linux에서의 UTF-8로 인코딩되어 있는 상태에서 자료가 Windows로 넘어올 때 글자가 깨지는 현상이 발생할 수도 있다. 참조 https://nittaku.tistory.com/341","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://eunjin-jun717.github.io/tags/hexo/"}]},{"title":"[Hexo] troubleshooting in Hexo","slug":"TroubleShooting/error","date":"2021-03-29T12:23:23.000Z","updated":"2021-04-26T08:13:14.048Z","comments":true,"path":"2021/03/29/TroubleShooting/error/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/29/TroubleShooting/error/","excerpt":"","text":"오류 상황: npm을 통해서 설치를 완료하였는데도 아래와 같은 오류가 발생하였다. 1bash: hexo: command not found 해결 방안 우선 node와 npm이 제대로 설치되었는지 확인할 것 12$ node -v$ npm -v 정상적으로 설치되어 있다면, 다음 step을 따를 것step 1) 자신의 blog 폴더 (ex. myblog) -&gt; node_modules -&gt; .bin경로를 복사함 step 2) 시스템 속성 -&gt; 고급 탭 -&gt; 환경 변수 step 3) 시스템 변수에서 Path 클릭 -&gt; 편집 클릭 -&gt; 새로만들기 -&gt; 경로 붙여넣기 -&gt; 확인 step 4) 다시 열어서 hexo server로 테스트해보기 123$ hexo serverINFO Hexo is running at http:&#x2F;&#x2F;localhost:4000 . Press Ctrl+C to stop. bash: hexo: command not found가 나오지 않는다면 해결!! 참조 https://www.programmersought.com/article/45443350618/","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://eunjin-jun717.github.io/tags/hexo/"}]},{"title":"[R] apply함수와 for문의 속도 비교","slug":"R/for_apply","date":"2021-03-27T15:00:00.000Z","updated":"2021-04-26T08:13:14.047Z","comments":true,"path":"2021/03/28/R/for_apply/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/28/R/for_apply/","excerpt":"","text":"개념 정리apply 함수 apply(X, MARGIN, FUN, …) X: 배열, 매트릭스 Margin: 행(1), 열(2) Fun: 함수 12my.matrx &lt;- matrix(c(1:10, 11:20, 21:30), nrow = 10, ncol=3) # 행이 10개, 열이 3개인 매트릭스 생성my.matrx 1234567891011## [,1] [,2] [,3]## [1,] 1 11 21## [2,] 2 12 22## [3,] 3 13 23## [4,] 4 14 24## [5,] 5 15 25## [6,] 6 16 26## [7,] 7 17 27## [8,] 8 18 28## [9,] 9 19 29## [10,] 10 20 30 1apply(my.matrx, 1, sum) # 매트릭스를 행단위 합 계산 1## [1] 33 36 39 42 45 48 51 54 57 60 1apply(my.matrx, 2, sum) # 열단위 합 계산 1## [1] 55 155 255 1apply(my.matrx, 2, function(x) length(x)) # 직접 함수를 정의해서 사용 가능 1## [1] 10 10 10 lapply 함수 lapply(X,FUN, …) X: 벡터, 리스트 반환값: 리스트12vec &lt;- c(1:10)vec 1## [1] 1 2 3 4 5 6 7 8 9 10 1lapply(vec, sum) #list 형태로 나옴 1234567891011121314151617181920212223242526272829## [[1]]## [1] 1## ## [[2]]## [1] 2## ## [[3]]## [1] 3## ## [[4]]## [1] 4## ## [[5]]## [1] 5## ## [[6]]## [1] 6## ## [[7]]## [1] 7## ## [[8]]## [1] 8## ## [[9]]## [1] 9## ## [[10]]## [1] 10 12345A &lt;- c(1:9)B &lt;- c(1:12)C &lt;- c(1:15)my.lst &lt;- list(A,B,C)my.lst 12345678## [[1]]## [1] 1 2 3 4 5 6 7 8 9## ## [[2]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12## ## [[3]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1lapply(my.lst, sum) # 각 list 안의 값들의 합 12345678## [[1]]## [1] 45## ## [[2]]## [1] 78## ## [[3]]## [1] 120 sapply 함수 sapply(X,FUN,…,simplify=TRUE, USE.NAMES=TRUE) lapply와 같은 동작을 하지만, 가능하면 출력을 단순화 시키는 함수 simplify=TRUE : 출력을 단순화시킴, simplify=FALSE : 단순화 시키지 않음 USE.NAMES=TRUE : 이름 속성도 반환, USE.NAMES=FALSE : 이름 속성 없이 반환 1vec 1## [1] 1 2 3 4 5 6 7 8 9 10 1sapply(vec, sum, simplify=FALSE) # 출력을 단순화 하지 않으므로 리스트 형태로 출력 1234567891011121314151617181920212223242526272829## [[1]]## [1] 1## ## [[2]]## [1] 2## ## [[3]]## [1] 3## ## [[4]]## [1] 4## ## [[5]]## [1] 5## ## [[6]]## [1] 6## ## [[7]]## [1] 7## ## [[8]]## [1] 8## ## [[9]]## [1] 9## ## [[10]]## [1] 10 1sapply(vec, sum, simplify=TRUE) # 출력을 단순화시킴 1## [1] 1 2 3 4 5 6 7 8 9 10 1my.lst 12345678## [[1]]## [1] 1 2 3 4 5 6 7 8 9## ## [[2]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12## ## [[3]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1sapply(my.lst, sum) # 출력을 단순화시킴 # simplify=TRUE 생략된 모습 1## [1] 45 78 120 1lapply(my.lst, sum) # 리스트를 반환 12345678## [[1]]## [1] 45## ## [[2]]## [1] 78## ## [[3]]## [1] 120 vapply 함수 vapply(X,FUN,FUN.VALUE,…,USE.NAMES=TRUE) sapply함수와 비슷함. 차이점: value로 예상되는 데이터 유형을 지정해야함 FUN.VALUE: 자료형 지정 1vec 1## [1] 1 2 3 4 5 6 7 8 9 10 1vapply(vec, sum, numeric(1)) # 1개의 숫자데이터로 나오게함 1## [1] 1 2 3 4 5 6 7 8 9 10 1my.lst 12345678## [[1]]## [1] 1 2 3 4 5 6 7 8 9## ## [[2]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12## ## [[3]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1vapply(my.lst, sum, numeric(1)) # 1개의 숫자데이터로 나오게함 1## [1] 45 78 120 12#vapply(my.lst, function(x) x, numeric(1)) # error 뜸#1개의 데이터만 들어갈 수 있는데 함수의 결과는 9개가 나오므로 error뜸 tapply 함수 tapply(X, INDEX, FUN=NULL, …, DEFAULT=NA,SIMPLIFY=TRUE) 그룹별로 처리함 factor형으로 인자를 줌 12tdata &lt;- as.data.frame(cbind(c(1,1,1,1,1,2,2,2,2,2), my.matrx)) # 열단위로 결합시킴tdata 1234567891011## V1 V2 V3 V4## 1 1 1 11 21## 2 1 2 12 22## 3 1 3 13 23## 4 1 4 14 24## 5 1 5 15 25## 6 2 6 16 26## 7 2 7 17 27## 8 2 8 18 28## 9 2 9 19 29## 10 2 10 20 30 1colnames(tdata) # tdata의 열 이름 1## [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; 1tapply(tdata$V2, tdata$V1, sum) # V1: index이며, V2가 함수의 인자로 전달됨 12## 1 2 ## 15 40 1# index가 1인 그룹으로 sum되고, index가 2인 그룹을 sum함 mapply 함수 mapply(FUN,…,MOREARGS=NULL, SIMPLIFY=TRUE, USE.NAMES=TRUE) sapply()와 유사하지만 mapply()는 다수의 인자를 함수에 넘길 수 있음 1mapply(rep, 1:10, 10:1) # 반복함수, 반복할 숫자, 반복되는 갯수 1234567891011121314151617181920212223242526272829## [[1]]## [1] 1 1 1 1 1 1 1 1 1 1## ## [[2]]## [1] 2 2 2 2 2 2 2 2 2## ## [[3]]## [1] 3 3 3 3 3 3 3 3## ## [[4]]## [1] 4 4 4 4 4 4 4## ## [[5]]## [1] 5 5 5 5 5 5## ## [[6]]## [1] 6 6 6 6 6## ## [[7]]## [1] 7 7 7 7## ## [[8]]## [1] 8 8 8## ## [[9]]## [1] 9 9## ## [[10]]## [1] 10 1tdata 1234567891011## V1 V2 V3 V4## 1 1 1 11 21## 2 1 2 12 22## 3 1 3 13 23## 4 1 4 14 24## 5 1 5 15 25## 6 2 6 16 26## 7 2 7 17 27## 8 2 8 18 28## 9 2 9 19 29## 10 2 10 20 30 12tdata$V5 &lt;- mapply(function(x,y) x*y, tdata$V1, tdata$V2) # V1과 V2의 값들에 대한 곱tdata 1234567891011## V1 V2 V3 V4 V5## 1 1 1 11 21 1## 2 1 2 12 22 2## 3 1 3 13 23 3## 4 1 4 14 24 4## 5 1 5 15 25 5## 6 2 6 16 26 12## 7 2 7 17 27 14## 8 2 8 18 28 16## 9 2 9 19 29 18## 10 2 10 20 30 20 For문, apply함수 속도 비교(1)For문 사용 시 속도123456789101112# 랜덤한 10000개의 숫자를 x1, x2에 저장N &lt;- 10000x1 &lt;- runif(N) # runif() : 랜덤숫자 발생함수x2 &lt;- runif(N)# x1과 x2를 열단위로 묶어서 d에 data frame 형태로 저장d &lt;- as.data.frame(cbind(x1, x2))# for loop으로 시간 체크system.time(for(i in c(1:length(d[,1])))&#123; # 1부터 &#x27;d의 1열 길이&#x27;만큼 for 문이 반복함 d$mean2[i] &lt;- mean(c(d[i,1], d[i,2])) # x1, x2 각각 더해 평균을 구한것을 d의 데이터 프레임에 넣음&#125;) 12## user system elapsed ## 0.80 0.05 0.84 For문은 1부터 10000까지 한 명이 순차적으로 일을 처리한다. apply 함수사용 시 속도12# apply 함수를 사용하여 같은 데이터를 처리해보자.system.time(d$mean1 &lt;- apply(d, 1, mean)) # d의 행에 대한 평균 12## user system elapsed ## 0.06 0.00 0.06 For문, apply함수 속도 비교(2)1install.packages(&#x27;nycflights13&#x27;, repos=&quot;http://cran.us.r-project.org&quot;) 1234## package &#39;nycflights13&#39; successfully unpacked and MD5 sums checked## ## The downloaded binary packages are in## C:\\Users\\study\\AppData\\Local\\Temp\\Rtmp08X2ug\\downloaded_packages 1install.packages(&#x27;dplyr&#x27;, repos=&quot;http://cran.us.r-project.org&quot;) 1234## package &#39;dplyr&#39; successfully unpacked and MD5 sums checked## ## The downloaded binary packages are in## C:\\Users\\study\\AppData\\Local\\Temp\\Rtmp08X2ug\\downloaded_packages 12library(dplyr)library(nycflights13) apply 함수사용 시 속도1head(flights) 123456789101112## # A tibble: 6 x 19## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;## 1 2013 1 1 517 515 2 830 819## 2 2013 1 1 533 529 4 850 830## 3 2013 1 1 542 540 2 923 850## 4 2013 1 1 544 545 -1 1004 1022## 5 2013 1 1 554 600 -6 812 837## 6 2013 1 1 554 558 -4 740 728## # ... with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1234567891011flights_selected &lt;- select(flights, flight, distance) # flight number와 distance를 뽑음data_flight &lt;- tapply(flights_selected$distance, flights_selected$flight, sum) # 데이터 양이 많아 우선적으로 각각의 flight마다 간 거리를 합함.df &lt;- data.frame(flight_num= c(1:length(data_flight)), flight_distance= data_flight)# data frame 형태로 만듦 (인덱스 길이= flight 갯수)# flight_num와 distance의 평균을 구함, 의미는 없으며 속도차이를 보기 위함!## apply문 사용 시 속도 # 행단위로 평균을 구함system.time(df$mean3 &lt;- apply(df, 1, mean)) 12## user system elapsed ## 0.03 0.00 0.03 1234## For문 사용 시 속도system.time(for(i in c(1:length(data_flight)))&#123; df$mean2[i] &lt;- mean(c(df[i,1], df[i,2]))&#125;) 12## user system elapsed ## 0.28 0.02 0.29 apply함수 사용 시 장점 For문과 비교해봤을 때, apply함수 사용 시 속도가 빠음 대용량 데이터에 대해 짧은 코드로 반복 연산 처리 가능 참조 https://ademos.people.uic.edu/Chapter4.html#:~:text=Apply%20functions%20are%20a%20family,and%20often%20require%20less%20code. http://rstudio-pubs-static.s3.amazonaws.com/5526_83e42f97a07141e88b75f642dbae8b1b.html","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}]},{"title":"[R] 주택 가격: 시각화 예제","slug":"R/Visualization_R/house_miniproj_eunjin","date":"2021-03-24T15:00:00.000Z","updated":"2021-04-26T08:13:14.042Z","comments":true,"path":"2021/03/25/R/Visualization_R/house_miniproj_eunjin/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/25/R/Visualization_R/house_miniproj_eunjin/","excerpt":"","text":"참조kaggle: house prices_ project r chunk에서 message=FALSE를 하게 되면 패키지를 불러올때의 관련 메세지가 안보임 12345678910111213library(knitr) # 마크다운 관련 패키지library(ggplot2) # 시각화 패키지library(plyr) # 데이터 분석, 가공 패키지library(dplyr) # 데이터 가공 패키지library(corrplot) # 상관행렬과 신뢰구간의 그래프library(caret) # 전처리, 모형 시각화library(gridExtra) # 시각화 이미지 분할library(scales) # ggplot2에 의해 사용된 scaling 인프라 제공library(Rmisc) # 데이터 분석, 유틸리티 동작library(ggrepel) # 겹치는 텍스트 레이블 제거library(randomForest) # 분류와 회귀를 위해 Breiman의 random forest algorithm 구현library(psych) # 기술통계량 구해주는 패키지#library(xgboost) # 머신러닝 패키지 R로 train.csv파일과 test.csv파일을 부름 12train &lt;-read.csv(&quot;train.csv&quot;, stringsAsFactors = F)test &lt;- read.csv(&quot;test.csv&quot;, stringsAsFactors = F) 1dim(train) # train의 데이터프레임 길이 1## [1] 1460 81 1str(train[,c(1:10, 81)]) # 처음 10개의 변수와 response variable 123456789101112## &#39;data.frame&#39;: 1460 obs. of 11 variables:## $ Id : int 1 2 3 4 5 6 7 8 9 10 ...## $ MSSubClass : int 60 20 60 70 60 50 20 60 50 190 ...## $ MSZoning : chr &quot;RL&quot; &quot;RL&quot; &quot;RL&quot; &quot;RL&quot; ...## $ LotFrontage: int 65 80 68 60 84 85 75 NA 51 50 ...## $ LotArea : int 8450 9600 11250 9550 14260 14115 10084 10382 6120 7420 ...## $ Street : chr &quot;Pave&quot; &quot;Pave&quot; &quot;Pave&quot; &quot;Pave&quot; ...## $ Alley : chr NA NA NA NA ...## $ LotShape : chr &quot;Reg&quot; &quot;Reg&quot; &quot;IR1&quot; &quot;IR1&quot; ...## $ LandContour: chr &quot;Lvl&quot; &quot;Lvl&quot; &quot;Lvl&quot; &quot;Lvl&quot; ...## $ Utilities : chr &quot;AllPub&quot; &quot;AllPub&quot; &quot;AllPub&quot; &quot;AllPub&quot; ...## $ SalePrice : int 208500 181500 223500 140000 250000 143000 307000 200000 129900 118000 ... 123test_labels &lt;- test$Id # 테스트 Id는 test_labels에 저장test$Id &lt;- NULLtrain$Id &lt;- NULL 123test$SalePrice &lt;- NA # Not Available 결측치, NULL은 출력되지 않지만 NA는 출력됨all &lt;- rbind(train, test) # train과 test 결합dim(all) # 결합된 것을 all이라하며 데이터 프레임 길이 구함 1## [1] 2919 80 1#Id가 없으므로 (데이터프레임 = 79개의 예측변수들 + SalePrice 인 response 변수) 123456ggplot(data = all[!is.na(all$SalePrice),], #SalePrice에 결측값인 NA가 포함되어있는지 확인함 # 이때 결측값이 아닌 SalePrice 만 선택 aes(x= SalePrice))+ # x축 : SalePrice geom_histogram(fill=&quot;blue&quot;, binwidth = 10000)+ # 히스토그램으로 표현, bar은 blue, bar의 두께는 10000으로 설정 scale_x_continuous(breaks=seq(0, 800000, by=100000), # x축의 범위는 0~800000, 100000단위로 끊어줌 labels= comma) # 숫자 3자리마다 &#x27;,&#x27; 넣음 히스토그램을 보면 좌측으로 치우쳐져 있다.이 말은 SalePrice가 낮은 집이 잘 팔린다는 뜻이고, SalePrice가 높은 집은 사는 사람이 거의 없다는 것을 의미한다. 1summary(all$SalePrice) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 34900 129975 163000 180921 214000 755000 1459 SalePrice의 최저가격: 34,900, 최고가격: 755,000,1사 분위수(1st Qu): 129,975 =&gt; 오름차순으로 정렬했을 때, 하위 25%의 SalePrice중앙값(Median): 163,000 =&gt; 오름차순으로 정렬했을 때, 중앙에 있는 값인 SalePrice평균값(Mean): 180,921 =&gt; SalePrice의 평균1사 분위수(3rd Qu): 214,000 =&gt; 오름차순으로 정렬했을 때, 상위 25%의 SalePriceNA’s : 결측값 123numericVars &lt;- which(sapply(all, is.numeric)) # numeric 변수들의 위치를 찾는데 그 결과를 벡터 또는 행렬로 반환numericVarNames &lt;- names(numericVars) # 저장된 numeric 변수들인 numericVars로 변수명 변경된 것을 Name벡터인 numericVarNames로 저장cat(&#x27;There are&#x27;, length(numericVars), &#x27;numeric variables&#x27;) #cat함수는 행을 바꾸지 않음 1## There are 37 numeric variables 123456789101112131415#numericVars의 갯수를 출력함all_numVar &lt;- all[, numericVars] # numericVars의 데이터들을 &#x27;all_numVar&#x27;에 저장 (numericVars는 그냥 벡터일뿐, 데이터프레임이 아님)cor_numVar &lt;- cor(all_numVar, use=&quot;pairwise.complete.obs&quot;) # all_numVar들의 상관관계를 저장# use=&quot;pairwise.complete.obs&quot; =&gt; 결측치가 포함된 데이터에서 상관관계를 구하기 위해 사용cor_sorted &lt;- as.matrix(sort(cor_numVar[, &#x27;SalePrice&#x27;], decreasing = TRUE)) # SalePrice와의 상관관계만을 내림차순으로 정렬한 뒤 행렬로 변환한 것을 cor_sorted 행렬에 저장CorHigh &lt;- names(which(apply(cor_sorted, 1, function(x) abs(x)&gt;0.5))) # 1:행, 2:열, function(x) &#123; abs (x)&#125;# 행단위로 SalePrice의 상관관계를 절댓값 형태로 취한 뒤 0.5 이상의 값만 추출함# 추출된 것들의 변수명을 변경하고 CorHigh에 저장 =&gt; 0.5이상의 상관관계는 &quot;관련이 높다&quot;라는 의미cor_numVar &lt;- cor_numVar[CorHigh, CorHigh] # 0.5이상의 상관관계를 가진 값들로만 cor_numVar에 다시 저장# tl.col(text legend color은 black), tl.pos(text legend position은 left와 top)# corrplot.mixed 함수: 시각화 방법을 혼합할 때 사용corrplot.mixed(cor_numVar, tl.col= &quot;black&quot;, tl.pos=&quot;lt&quot;) #tl: text legend, cl: color legend 상관관계가 0.5이상인 데이터들을 봤을 때, SalePrice와 가장 높은 상관관계를 가지는 것은 “OverallQual”인 전반적인 품질이었다. =&gt; 0.791 독립변수들 간에 강한 상관관계가 나타나는 문제를 multicollinearity (다중공선성)라고 하는데 이것이 문제인 것으로 보인다. SalePrice와 가장 상관관계가 높은 OverallQual를 제외하고 다음으로 높은 GrLivArea 와 GarageCars 사이의 상관관계를 보았을 때 매우 높은 0.8897 이다. 즉, 독립변수들 간의 강한 상관관계를 나타낸다. 상관관계가 0.5 이상인 높은 상관관계를 가진 변수는 나머지 6개가 있다.TotalBsmtSF, 1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt, YearRemodAdd","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}],"author":"Eunjin"},{"title":"[Hexo] What is the main and branch in Github","slug":"Hexo/meaning","date":"2021-03-23T12:40:58.000Z","updated":"2021-04-26T08:13:14.029Z","comments":true,"path":"2021/03/23/Hexo/meaning/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/23/Hexo/meaning/","excerpt":"","text":"Main 과 Branch 그림에서 보이는 main과 branch의 역할이 무엇인지 궁금하여 찾아보았다. Branch란? 여러 개발자들이 동시에 다양한 작업을 할 수 있게 만들어 주는 기능 각자 독립적인 저장소안에서 소스코드를 원하는대로 작업할 수 있다. 분리된 작업들을 나중에 하나의 새로운 버전으로 만들어낼 수 있다. 장점: 여러 작업들을 동시에 진행할 수 있다. 문제가 발생했을 경우 원인이 되는 작업을 찾아내 대책을 세우기 쉽다.Main이란? default branch 참조https://backlog.com/git-tutorial/kr/stepup/stepup1_1.html","categories":[],"tags":[{"name":"github","slug":"github","permalink":"https://eunjin-jun717.github.io/tags/github/"}]},{"title":"[R] R코딩 기초함수","slug":"R/R-edu2","date":"2021-03-22T15:00:00.000Z","updated":"2021-06-07T02:39:35.823Z","comments":true,"path":"2021/03/23/R/R-edu2/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/23/R/R-edu2/","excerpt":"","text":"Visualization Practice using “counties”파일 불러오기 get working directory 라는 뜻으로 현재 위치를 알려준다.1getwd() set as working directory 라는 뜻으로 위치를 새로 설정할 수 있다.1setwd(&#x27;R_NCS_2020/1_day/&#x27;) dplyr 설치 방법 2가지 method1) tidyverse 설치 12install.packages(&quot;tidyverse&quot;)library(tidyverse) method2) dplyr만 설치 12install.packages(&quot;dplyr&quot;)library(dplyr) 데이터 가져오기 data 폴더에 있는 counties.xlsx 파일을 가져온다.만약 경로 error가 뜨면 getwd()를 하여 현재 파일 위치가 잘못되어있는지 확인해본다. 1counties &lt;- readxl::read_xlsx(&quot;data/counties.xslx&quot;, sheet = 1) 데이터 확인 glimpse 를 사용하여 counties에 무슨 변수들이 있는지 살펴볼 수 있다. 1glimpse(counties) Select 사용하여 원하는 변수들 추출 counties 변수들 중에서 state, region, men, women, population 변수들의 데이터만 고른다.select한 것을 counties_selected라는 변수 이름으로 저장한다. counties_selected 를 state 기준으로 내림차순 정렬한다. 12345counties_selected &lt;- counties %&gt;% select(state, region, men, women, population) counties_selected %&gt;% arrange(desc(state)) Filter 사용하여 불필요한 것 제거하고 보기 population이 10000명 이하인 것들만 보여준다.12counties_selected %&gt;% filter(population &lt; 10000) 조건이 두가지일 경우에는 아래와 같이 한다. state가 California이거나 population이 100000명 이상인 것을 추출한다. 12counties_selected %&gt;% filter(state == &quot;California&quot; | population &gt; 100000) Arange 함수 오름차순, 내림차순으로 정렬 가능하다.123counties_selected %&gt;% filter(state == &quot;California&quot; | population &gt; 100000) %&gt;% arrange(desc(public_work)) Mutate 함수 새로 변수를 추가한다는 것 보다는 의미있는 데이터 발견하고자 할 때 사용한다. public workers를 구해서 배열하는 코드이다. 123456counties_seletec &lt;- counties %&gt;% select(state, county, private_work, public_work, population)counties_seleteced %&gt;% mutate(public_workers = population * private_work /100) %&gt;% arrange(public_workers) Count 함수 각 행의 갯수를 셀 수 있고, 정렬까지 가능하다. sort=TRUE이면 자동으로 내림차순12counties_selected %&gt;% count(state, sort =TRUE) 가중치를 두어 주별로 걸어서 출퇴근하는 사람의 수를 카운트할 수 있다. 12counties_selected %&gt;% count(state, wt= walkers_pop) 참조 https://github.com/dschloe/R_edu","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}]},{"title":"[R] R코딩 기초","slug":"R/R-edu1","date":"2021-03-22T15:00:00.000Z","updated":"2021-06-07T02:39:35.826Z","comments":true,"path":"2021/03/23/R/R-edu1/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/23/R/R-edu1/","excerpt":"","text":"R 코딩 기초실행방법 Windows: Ctrl + Enter Mac: Command + Enter 변수 저장 시1a &lt;- 3 Alt + - key 누르면 된다. 변수 호출 시123print(x) #혹은x 변수 종류변수 type 숫자형 변수 1my_numeric &lt;- 17 문자형 변수 1my_character &lt;- &quot;eunjin&quot; 논리형 변수 12my_logical &lt;- FALSE my_logical &lt;- TRUE 벡터 생성12number_vector &lt;- c(1,2,3)number_vertor &lt;- c(1,&quot;2&quot;,3) 첫번째 라인은 숫자로만 이루어져있기 때문에 변수 type을 알아보면 “numeric” 으로 나온다. 두번째 라인은 숫자, 문자가 섞여져 있다.이때 컴퓨터는 문자형 -&gt; 숫자형 -&gt; 논리형 으로 저장된다.따라서 두번째 라인의 변수 type은 문자형이 나온다. =&gt; “character” 1class(number_vector) 변수 type을 알아볼때는 class()를 사용한다. 범주형 변수의 순서 Levels: 낮음 높음123height_vector &lt;- c(&quot;낮음&quot;, &quot;높음&quot;, &quot;낮음&quot;)factor_height_vector &lt;- factor(height_vector)factor_height_vector Levels 순서 바꾸기 =&gt; 높음 낮음12levels(factor_sex_vector) &lt;- c(&quot;남성&quot;, &quot;여성&quot;)factor_sex_vector Levels 이름 변경 =&gt; Levels: 매우높음 매우낮음1234factor_height_vector &lt;- factor(factor_height_vector, levels = c(&quot;매우높음&quot;, &quot;매우낮음&quot;))factor_height_vector 참조 https://github.com/dschloe/R_edu","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}]},{"title":"[R]Simple visualization","slug":"R/Visualization_R/visualization","date":"2021-03-22T13:09:02.000Z","updated":"2021-04-26T08:13:14.044Z","comments":true,"path":"2021/03/22/R/Visualization_R/visualization/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/22/R/Visualization_R/visualization/","excerpt":"","text":"간단하게 시각화 하는 과정 step1) 패키지 설치1install.packages(&quot;ggplot2&quot;) step2) 패키지 불러오기1library(ggplot2) step3) 데이터 불러오기1data(&quot;iris&quot;) step4) 데이터 확인하기1str(iris) step5) 가공되지 않은 Raw data 가공하기 step6) 시각화하기12345ggplot(data=iris, mapping = aes(x = Petal.Length, y = Petal.Width, colour = Species)) + geom_point(size=3) *tap 누르면 자동입력기능 있음.*help에서 ggplot sample 확인 가능. 참조 https://github.com/dschloe/R_edu","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}]},{"title":"[Hexo]How to create a blog with Hexo","slug":"R/firstday","date":"2021-03-22T12:38:36.000Z","updated":"2021-04-26T08:13:14.045Z","comments":true,"path":"2021/03/22/R/firstday/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/22/R/firstday/","excerpt":"","text":"github에 2개의 Repository 생성 포스트 버전관리: user_name 포스트 배포용 관리: user_name.github.io Blog 초기화$ hexo init myblog Hexo 모듈 설치$ npm install -g hexo-cli myblog로 들어간 뒤 아래와 같이 입력$ cd myblog$ npm install$ npm install hexo-server –save$ npm install hexo-deployer-git –save 참고로, 복사+붙여넣기 하면 오류를 줄일 수 있음 Local server로 테스트$ hexo server Pycharm pycharm을 열어서 myblog 폴더를 연다. config.yml 파일을 열어 title, author을 수정한다.title: Eunjin’s Blogauthor: Eunjin Jun URL도 아래와 같이 수정한다. url: https://user_name.github.io Deployment의 deploy를 아래와 같이 입력한다.deploy:type: gitrepo: https://github.com/user_name/user_name.github.io.gitbranch: main Hexo generate &amp; deploy 활성화시킨 뒤 배포한다.$ hexo generate$ hexo deploy 한꺼번에 명령을 할 수도 있다.$ hexo deploy –generate Themes 설치ex) icarus 설치 icarus 설치$ npm install hexo-theme-icarus config.yml의 Extensions의 theme을 icarus로 변경theme: icarus $ hexo server이때, Error에서 뜨는 설명대로 그대로 복사한 뒤 붙여넣기하여 설치하기$ npm install –save &#98;&#x75;&#108;&#109;&#x61;&#45;&#115;&#116;&#121;&#x6c;&#x75;&#x73;&#x40;&#x30;&#46;&#56;&#x2e;&#x30; hexo-renderer-inferno@^0.1.3 다시 local server 테스트$ hexo server$ hexo deploy –generate 참조 https://dschloe.github.io/settings/hexo_blog/","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://eunjin-jun717.github.io/tags/hexo/"}]}],"categories":[],"tags":[{"name":"DL","slug":"DL","permalink":"https://eunjin-jun717.github.io/tags/DL/"},{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"},{"name":"statistic","slug":"statistic","permalink":"https://eunjin-jun717.github.io/tags/statistic/"},{"name":"ML","slug":"ML","permalink":"https://eunjin-jun717.github.io/tags/ML/"},{"name":"hexo","slug":"hexo","permalink":"https://eunjin-jun717.github.io/tags/hexo/"},{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"},{"name":"github","slug":"github","permalink":"https://eunjin-jun717.github.io/tags/github/"}]}