{"meta":{"title":"Eunjin's Blog","subtitle":"","description":"","author":"Eunjin Jun","url":"https://eunjin-jun717.github.io","root":"/"},"pages":[],"posts":[{"title":"","slug":"Python/Kaggle/tps-april-eda-feature-engineering","date":"2021-04-26T07:52:01.941Z","updated":"2021-04-26T08:12:40.480Z","comments":true,"path":"2021/04/26/Python/Kaggle/tps-april-eda-feature-engineering/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/26/Python/Kaggle/tps-april-eda-feature-engineering/","excerpt":"","text":"First look (1) Check data for NA EDA(Exploratory Data Analysis) (1) Survived (2) Age (3) Cabin (3) Family (3) Pclass (3) Sex (3) Embarked (3) Fare EDA(Exploratory Data Analysis) referenceshttps://www.kaggle.com/demidova/titanic-eda-tutorial[https://www.kaggle.com/demidova/titanic-logistic-regression-random-forest-xgboost?scriptVersionId=46567425] https://namu.wiki/jump/9AGb4mj%2Bgar2D116rRySHULPcuF9aQA9dU1%2FKaQlJabHnX1Bwo7dW3QKZZU5EDX7tyS7%2BeKInzFlBX0PyH2gvmr0xlEeT19AQhYRU4yv8erx25eqVyS5NlWU2pDAk3mhBaO4i%2BaABck5vAWwFaAE0g%3D%3D (1) Data Import12345678910111213import pandas as pdimport matplotlibimport matplotlib.pyplot as pltimport numpy as npimport seaborn as sbimport osprint(&quot;Version Pandas&quot;, pd.__version__)print(&quot;Version Matplotlib&quot;, matplotlib.__version__)print(&quot;Version Numpy&quot;, np.__version__)print(&quot;Version Seaborn&quot;, sb.__version__)os.listdir(&#x27;../input/tabular-playground-series-apr-2021/&#x27;) Version Pandas 1.2.3 Version Matplotlib 3.4.1 Version Numpy 1.19.5 Version Seaborn 0.11.1 [&#39;sample_submission.csv&#39;, &#39;train.csv&#39;, &#39;test.csv&#39;] 123456BASE_DIR = &#x27;../input/tabular-playground-series-apr-2021/&#x27;train = pd.read_csv(BASE_DIR + &#x27;train.csv&#x27;)test = pd.read_csv(BASE_DIR + &#x27;test.csv&#x27;)sample_submission = pd.read_csv(BASE_DIR + &#x27;sample_submission.csv&#x27;)train.shape, test.shape, sample_submission.shape ((100000, 12), (100000, 11), (100000, 2)) 1train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 0 1 1 Oconnor, Frankie male NaN 2 0 209245 27.14 C12239 S 1 1 0 3 Bryan, Drew male NaN 0 0 27323 13.35 NaN S 2 2 0 3 Owens, Kenneth male 0.33 1 2 CA 457703 71.29 NaN S 3 3 0 3 Kramer, James male 19.00 0 0 A. 10866 13.04 NaN S 4 4 1 3 Bond, Michael male 25.00 0 0 427635 7.76 NaN S 1test.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 100000 3 Holliday, Daniel male 19.0 0 0 24745 63.01 NaN S 1 100001 3 Nguyen, Lorraine female 53.0 0 0 13264 5.81 NaN S 2 100002 1 Harris, Heather female 19.0 0 0 25990 38.91 B15315 C 3 100003 2 Larsen, Eric male 25.0 0 0 314011 12.93 NaN S 4 100004 1 Cleary, Sarah female 17.0 0 2 26203 26.89 B22515 C 1sample_submission.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived 0 100000 1 1 100001 1 2 100002 1 3 100003 1 4 100004 1 1234frames= [train, test]total_df=pd.concat(frames, sort=False)print(&#x27;total data shape: &#x27;, total_df.shape)total_df.head() total data shape: (200000, 12) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 0 1.0 1 Oconnor, Frankie male NaN 2 0 209245 27.14 C12239 S 1 1 0.0 3 Bryan, Drew male NaN 0 0 27323 13.35 NaN S 2 2 0.0 3 Owens, Kenneth male 0.33 1 2 CA 457703 71.29 NaN S 3 3 0.0 3 Kramer, James male 19.00 0 0 A. 10866 13.04 NaN S 4 4 1.0 3 Bond, Michael male 25.00 0 0 427635 7.76 NaN S 1total_df.describe(include=[object]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Name Sex Ticket Cabin Embarked count 200000 200000 190196 61303 199473 unique 174854 2 132613 45442 3 top Smith, James male A/5 C11139 S freq 61 125871 646 7 140981 1total_df.describe(include=[object]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Name Sex Ticket Cabin Embarked count 200000 200000 190196 61303 199473 unique 174854 2 132613 45442 3 top Smith, James male A/5 C11139 S freq 61 125871 646 7 140981 (2) Check data for NA dataset의 feature들을 살펴보고, null data의 여부를 체크해보자 종속변수 Survived(생존여부): target label (1,0) -&gt; integer 독립변수 PassengerId: 10000명 Pclass(티켓의 클래스): Upper(1), Middle(2), Lower(3) -&gt; categorical -&gt; integer Name(이름): 탑승자 성명들 Sex(성별): Male, Female -&gt; binary -&gt; string Age(나이): continuous -&gt; integer SibSp(함께 탑승한 형제와 배우자의 수): quantitative -&gt; integer Parch(함께 탑승한 부모, 아이의 수): quantitative -&gt; integer Ticket(티켓 번호): alphabet + integer -&gt; string Fare(탑승료): continous -&gt; float Cabin(객실 번호): alphabet + integer -&gt; string Embarked(탑승항구): C(Cherbourg), Q(Queenstown), S(Southhampton) -&gt; string references https://kaggle-kr.tistory.com/17 1total_df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 200000 entries, 0 to 99999 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 200000 non-null int64 1 Survived 100000 non-null float64 2 Pclass 200000 non-null int64 3 Name 200000 non-null object 4 Sex 200000 non-null object 5 Age 193221 non-null float64 6 SibSp 200000 non-null int64 7 Parch 200000 non-null int64 8 Ticket 190196 non-null object 9 Fare 199733 non-null float64 10 Cabin 61303 non-null object 11 Embarked 199473 non-null object dtypes: float64(3), int64(4), object(5) memory usage: 19.8+ MB Age, Fare -&gt; numeric variablesPclass -&gt; integer but in fact ‘categorical variable’ 12345total_df_na=total_df.isna().sum()train_na=train.isna().sum()test_na=test.isna().sum()pd.concat([train_na, test_na, total_df_na], axis=1, sort=False, keys=[&#x27;Train NA&#x27;,&#x27;Test NA&#x27;,&#x27;Total NA&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Train NA Test NA Total NA PassengerId 0 0.0 0 Survived 0 NaN 100000 Pclass 0 0.0 0 Name 0 0.0 0 Sex 0 0.0 0 Age 3292 3487.0 6779 SibSp 0 0.0 0 Parch 0 0.0 0 Ticket 4623 5181.0 9804 Fare 134 133.0 267 Cabin 67866 70831.0 138697 Embarked 250 277.0 527 missing data를 handling하기 위해서 EDA에서는 dataset을 합쳤지만, ML에서는 ‘data leakage’를 피하기 위해서 오직 train data set만 사용할 것이다. 1total_df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Age SibSp Parch Fare count 200000.000000 100000.000000 200000.000000 193221.000000 200000.000000 200000.000000 199733.000000 mean 99999.500000 0.427740 2.237920 34.464565 0.442120 0.473695 44.652071 std 57735.171256 0.494753 0.868273 16.783847 0.819392 0.937125 67.436104 min 0.000000 0.000000 1.000000 0.080000 0.000000 0.000000 0.050000 25% 49999.750000 0.000000 1.000000 22.000000 0.000000 0.000000 10.080000 50% 99999.500000 0.000000 3.000000 31.000000 0.000000 0.000000 20.250000 75% 149999.250000 1.000000 3.000000 48.000000 1.000000 1.000000 34.850000 max 199999.000000 1.000000 3.000000 87.000000 8.000000 9.000000 744.660000 (1) Survived train set에서 survived의 0,1 분포가 어떤지 확인해보겠습니다. 분포에 따라 모델의 평가 방법이 달라질 수 있습니다. 123456789101112131415plt.figure(figsize=(6, 4.5))ax= sb.countplot(x=&#x27;Survived&#x27;, data=total_df, palette=[&#x27;#4287f5&#x27;,&#x27;#7cd91e&#x27;])plt.xticks(np.arange(2), [&#x27;Drowned&#x27;,&#x27;Survived&#x27;])plt.title(&#x27;Overall survival&#x27;, fontsize=14)plt.xlabel(&#x27;Survived vs Drowned&#x27;)plt.ylabel(&#x27;Number of Passendgers&#x27;)labels=(total_df[&#x27;Survived&#x27;].value_counts())for i,v in enumerate(labels): ax.text(i, v-40, str(v), horizontalalignment=&#x27;center&#x27;, size=14, color=&#x27;w&#x27;, fontweight=&#x27;bold&#x27;) plt.show() 1total_df[&#x27;Survived&#x27;].value_counts(normalize=True) 0.0 0.57226 1.0 0.42774 Name: Survived, dtype: float64 (2) Independent Variables references https://wikidocs.net/75068 1) Age6779 : age missing values 3292 : train dataset 3487 : test dataset 123456789plt.figure(figsize=(15,3))sb.distplot(total_df[(total_df[&#x27;Age&#x27;]&gt;0)].Age, kde_kws=&#123;&#x27;lw&#x27;:3&#125;, bins=50)plt.title(&#x27;Distribution of passengers age (total data)&#x27;, fontsize=14)plt.xlabel(&#x27;Age&#x27;)plt.ylabel(&#x27;Frequency&#x27;)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) 12age_distr= pd.DataFrame(total_df[&#x27;Age&#x27;].describe())age_distr.transpose() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Age 193221.0 34.464565 16.783847 0.08 22.0 31.0 48.0 87.0 0.08세 ~ 87세까지 다양하게 나이대가 있으며 mean=34.46세 이다. 1-1) Age by surviving status123456789plt.figure(figsize=(15,3))sb.boxplot(y=&#x27;Survived&#x27;, x=&#x27;Age&#x27;, data=train, palette=[&#x27;#4287f5&#x27;,&#x27;#7cd91e&#x27;], fliersize=0, orient=&#x27;h&#x27;)sb.stripplot(y=&#x27;Survived&#x27;,x=&#x27;Age&#x27;, data=train, linewidth=0.6, palette=[&#x27;#4287f5&#x27;,&#x27;#7cd91e&#x27;], orient=&#x27;h&#x27;)plt.yticks(np.arange(2), [&#x27;Drowned&#x27;,&#x27;Survived&#x27;])plt.title(&#x27;Age distribution grouped by survivng status (train data)&#x27;, fontsize=14)plt.ylabel(&#x27;Passengers status after the tragedy&#x27;)plt.tight_layout() 1pd.DataFrame(total_df.groupby(&#x27;Survived&#x27;)[&#x27;Age&#x27;].describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Survived 0.0 55290.0 36.708695 17.809058 0.08 24.0 36.0 52.0 83.0 1.0 41418.0 40.553799 18.742172 0.08 27.0 43.0 55.0 87.0 1-2) Age by Pclass1234567891011121314151617181920212223242526plt.figure(figsize=(20,6))palette=sb.cubehelix_palette(5, start=3)plt.subplot(1,2,1)sb.boxplot(x=&#x27;Pclass&#x27;, y=&#x27;Age&#x27;, data=total_df, palette=palette, fliersize=0)plt.xticks(np.arange(3), [&#x27;1st class&#x27;,&#x27;2nd class&#x27;,&#x27;3rd class&#x27;])plt.title(&#x27;Age distribution grouped by ticket class (total data)&#x27;, fontsize=16)plt.xlabel(&#x27;Ticket class&#x27;)plt.subplot(1,2,2)age_1_class = total_df[(total_df[&#x27;Age&#x27;]&gt;0)&amp;(total_df[&#x27;Pclass&#x27;]==1)]age_2_class = total_df[(total_df[&#x27;Age&#x27;]&gt;0)&amp;(total_df[&#x27;Pclass&#x27;]==2)]age_3_class = total_df[(total_df[&#x27;Age&#x27;]&gt;0)&amp;(total_df[&#x27;Pclass&#x27;]==3)]# Ploting the 3 variables that we createsb.kdeplot(age_1_class[&quot;Age&quot;], shade=True, color=&#x27;#eed4d0&#x27;, label = &#x27;1st class&#x27;)sb.kdeplot(age_2_class[&quot;Age&quot;], shade=True, color=&#x27;#cda0aa&#x27;, label = &#x27;2nd class&#x27;)sb.kdeplot(age_3_class[&quot;Age&quot;], shade=True,color=&#x27;#a2708e&#x27;, label = &#x27;3rd class&#x27;)plt.title(&#x27;Age distribution grouped by ticket class (total data)&#x27;,fontsize= 16)plt.xlabel(&#x27;Age&#x27;)plt.xlim(0, 90)plt.tight_layout()plt.show() 1pd.DataFrame(total_df.groupby(&#x27;Pclass&#x27;)[&#x27;Age&#x27;].describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Pclass 1 55365.0 40.672757 14.715634 0.08 28.0 41.0 52.0 83.0 2 36606.0 36.855067 18.470623 0.08 23.0 35.0 52.0 87.0 3 101250.0 30.205570 15.954521 0.08 20.0 26.0 41.0 85.0 2nd 클래스는 1st, 3rd 클래스에 비해 더 넓은 분포를 가진다. 또한 거의 대칭 적이다.가장 나이가 적은 passenger은 1,2,3 등급 동일한 나이인 0.08세이다. 가장 나이가 많은 passenger은 2nd 클래스의 87세이다. 3rd 클래스 mean age= 30.2세2nd 클래스 mean age= 36.9세1st 클래스 mean age= 40.7세 1-3) Age vs Pclass vs Sex12345678910111213141516171819202122232425plt.figure(figsize=(20, 5))palette = &quot;Set3&quot;plt.subplot(1, 3, 1)sb.boxplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_1_class, palette = palette, fliersize = 0)#sb.stripplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_1_class,linewidth = 0.6, palette = palette)plt.title(&#x27;1st class Age distribution by Sex&#x27;,fontsize= 14)plt.ylim(-5, 80)plt.subplot(1, 3, 2)sb.boxplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_2_class, palette = palette, fliersize = 0)#sb.stripplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_2_class,linewidth = 0.6, palette = palette)plt.title(&#x27;2nd class Age distribution by Sex&#x27;,fontsize= 14)plt.ylim(-5, 80)plt.subplot(1, 3, 3)sb.boxplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_3_class, order = [&#x27;female&#x27;, &#x27;male&#x27;], palette = palette, fliersize = 0)#sb.stripplot(x = &#x27;Sex&#x27;, y = &#x27;Age&#x27;, data = age_3_class,order = [&#x27;female&#x27;, &#x27;male&#x27;], linewidth = 0.6, palette = palette)plt.title(&#x27;3rd class Age distribution by Sex&#x27;,fontsize= 14)plt.ylim(-5, 80)plt.show() 12345age_1_class_stat = pd.DataFrame(age_1_class.groupby(&#x27;Sex&#x27;)[&#x27;Age&#x27;].describe())age_2_class_stat = pd.DataFrame(age_2_class.groupby(&#x27;Sex&#x27;)[&#x27;Age&#x27;].describe())age_3_class_stat = pd.DataFrame(age_3_class.groupby(&#x27;Sex&#x27;)[&#x27;Age&#x27;].describe())pd.concat([age_1_class_stat, age_2_class_stat, age_3_class_stat], axis=0, sort = False, keys = [&#x27;1st&#x27;, &#x27;2nd&#x27;, &#x27;3rd&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Sex 1st female 25790.0 41.974173 15.253798 0.08 29.0 43.0 54.0 83.0 male 29575.0 39.537896 14.132517 0.08 28.0 39.0 51.0 80.0 2nd female 17554.0 37.283031 20.558883 0.08 22.0 36.0 55.0 87.0 male 19052.0 36.460753 16.302224 0.08 24.0 34.0 50.0 85.0 3rd female 28349.0 27.222629 17.974800 0.08 15.0 24.0 39.0 85.0 male 72901.0 31.365545 14.936175 0.08 22.0 27.0 41.0 83.0 2) Cabin 첫번째 코드만 추출함 A: lst class B C: 3rd class D: walking area E: 1st and 2nd class F: 2nd class, 2rd class G: boiler room T: boat deck U: Unknown 12total_df[&#x27;Cabin&#x27;]=total_df[&#x27;Cabin&#x27;].str.split(&#x27;&#x27;,expand=True)[1]total_df.loc[total_df[&#x27;Cabin&#x27;].isna(), &#x27;Cabin&#x27;]=&#x27;X&#x27; 12345678910111213141516171819202122232425fig = plt.figure(figsize=(20, 5))ax1 = fig.add_subplot(131)sb.countplot(x = &#x27;Cabin&#x27;, data = total_df, palette = &quot;hls&quot;, order = total_df[&#x27;Cabin&#x27;].value_counts().index, ax = ax1)plt.title(&#x27;Passengers distribution by Cabin&#x27;,fontsize= 16)plt.ylabel(&#x27;Number of passengers&#x27;)ax2 = fig.add_subplot(132)Cabin_by_class = total_df.groupby(&#x27;Cabin&#x27;)[&#x27;Pclass&#x27;].value_counts(normalize = True).unstack()Cabin_by_class.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;,color = [&#x27;#eed4d0&#x27;, &#x27;#cda0aa&#x27;, &#x27;#a2708e&#x27;], ax = ax2)plt.legend((&#x27;1st class&#x27;, &#x27;2nd class&#x27;, &#x27;3rd class&#x27;), loc=(1.04,0))plt.title(&#x27;Proportion of classes on each Cabin&#x27;,fontsize= 16)plt.xticks(rotation = False)ax3 = fig.add_subplot(133)Cabin_by_survived = total_df.groupby(&#x27;Cabin&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True).unstack()Cabin_by_survived = Cabin_by_survived.sort_values(by = 1, ascending = False)Cabin_by_survived.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], ax = ax3)plt.title(&#x27;Proportion of survived/drowned passengers by Cabin&#x27;,fontsize= 16)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))plt.xticks(rotation = False)plt.tight_layout()plt.show() /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): 대부분의 passengers는 Cabin code가 없다. Cabin code가 나와있는 승객들 중 가장 많은 수를 차지하는 deck은 ‘C’이며 lst class ticket이다. ‘C’ deck은 살아남은 승객들 중 4번째이다. 가장 많은 생존률을 가진 deck은 ‘F’이다. ‘A’ deck은 lifeboats와 가장 가까운 deck이였지만 생존률은 가장 낮은 확률을 보이고 있다. 3) Family Family size = Sib + Parch +1 1234total_df[&#x27;Family_size&#x27;]=total_df[&#x27;SibSp&#x27;]+total_df[&#x27;Parch&#x27;]+1family_size=total_df[&#x27;Family_size&#x27;].value_counts()print(&#x27;Family size and number of passengers:&#x27;)print(family_size) Family size and number of passengers: 1 116448 2 30139 3 25289 4 19724 5 4151 6 2021 7 1184 10 397 11 242 8 162 9 157 14 46 12 26 13 7 18 4 15 3 Name: Family_size, dtype: int64 12345678910111213141516171819202122fig = plt.figure(figsize = (12,4))ax1 = fig.add_subplot(121)ax = sb.countplot(total_df[&#x27;Family_size&#x27;], ax = ax1)# calculate passengers for each categorylabels = (total_df[&#x27;Family_size&#x27;].value_counts())# add result numbers on barchartfor i, v in enumerate(labels): ax.text(i, v+6, str(v), horizontalalignment = &#x27;center&#x27;, size = 10, color = &#x27;black&#x27;) plt.title(&#x27;Passengers distribution by family size&#x27;)plt.ylabel(&#x27;Number of passengers&#x27;)ax2 = fig.add_subplot(122)d = total_df.groupby(&#x27;Family_size&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True).unstack()d.plot(kind=&#x27;bar&#x27;, color=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], stacked=&#x27;True&#x27;, ax = ax2)plt.title(&#x27;Proportion of survived/drowned passengers by family size (train data)&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))plt.xticks(rotation = False)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): family size가 15명인 그룹은 모두 살아남지 못하였다. 대부분은 혼자 여행하는 사람들이였고, 생존율은 40% 정도 이다. 가장 높은 생존율을 보이는 family size는 2,3 정도이다. 4개의 category로 family size group을 나누어보겠다. single usual(sizes 2,3,4,5) big(6,7,8,9) large(all bigger then 10) 1234total_df[&#x27;Family_size_group&#x27;]=total_df[&#x27;Family_size&#x27;].map(lambda x: &#x27;f_single&#x27; if x ==1 else(&#x27;f_usual&#x27; if 6&gt;x&gt;=2 else(&#x27;f_big&#x27; if 10&gt;x&gt;=6 else(&#x27;f_large&#x27;)))) 1234567891011121314151617181920fig = plt.figure(figsize = (14,5))ax1 = fig.add_subplot(121)d = total_df.groupby(&#x27;Family_size_group&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True).unstack()d = d.sort_values(by = 1, ascending = False)d.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color = [&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], ax = ax1)plt.title(&#x27;Proportion of survived/drowned passengers by family size&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))_ = plt.xticks(rotation=False)ax2 = fig.add_subplot(122)d2 = total_df.groupby(&#x27;Family_size_group&#x27;)[&#x27;Pclass&#x27;].value_counts(normalize = True).unstack()d2 = d2.sort_values(by = 1, ascending = False)d2.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color = [&#x27;#eed4d0&#x27;, &#x27;#cda0aa&#x27;, &#x27;#a2708e&#x27;], ax = ax2)plt.legend((&#x27;1st class&#x27;, &#x27;2nd class&#x27;, &#x27;3rd class&#x27;), loc=(1.04,0))plt.title(&#x27;Proportion of 1st/2nd/3rd ticket class in family group size&#x27;)_ = plt.xticks(rotation=False)plt.tight_layout() 4) Pclass1234567891011ax = sb.countplot(total_df[&#x27;Pclass&#x27;], palette = [&#x27;#eed4d0&#x27;, &#x27;#cda0aa&#x27;, &#x27;#a2708e&#x27;])# calculate passengers for each categorylabels = (total_df[&#x27;Pclass&#x27;].value_counts(sort = False))# add result numbers on barchartfor i, v in enumerate(labels): ax.text(i, v+2, str(v), horizontalalignment = &#x27;center&#x27;, size = 12, color = &#x27;black&#x27;, fontweight = &#x27;bold&#x27;) plt.title(&#x27;Passengers distribution by Pclass&#x27;)plt.ylabel(&#x27;Number of passengers&#x27;)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning 1234567891011121314151617fig = plt.figure(figsize=(14, 5))ax1 = fig.add_subplot(121)sb.countplot(x = &#x27;Pclass&#x27;, hue = &#x27;Survived&#x27;, data = total_df, palette=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], ax = ax1)plt.title(&#x27;Number of survived/drowned passengers by class (train data)&#x27;)plt.ylabel(&#x27;Number of passengers&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))_ = plt.xticks(rotation=False)ax2 = fig.add_subplot(122)d = total_df.groupby(&#x27;Pclass&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True).unstack()d.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, ax = ax2, color =[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;])plt.title(&#x27;Proportion of survived/drowned passengers by class (train data)&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))_ = plt.xticks(rotation=False)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): 가장 많은 승객이 탄 3등급 임에도 불구하고 생존율은 가장 적은 승객이 탑승한 1등급에 비해 더 적은 생존율을 보인다. 4-1) Pclass vs Surviving vs Sex123sb.catplot(x = &#x27;Pclass&#x27;, hue = &#x27;Survived&#x27;, col = &#x27;Sex&#x27;, kind = &#x27;count&#x27;, data = total_df , palette=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;])plt.tight_layout() 1등급 클래스의 남성 승객들의 대부분은 살아남지 못하였고 여성들은 대부분 살아남았다. 3등급 클래스의 여성의 절반 이상은 살아남았다. 5) Embarked12345678910111213141516171819202122232425262728293031323334fig = plt.figure(figsize = (15,4))ax1 = fig.add_subplot(131)palette = sb.cubehelix_palette(5, start = 2)ax = sb.countplot(total_df[&#x27;Embarked&#x27;], palette = palette, order = [&#x27;C&#x27;, &#x27;Q&#x27;, &#x27;S&#x27;], ax = ax1)plt.title(&#x27;Number of passengers by Embarked&#x27;)plt.ylabel(&#x27;Number of passengers&#x27;)# calculate passengers for each categorylabels = (total_df[&#x27;Embarked&#x27;].value_counts())labels = labels.sort_index()# add result numbers on barchartfor i, v in enumerate(labels): ax.text(i, v+10, str(v), horizontalalignment = &#x27;center&#x27;, size = 10, color = &#x27;black&#x27;) ax2 = fig.add_subplot(132)surv_by_emb = total_df.groupby(&#x27;Embarked&#x27;)[&#x27;Survived&#x27;].value_counts(normalize = True)surv_by_emb = surv_by_emb.unstack().sort_index()surv_by_emb.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color=[&quot;#3f3e6fd1&quot;, &quot;#85c6a9&quot;], ax = ax2)plt.title(&#x27;Proportion of survived/drowned passengers by Embarked (train data)&#x27;)plt.legend(( &#x27;Drowned&#x27;, &#x27;Survived&#x27;), loc=(1.04,0))_ = plt.xticks(rotation=False)ax3 = fig.add_subplot(133)class_by_emb = total_df.groupby(&#x27;Embarked&#x27;)[&#x27;Pclass&#x27;].value_counts(normalize = True)class_by_emb = class_by_emb.unstack().sort_index()class_by_emb.plot(kind=&#x27;bar&#x27;, stacked=&#x27;True&#x27;, color = [&#x27;#eed4d0&#x27;, &#x27;#cda0aa&#x27;, &#x27;#a2708e&#x27;], ax = ax3)plt.legend((&#x27;1st class&#x27;, &#x27;2nd class&#x27;, &#x27;3rd class&#x27;), loc=(1.04,0))plt.title(&#x27;Proportion of clases by Embarked&#x27;)_ = plt.xticks(rotation=False)plt.tight_layout() /opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /opt/conda/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): 대부분의 승객(140981)들은 S 항구에서 출발하였고 S 항구에서 출발한 승객들의 생존율은 가장 낮았다. 또한 3등급 클래스 사람들이 대부분이다. C항구에서 출발한 승객들은 75% 이상의 생존율을 보인다. 가장 적은 승객들이 탑승한 Q항구에는 가장 많은 l등급 클래스의 승객들이 탑승하였다. 1234sb.catplot(x=&quot;Embarked&quot;, y=&quot;Fare&quot;, kind=&quot;violin&quot;, inner=None, data=total_df, height = 6, palette = palette, order = [&#x27;C&#x27;, &#x27;Q&#x27;, &#x27;S&#x27;])plt.title(&#x27;Distribution of Fare by Embarked&#x27;)plt.tight_layout() 1pd.DataFrame(total_df.groupby(&#x27;Embarked&#x27;)[&#x27;Fare&#x27;].describe()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count mean std min 25% 50% 75% max Embarked C 44440.0 73.673693 87.985577 1.51 13.54 31.42 89.5025 744.46 Q 13977.0 78.479737 92.941969 2.29 10.81 28.45 115.6700 744.66 S 140791.0 32.125407 50.934735 0.05 9.50 13.24 29.9900 727.65 6) Fare123fig, ax = plt.subplots(1, 1, figsize=(8, 8))g = sb.distplot(total_df[&#x27;Fare&#x27;], color=&#x27;r&#x27;, label=&#x27;Skewness : &#123;:.2f&#125;&#x27;.format(total_df[&#x27;Fare&#x27;].skew()), ax=ax)g = g.legend(loc=&#x27;best&#x27;) /opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) 1234567fare_map = total_df[[&#x27;Fare&#x27;, &#x27;Pclass&#x27;]].dropna().groupby(&#x27;Pclass&#x27;).median().to_dict()total_df[&#x27;Fare&#x27;] = total_df[&#x27;Fare&#x27;].fillna(total_df[&#x27;Pclass&#x27;].map(fare_map[&#x27;Fare&#x27;]))total_df[&#x27;Fare&#x27;] = total_df[&#x27;Fare&#x27;].map(lambda i: np.log(i) if i &gt; 0 else 0)fig, ax = plt.subplots(1, 1, figsize=(8, 8))g = sb.distplot(total_df[&#x27;Fare&#x27;], color=&#x27;b&#x27;, label=&#x27;Skewness : &#123;:.2f&#125;&#x27;.format(total_df[&#x27;Fare&#x27;].skew()), ax=ax)g = g.legend(loc=&#x27;best&#x27;) /opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) Feature Engineering Null values 확인 1total_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Family_size Family_size_group 0 0 1.0 1 Oconnor, Frankie male NaN 2 0 209245 3.301009 C S 3 f_usual 1 1 0.0 3 Bryan, Drew male NaN 0 0 27323 2.591516 X S 1 f_single 2 2 0.0 3 Owens, Kenneth male 0.33 1 2 CA 457703 4.266756 X S 4 f_usual 3 3 0.0 3 Kramer, James male 19.00 0 0 A. 10866 2.568022 X S 1 f_single 4 4 1.0 3 Bond, Michael male 25.00 0 0 427635 2.048982 X S 1 f_single ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 199995 NaN 3 Cash, Cheryle female 27.00 0 0 7686 2.314514 X Q 1 f_single 99996 199996 NaN 1 Brown, Howard male 59.00 1 0 13004 4.224056 X S 2 f_usual 99997 199997 NaN 3 Lightfoot, Cameron male 47.00 0 0 4383317 2.386007 X S 1 f_single 99998 199998 NaN 1 Jacobsen, Margaret female 49.00 1 2 PC 26988 3.390473 B C 4 f_usual 99999 199999 NaN 1 Fishback, Joanna female 41.00 0 2 PC 41824 5.275100 E C 3 f_usual 200000 rows × 14 columns 1total_df.isna().sum() PassengerId 0 Survived 100000 Pclass 0 Name 0 Sex 0 Age 6779 SibSp 0 Parch 0 Ticket 9804 Fare 0 Cabin 0 Embarked 527 Family_size 0 Family_size_group 0 dtype: int64 1) Data Correlation12345678910111213141516fig, ax=plt.subplots(1, 3, figsize=(17,5))feature_lst=[&#x27;Pclass&#x27;,&#x27;Age&#x27;,&#x27;Fare&#x27;,&#x27;Sex&#x27;,&#x27;Family_size&#x27;]corr=total_df[feature_lst].corr()mask=np.zeros_like(corr, dtype=np.bool)mask[np.triu_indices_from(mask)]=Truefor idx, method in enumerate([&#x27;pearson&#x27;,&#x27;kendall&#x27;,&#x27;spearman&#x27;]): sb.heatmap(total_df[feature_lst].corr(method=method), ax=ax[idx], square=True, annot=True, fmt=&#x27;.2f&#x27;, center=0, linewidth=2, cbar=False, cmap=sb.diverging_palette(240, 10, as_cmap=True), mask=mask) ax[idx].set_title(f&#x27;&#123;method.capitalize()&#125; Correlation&#x27;, loc=&#x27;left&#x27;, fontweight=&#x27;bold&#x27;) plt.show() 2) Age 각 클래스마다 나이의 평균을 각 클래스마다의 null 값에 넣어주었다. 12age_map= total_df[[&#x27;Age&#x27;,&#x27;Pclass&#x27;]].dropna().groupby(&#x27;Pclass&#x27;).median().to_dict()total_df[&#x27;Age&#x27;]=total_df[&#x27;Age&#x27;].fillna(total_df[&#x27;Pclass&#x27;].map(age_map[&#x27;Age&#x27;])) 3) Embarked1print(&#x27;Embarked has &#x27;, sum(total_df[&#x27;Embarked&#x27;].isnull()), &#x27; Null values&#x27;) Embarked has 527 Null values 1total_df[&#x27;Embarked&#x27;] = total_df[&#x27;Embarked&#x27;].fillna(&#x27;S&#x27;) 1total_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Family_size Family_size_group 0 0 1.0 1 Oconnor, Frankie male 41.00 2 0 209245 3.301009 C S 3 f_usual 1 1 0.0 3 Bryan, Drew male 26.00 0 0 27323 2.591516 X S 1 f_single 2 2 0.0 3 Owens, Kenneth male 0.33 1 2 CA 457703 4.266756 X S 4 f_usual 3 3 0.0 3 Kramer, James male 19.00 0 0 A. 10866 2.568022 X S 1 f_single 4 4 1.0 3 Bond, Michael male 25.00 0 0 427635 2.048982 X S 1 f_single ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 199995 NaN 3 Cash, Cheryle female 27.00 0 0 7686 2.314514 X Q 1 f_single 99996 199996 NaN 1 Brown, Howard male 59.00 1 0 13004 4.224056 X S 2 f_usual 99997 199997 NaN 3 Lightfoot, Cameron male 47.00 0 0 4383317 2.386007 X S 1 f_single 99998 199998 NaN 1 Jacobsen, Margaret female 49.00 1 2 PC 26988 3.390473 B C 4 f_usual 99999 199999 NaN 1 Fishback, Joanna female 41.00 0 2 PC 41824 5.275100 E C 3 f_usual 200000 rows × 14 columns 4) Name1total_df[&#x27;Name&#x27;] = total_df[&#x27;Name&#x27;].map(lambda x: x.split(&#x27;,&#x27;)[0]) 5) Ticket1total_df[&#x27;Ticket&#x27;] = total_df[&#x27;Ticket&#x27;].fillna(&#x27;X&#x27;).map(lambda x:str(x).split()[0] if len(str(x).split()) &gt; 1 else &#x27;X&#x27;) 6) Drop PassengerId, Name, SibSp, Parch, Cabin 1total_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Family_size Family_size_group 0 0 1.0 1 Oconnor male 41.00 2 0 X 3.301009 C S 3 f_usual 1 1 0.0 3 Bryan male 26.00 0 0 X 2.591516 X S 1 f_single 2 2 0.0 3 Owens male 0.33 1 2 CA 4.266756 X S 4 f_usual 3 3 0.0 3 Kramer male 19.00 0 0 A. 2.568022 X S 1 f_single 4 4 1.0 3 Bond male 25.00 0 0 X 2.048982 X S 1 f_single ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 199995 NaN 3 Cash female 27.00 0 0 X 2.314514 X Q 1 f_single 99996 199996 NaN 1 Brown male 59.00 1 0 X 4.224056 X S 2 f_usual 99997 199997 NaN 3 Lightfoot male 47.00 0 0 X 2.386007 X S 1 f_single 99998 199998 NaN 1 Jacobsen female 49.00 1 2 PC 3.390473 B C 4 f_usual 99999 199999 NaN 1 Fishback female 41.00 0 2 PC 5.275100 E C 3 f_usual 200000 rows × 14 columns 12total_df.drop([&#x27;PassengerId&#x27;,&#x27;Name&#x27;,&#x27;Family_size_group&#x27;,&#x27;Family_size&#x27;], axis=1, inplace=True)total_df.shape (200000, 10) 12345total_df[&#x27;Sex&#x27;]=total_df[&#x27;Sex&#x27;].map(&#123;&#x27;female&#x27;:0, &#x27;male&#x27;:1&#125;)total_df=pd.get_dummies(total_df, columns=[&#x27;Embarked&#x27;], prefix=&#x27;Embarked&#x27;)total_df=pd.get_dummies(total_df, columns=[&#x27;Cabin&#x27;], prefix=&#x27;Cabin&#x27;)total_df=pd.get_dummies(total_df, columns=[&#x27;Ticket&#x27;], prefix=&#x27;Ticket&#x27;)#total_df=pd.get_dummies(total_df, columns=[&#x27;Family_size_group&#x27;], prefix=&#x27;Family_size_group&#x27;) 1total_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Survived Pclass Sex Age SibSp Parch Fare Embarked_C Embarked_Q Embarked_S ... Ticket_SOTON/OQ Ticket_STON/O Ticket_STON/O2. Ticket_STON/OQ. Ticket_SW/PP Ticket_W./C. Ticket_W.E.P. Ticket_W/C Ticket_WE/P Ticket_X 0 1.0 1 1 41.00 2 0 3.301009 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 1 0.0 3 1 26.00 0 0 2.591516 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 2 0.0 3 1 0.33 1 2 4.266756 0 0 1 ... 0 0 0 0 0 0 0 0 0 0 3 0.0 3 1 19.00 0 0 2.568022 0 0 1 ... 0 0 0 0 0 0 0 0 0 0 4 1.0 3 1 25.00 0 0 2.048982 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 99995 NaN 3 0 27.00 0 0 2.314514 0 1 0 ... 0 0 0 0 0 0 0 0 0 1 99996 NaN 1 1 59.00 1 0 4.224056 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 99997 NaN 3 1 47.00 0 0 2.386007 0 0 1 ... 0 0 0 0 0 0 0 0 0 1 99998 NaN 1 0 49.00 1 2 3.390473 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 99999 NaN 1 0 41.00 0 2 5.275100 1 0 0 ... 0 0 0 0 0 0 0 0 0 0 200000 rows × 69 columns Split data123456X = total_df[:train.shape[0]]print(&quot;X Shape is:&quot;, X.shape)y = X[&#x27;Survived&#x27;]X.drop([&#x27;Survived&#x27;], axis=1, inplace=True)test_data = total_df[train.shape[0]:].drop(columns=[&#x27;Survived&#x27;])test_data.info() X Shape is: (100000, 69) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 100000 entries, 0 to 99999 Data columns (total 68 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Pclass 100000 non-null int64 1 Sex 100000 non-null int64 2 Age 100000 non-null float64 3 SibSp 100000 non-null int64 4 Parch 100000 non-null int64 5 Fare 100000 non-null float64 6 Embarked_C 100000 non-null uint8 7 Embarked_Q 100000 non-null uint8 8 Embarked_S 100000 non-null uint8 9 Cabin_A 100000 non-null uint8 10 Cabin_B 100000 non-null uint8 11 Cabin_C 100000 non-null uint8 12 Cabin_D 100000 non-null uint8 13 Cabin_E 100000 non-null uint8 14 Cabin_F 100000 non-null uint8 15 Cabin_G 100000 non-null uint8 16 Cabin_T 100000 non-null uint8 17 Cabin_X 100000 non-null uint8 18 Ticket_A. 100000 non-null uint8 19 Ticket_A./5. 100000 non-null uint8 20 Ticket_A.5. 100000 non-null uint8 21 Ticket_A/4 100000 non-null uint8 22 Ticket_A/4. 100000 non-null uint8 23 Ticket_A/5 100000 non-null uint8 24 Ticket_A/5. 100000 non-null uint8 25 Ticket_A/S 100000 non-null uint8 26 Ticket_A4. 100000 non-null uint8 27 Ticket_AQ/3. 100000 non-null uint8 28 Ticket_AQ/4 100000 non-null uint8 29 Ticket_C 100000 non-null uint8 30 Ticket_C.A. 100000 non-null uint8 31 Ticket_C.A./SOTON 100000 non-null uint8 32 Ticket_CA 100000 non-null uint8 33 Ticket_CA. 100000 non-null uint8 34 Ticket_F.C. 100000 non-null uint8 35 Ticket_F.C.C. 100000 non-null uint8 36 Ticket_Fa 100000 non-null uint8 37 Ticket_LP 100000 non-null uint8 38 Ticket_P/PP 100000 non-null uint8 39 Ticket_PC 100000 non-null uint8 40 Ticket_PP 100000 non-null uint8 41 Ticket_S.C./A.4. 100000 non-null uint8 42 Ticket_S.C./PARIS 100000 non-null uint8 43 Ticket_S.O./P.P. 100000 non-null uint8 44 Ticket_S.O.C. 100000 non-null uint8 45 Ticket_S.O.P. 100000 non-null uint8 46 Ticket_S.P. 100000 non-null uint8 47 Ticket_S.W./PP 100000 non-null uint8 48 Ticket_SC 100000 non-null uint8 49 Ticket_SC/A.3 100000 non-null uint8 50 Ticket_SC/A4 100000 non-null uint8 51 Ticket_SC/AH 100000 non-null uint8 52 Ticket_SC/PARIS 100000 non-null uint8 53 Ticket_SC/Paris 100000 non-null uint8 54 Ticket_SCO/W 100000 non-null uint8 55 Ticket_SO/C 100000 non-null uint8 56 Ticket_SOTON/O.Q. 100000 non-null uint8 57 Ticket_SOTON/O2 100000 non-null uint8 58 Ticket_SOTON/OQ 100000 non-null uint8 59 Ticket_STON/O 100000 non-null uint8 60 Ticket_STON/O2. 100000 non-null uint8 61 Ticket_STON/OQ. 100000 non-null uint8 62 Ticket_SW/PP 100000 non-null uint8 63 Ticket_W./C. 100000 non-null uint8 64 Ticket_W.E.P. 100000 non-null uint8 65 Ticket_W/C 100000 non-null uint8 66 Ticket_WE/P 100000 non-null uint8 67 Ticket_X 100000 non-null uint8 dtypes: float64(2), int64(4), uint8(62) memory usage: 11.3 MB /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:4315: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, 12345from sklearn.model_selection import train_test_split#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state=42)X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, stratify = X[[&#x27;Pclass&#x27;]], random_state=42)X_train.shape, X_valid.shape, y_train.shape, y_valid.shape ((70000, 68), (30000, 68), (70000,), (30000,)) Modeling referenceshttps://www.kaggle.com/j2hoon85/tps-april-sklearn-pycaret-for-newbies XGBoost Shap123456789101112131415import xgboostimport shap# train an XGBoost modelxgb_model = xgboost.XGBClassifier().fit(X_train, y_train)# explain the model&#x27;s predictions using SHAP# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)explainer = shap.Explainer(xgb_model)shap_values = explainer(X)# visualize the first prediction&#x27;s explanationshap.plots.waterfall(shap_values[0])shap.plots.bar(shap_values) The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. [06:00:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. ntree_limit is deprecated, use `iteration_range` or model slicing instead. Decision Tree123from sklearn.metrics import accuracy_scoredef acc_score(y_true, y_pred, **kwargs): return accuracy_score(y_true, (y_pred &gt; 0.5).astype(int), **kwargs) 123456789101112131415161718192021222324252627from sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import roc_curve, roc_auc_scorefrom matplotlib import pyplot as plttree_model = DecisionTreeClassifier(max_depth=7)tree_model.fit(X_train, y_train)predictions = tree_model.predict_proba(X_valid)AUC = roc_auc_score(y_valid, predictions[:,1])ACC = acc_score(y_valid, predictions[:,1])print(&quot;Model AUC:&quot;, AUC)print(&quot;Model Accurarcy:&quot;, ACC)print(&quot;\\n&quot;)fpr, tpr, _ = roc_curve(y_valid, predictions[:,1])fig, ax = plt.subplots(figsize=(10, 6))ax.plot(fpr, tpr)ax.text(x = 0.3, y = 0.4, s = &quot;Model AUC is &#123;&#125;\\n\\nModel Accuracy is &#123;&#125;&quot;.format(np.round(AUC, 2), np.round(ACC, 2)), fontsize=16, bbox=dict(facecolor=&#x27;gray&#x27;, alpha=0.3))ax.set_xlabel(&#x27;FPR&#x27;)ax.set_ylabel(&#x27;TPR&#x27;)ax.set_title(&#x27;ROC curve&#x27;)plt.show() Model AUC: 0.8505605139706627 Model Accurarcy: 0.7833333333333333 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn import treefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrixfrom matplotlib import pyplot as pltSEED = 0class sk_helper(object): def __init__(self, model, seed=0, params=&#123;&#125;): params[&#x27;random_state&#x27;]=seed self.model=model(**params) self.model_name=str(model).split(&#x27;.&#x27;)[-1][:-2] def train(self, X_train, y_train): self.model.fit(X_train, y_train) def predict(self, y_valid): return self.model.predict(y_valid) def fit(self, x, y): return self.model.fit(x,y) # feature importance def feature_importances(self, X_train, y_train): return self.model.fit(X_train, y_train).feature_importances_ # roc_curve def roc_curve_graph(self, X_train, y_train, X_valid, y_valid): self.model.fit(X_train, y_train) print(&quot;model_name:&quot;, self.model_name) model_name = self.model_name preds_proba = self.model.predict_proba(X_valid) preds = (preds_proba[:, 1] &gt; 0.5).astype(int) auc = roc_auc_score(y_valid, preds_proba[:, 1]) acc = accuracy_score(y_valid, preds) confusion = confusion_matrix(y_valid, preds) print(&#x27;Confusion Matrix&#x27;) print(confusion) print(&quot;Model AUC: &#123;0:.3f&#125;, Model Accuracy: &#123;1:.3f&#125;\\n&quot;.format(auc, acc)) fpr, tpr, _ = roc_curve(y_valid, preds_proba[:,1]) fig, ax = plt.subplots(figsize=(10, 6)) ax.plot(fpr, tpr) ax.text(x = 0.3, y = 0.4, s = &quot;Model AUC is &#123;&#125;\\n\\nModel Accuracy is &#123;&#125;&quot;.format(np.round(auc, 2), np.round(acc, 2)), fontsize=16, bbox=dict(facecolor=&#x27;gray&#x27;, alpha=0.3)) ax.set_xlabel(&#x27;FPR&#x27;) ax.set_ylabel(&#x27;TPR&#x27;) ax.set_title(&#x27;ROC curve of &#123;&#125;&#x27;.format(model_name), fontsize=16) plt.show() 1234567891011121314151617%%timefrom sklearn.ensemble import RandomForestClassifierrf_params = &#123; &#x27;n_jobs&#x27;: -1, &#x27;n_estimators&#x27;: 500, &#x27;warm_start&#x27;: True, #&#x27;max_features&#x27;: 0.2, &#x27;max_depth&#x27;: 10, &#x27;min_samples_leaf&#x27;: 2, &#x27;max_features&#x27; : &#x27;sqrt&#x27;, &#x27;verbose&#x27;: 1&#125;rf_model=sk_helper(model=RandomForestClassifier, seed=SEED, params=rf_params)rf_model.roc_curve_graph(X_train, y_train, X_valid, y_valid) [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 0.8s [Parallel(n_jobs=-1)]: Done 192 tasks | elapsed: 3.5s [Parallel(n_jobs=-1)]: Done 442 tasks | elapsed: 8.2s [Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 9.3s finished [Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 42 tasks | elapsed: 0.1s model_name: RandomForestClassifier [Parallel(n_jobs=4)]: Done 192 tasks | elapsed: 0.3s [Parallel(n_jobs=4)]: Done 442 tasks | elapsed: 0.6s [Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed: 0.7s finished Confusion Matrix [[14356 2879] [ 3582 9183]] Model AUC: 0.855, Model Accuracy: 0.785 CPU times: user 38.2 s, sys: 250 ms, total: 38.4 s Wall time: 10.5 s 1234567891011121314151617181920212223%%timeimport lightgbm as lgblgb_params = &#123; &#x27;metric&#x27;: &#x27;binary_logloss&#x27;, &#x27;n_estimators&#x27;: 1000, &#x27;objective&#x27;: &#x27;binary&#x27;, &#x27;random_state&#x27;: 2021, &#x27;learning_rate&#x27;: 0.01, &#x27;min_child_samples&#x27;: 150, &#x27;reg_alpha&#x27;: 3e-5, &#x27;reg_lambda&#x27;: 9e-2, &#x27;num_leaves&#x27;: 20, &#x27;max_depth&#x27;: 16, &#x27;colsample_bytree&#x27;: 0.8, &#x27;subsample&#x27;: 0.8, &#x27;subsample_freq&#x27;: 2, &#x27;max_bin&#x27;: 240&#125;lgb_model=sk_helper(model=lgb.LGBMClassifier, seed=SEED, params=rf_params)lgb_model.roc_curve_graph(X_train, y_train, X_valid, y_valid) .datatable table.frame { margin-bottom: 0; } .datatable table.frame thead { border-bottom: none; } .datatable table.frame tr.coltypes td { color: #FFFFFF; line-height: 6px; padding: 0 0.5em;} .datatable .bool { background: #DDDD99; } .datatable .object { background: #565656; } .datatable .int { background: #5D9E5D; } .datatable .float { background: #4040CC; } .datatable .str { background: #CC4040; } .datatable .row_index { background: var(--jp-border-color3); border-right: 1px solid var(--jp-border-color0); color: var(--jp-ui-font-color3); font-size: 9px;} .datatable .frame tr.coltypes .row_index { background: var(--jp-border-color0);} .datatable th:nth-child(2) { padding-left: 12px; } .datatable .hellipsis { color: var(--jp-cell-editor-border-color);} .datatable .vellipsis { background: var(--jp-layout-color0); color: var(--jp-cell-editor-border-color);} .datatable .na { color: var(--jp-cell-editor-border-color); font-size: 80%;} .datatable .footer { font-size: 9px; } .datatable .frame_dimensions { background: var(--jp-border-color3); border-top: 1px solid var(--jp-border-color0); color: var(--jp-ui-font-color3); display: inline-block; opacity: 0.6; padding: 1px 10px 1px 5px;} [LightGBM] [Warning] Unknown parameter: max_features [LightGBM] [Warning] Unknown parameter: min_samples_leaf [LightGBM] [Warning] Unknown parameter: warm_start [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Warning] Unknown parameter: max_features [LightGBM] [Warning] Unknown parameter: min_samples_leaf [LightGBM] [Warning] Unknown parameter: warm_start [LightGBM] [Warning] Accuracy may be bad since you didn&#39;t explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31). [LightGBM] [Info] Number of positive: 30009, number of negative: 39991 [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020202 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 543 [LightGBM] [Info] Number of data points in the train set: 70000, number of used features: 65 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.428700 -&gt; initscore=-0.287157 [LightGBM] [Info] Start training from score -0.287157 [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf [LightGBM] [Warning] No further splits with positive gain, best gain: -inf model_name: LGBMClassifier Confusion Matrix [[14065 3170] [ 3286 9479]] Model AUC: 0.855, Model Accuracy: 0.785 CPU times: user 5.81 s, sys: 134 ms, total: 5.94 s Wall time: 3.82 s 1!pip install catboost Requirement already satisfied: catboost in /opt/conda/lib/python3.7/site-packages (0.25.1) Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from catboost) (3.4.1) Requirement already satisfied: plotly in /opt/conda/lib/python3.7/site-packages (from catboost) (4.14.3) Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from catboost) (1.5.4) Requirement already satisfied: numpy&gt;=1.16.0 in /opt/conda/lib/python3.7/site-packages (from catboost) (1.19.5) Requirement already satisfied: pandas&gt;=0.24.0 in /opt/conda/lib/python3.7/site-packages (from catboost) (1.2.3) Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from catboost) (1.15.0) Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from catboost) (0.8.4) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.1) Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas&gt;=0.24.0-&gt;catboost) (2021.1) Requirement already satisfied: pillow&gt;=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;catboost) (7.2.0) Requirement already satisfied: pyparsing&gt;=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;catboost) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;catboost) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;catboost) (0.10.0) Requirement already satisfied: retrying&gt;=1.3.3 in /opt/conda/lib/python3.7/site-packages (from plotly-&gt;catboost) (1.3.3) 12345678910111213141516%%timefrom catboost import CatBoostClassifiercb_params = &#123; &#x27;max_depth&#x27;: 8, &#x27;learning_rate&#x27;: 0.01, &#x27;n_estimators&#x27;: 1000, &#x27;max_bin&#x27;: 280, &#x27;min_data_in_leaf&#x27;: 64, &#x27;l2_leaf_reg&#x27;: 0.01, &#x27;subsample&#x27;: 0.8&#125;cb_model=sk_helper(model=CatBoostClassifier, seed=SEED, params=cb_params)cb_model.roc_curve_graph(X_train, y_train, X_valid, y_valid) 0: learn: 0.6884406 total: 80.5ms remaining: 1m 20s 1: learn: 0.6838170 total: 104ms remaining: 51.9s 2: learn: 0.6792866 total: 120ms remaining: 40s 3: learn: 0.6749307 total: 144ms remaining: 35.9s 4: learn: 0.6705501 total: 168ms remaining: 33.4s 5: learn: 0.6662601 total: 192ms remaining: 31.8s 6: learn: 0.6621476 total: 214ms remaining: 30.4s 7: learn: 0.6580493 total: 238ms remaining: 29.5s 8: learn: 0.6541011 total: 260ms remaining: 28.6s 9: learn: 0.6502757 total: 276ms remaining: 27.3s 10: learn: 0.6465377 total: 302ms remaining: 27.1s 11: learn: 0.6427935 total: 325ms remaining: 26.8s 12: learn: 0.6391145 total: 348ms remaining: 26.4s 13: learn: 0.6356544 total: 369ms remaining: 26s 14: learn: 0.6321140 total: 393ms remaining: 25.8s 15: learn: 0.6287126 total: 417ms remaining: 25.6s 16: learn: 0.6255104 total: 436ms remaining: 25.2s 17: learn: 0.6222667 total: 458ms remaining: 25s 18: learn: 0.6190328 total: 484ms remaining: 25s 19: learn: 0.6160637 total: 498ms remaining: 24.4s 20: learn: 0.6130081 total: 532ms remaining: 24.8s 21: learn: 0.6101029 total: 553ms remaining: 24.6s 22: learn: 0.6072655 total: 578ms remaining: 24.6s 23: learn: 0.6043829 total: 602ms remaining: 24.5s 24: learn: 0.6018266 total: 620ms remaining: 24.2s 25: learn: 0.5990827 total: 644ms remaining: 24.1s 26: learn: 0.5964782 total: 668ms remaining: 24.1s 27: learn: 0.5939595 total: 688ms remaining: 23.9s 28: learn: 0.5915331 total: 704ms remaining: 23.6s 29: learn: 0.5889863 total: 728ms remaining: 23.5s 30: learn: 0.5865869 total: 753ms remaining: 23.5s 31: learn: 0.5842859 total: 776ms remaining: 23.5s 32: learn: 0.5819020 total: 801ms remaining: 23.5s 33: learn: 0.5797240 total: 825ms remaining: 23.4s 34: learn: 0.5775389 total: 848ms remaining: 23.4s 35: learn: 0.5755076 total: 864ms remaining: 23.1s 36: learn: 0.5736566 total: 878ms remaining: 22.9s 37: learn: 0.5717165 total: 902ms remaining: 22.8s 38: learn: 0.5696527 total: 925ms remaining: 22.8s 39: learn: 0.5677411 total: 947ms remaining: 22.7s 40: learn: 0.5658559 total: 971ms remaining: 22.7s 41: learn: 0.5639679 total: 994ms remaining: 22.7s 42: learn: 0.5621573 total: 1.02s remaining: 22.6s 43: learn: 0.5603957 total: 1.04s remaining: 22.6s 44: learn: 0.5585892 total: 1.07s remaining: 22.7s 45: learn: 0.5569204 total: 1.09s remaining: 22.6s 46: learn: 0.5552463 total: 1.11s remaining: 22.6s 47: learn: 0.5536250 total: 1.14s remaining: 22.6s 48: learn: 0.5520718 total: 1.16s remaining: 22.5s 49: learn: 0.5504821 total: 1.19s remaining: 22.6s 50: learn: 0.5489653 total: 1.21s remaining: 22.6s 51: learn: 0.5474109 total: 1.24s remaining: 22.6s 52: learn: 0.5459763 total: 1.26s remaining: 22.6s 53: learn: 0.5445562 total: 1.29s remaining: 22.6s 54: learn: 0.5431438 total: 1.31s remaining: 22.6s 55: learn: 0.5417719 total: 1.34s remaining: 22.6s 56: learn: 0.5404477 total: 1.36s remaining: 22.6s 57: learn: 0.5391074 total: 1.39s remaining: 22.5s 58: learn: 0.5377986 total: 1.41s remaining: 22.5s 59: learn: 0.5365282 total: 1.44s remaining: 22.5s 60: learn: 0.5353029 total: 1.46s remaining: 22.5s 61: learn: 0.5340683 total: 1.48s remaining: 22.4s 62: learn: 0.5329104 total: 1.51s remaining: 22.4s 63: learn: 0.5317566 total: 1.53s remaining: 22.4s 64: learn: 0.5306147 total: 1.55s remaining: 22.4s 65: learn: 0.5294422 total: 1.58s remaining: 22.4s 66: learn: 0.5283472 total: 1.6s remaining: 22.4s 67: learn: 0.5272628 total: 1.63s remaining: 22.4s 68: learn: 0.5262269 total: 1.66s remaining: 22.4s 69: learn: 0.5252543 total: 1.68s remaining: 22.3s 70: learn: 0.5242217 total: 1.71s remaining: 22.3s 71: learn: 0.5232597 total: 1.73s remaining: 22.3s 72: learn: 0.5222463 total: 1.76s remaining: 22.3s 73: learn: 0.5213520 total: 1.78s remaining: 22.3s 74: learn: 0.5204568 total: 1.8s remaining: 22.3s 75: learn: 0.5196299 total: 1.83s remaining: 22.2s 76: learn: 0.5187603 total: 1.85s remaining: 22.2s 77: learn: 0.5179326 total: 1.88s remaining: 22.2s 78: learn: 0.5170868 total: 1.9s remaining: 22.2s 79: learn: 0.5162588 total: 1.92s remaining: 22.1s 80: learn: 0.5154756 total: 1.95s remaining: 22.1s 81: learn: 0.5146912 total: 1.97s remaining: 22.1s 82: learn: 0.5139581 total: 2s remaining: 22.1s 83: learn: 0.5131996 total: 2.02s remaining: 22.1s 84: learn: 0.5124481 total: 2.04s remaining: 22s 85: learn: 0.5116752 total: 2.07s remaining: 22s 86: learn: 0.5109482 total: 2.1s remaining: 22s 87: learn: 0.5103112 total: 2.12s remaining: 22s 88: learn: 0.5096581 total: 2.15s remaining: 22s 89: learn: 0.5089431 total: 2.18s remaining: 22s 90: learn: 0.5082522 total: 2.2s remaining: 22s 91: learn: 0.5076517 total: 2.23s remaining: 22s 92: learn: 0.5070308 total: 2.25s remaining: 21.9s 93: learn: 0.5063997 total: 2.27s remaining: 21.9s 94: learn: 0.5058001 total: 2.3s remaining: 21.9s 95: learn: 0.5051906 total: 2.33s remaining: 21.9s 96: learn: 0.5046129 total: 2.35s remaining: 21.9s 97: learn: 0.5040508 total: 2.38s remaining: 21.9s 98: learn: 0.5034705 total: 2.4s remaining: 21.9s 99: learn: 0.5029049 total: 2.42s remaining: 21.8s 100: learn: 0.5023410 total: 2.45s remaining: 21.8s 101: learn: 0.5018054 total: 2.47s remaining: 21.8s 102: learn: 0.5012927 total: 2.5s remaining: 21.8s 103: learn: 0.5007620 total: 2.53s remaining: 21.8s 104: learn: 0.5002364 total: 2.55s remaining: 21.8s 105: learn: 0.4997446 total: 2.58s remaining: 21.7s 106: learn: 0.4992382 total: 2.6s remaining: 21.7s 107: learn: 0.4987651 total: 2.63s remaining: 21.7s 108: learn: 0.4983881 total: 2.64s remaining: 21.6s 109: learn: 0.4979742 total: 2.67s remaining: 21.6s 110: learn: 0.4975065 total: 2.7s remaining: 21.6s 111: learn: 0.4970813 total: 2.73s remaining: 21.6s 112: learn: 0.4966415 total: 2.75s remaining: 21.6s 113: learn: 0.4962375 total: 2.77s remaining: 21.6s 114: learn: 0.4958272 total: 2.8s remaining: 21.5s 115: learn: 0.4954165 total: 2.82s remaining: 21.5s 116: learn: 0.4950403 total: 2.85s remaining: 21.5s 117: learn: 0.4946756 total: 2.87s remaining: 21.5s 118: learn: 0.4943074 total: 2.9s remaining: 21.4s 119: learn: 0.4939117 total: 2.92s remaining: 21.4s 120: learn: 0.4935388 total: 2.95s remaining: 21.4s 121: learn: 0.4932137 total: 2.97s remaining: 21.4s 122: learn: 0.4928860 total: 3s remaining: 21.4s 123: learn: 0.4925254 total: 3.02s remaining: 21.4s 124: learn: 0.4922541 total: 3.04s remaining: 21.3s 125: learn: 0.4919255 total: 3.06s remaining: 21.2s 126: learn: 0.4915802 total: 3.08s remaining: 21.2s 127: learn: 0.4912600 total: 3.11s remaining: 21.2s 128: learn: 0.4909621 total: 3.13s remaining: 21.2s 129: learn: 0.4906550 total: 3.16s remaining: 21.2s 130: learn: 0.4903320 total: 3.19s remaining: 21.1s 131: learn: 0.4900083 total: 3.21s remaining: 21.1s 132: learn: 0.4896774 total: 3.24s remaining: 21.1s 133: learn: 0.4893788 total: 3.26s remaining: 21.1s 134: learn: 0.4890677 total: 3.29s remaining: 21.1s 135: learn: 0.4887482 total: 3.31s remaining: 21s 136: learn: 0.4884599 total: 3.34s remaining: 21s 137: learn: 0.4881724 total: 3.37s remaining: 21s 138: learn: 0.4878845 total: 3.39s remaining: 21s 139: learn: 0.4876288 total: 3.42s remaining: 21s 140: learn: 0.4873738 total: 3.44s remaining: 21s 141: learn: 0.4871044 total: 3.47s remaining: 20.9s 142: learn: 0.4868465 total: 3.5s remaining: 21s 143: learn: 0.4865916 total: 3.53s remaining: 21s 144: learn: 0.4863548 total: 3.57s remaining: 21.1s 145: learn: 0.4860985 total: 3.6s remaining: 21.1s 146: learn: 0.4858511 total: 3.63s remaining: 21.1s 147: learn: 0.4856274 total: 3.65s remaining: 21s 148: learn: 0.4853693 total: 3.68s remaining: 21s 149: learn: 0.4851541 total: 3.7s remaining: 21s 150: learn: 0.4849915 total: 3.72s remaining: 20.9s 151: learn: 0.4847782 total: 3.74s remaining: 20.9s 152: learn: 0.4845270 total: 3.77s remaining: 20.9s 153: learn: 0.4842832 total: 3.8s remaining: 20.9s 154: learn: 0.4840638 total: 3.82s remaining: 20.8s 155: learn: 0.4838815 total: 3.85s remaining: 20.8s 156: learn: 0.4836920 total: 3.87s remaining: 20.8s 157: learn: 0.4835030 total: 3.9s remaining: 20.8s 158: learn: 0.4832918 total: 3.92s remaining: 20.7s 159: learn: 0.4831511 total: 3.93s remaining: 20.7s 160: learn: 0.4829497 total: 3.96s remaining: 20.6s 161: learn: 0.4827754 total: 3.98s remaining: 20.6s 162: learn: 0.4825804 total: 4.01s remaining: 20.6s 163: learn: 0.4823916 total: 4.04s remaining: 20.6s 164: learn: 0.4822085 total: 4.06s remaining: 20.5s 165: learn: 0.4820429 total: 4.08s remaining: 20.5s 166: learn: 0.4818731 total: 4.11s remaining: 20.5s 167: learn: 0.4817004 total: 4.13s remaining: 20.5s 168: learn: 0.4815285 total: 4.16s remaining: 20.4s 169: learn: 0.4813635 total: 4.18s remaining: 20.4s 170: learn: 0.4812079 total: 4.2s remaining: 20.4s 171: learn: 0.4810382 total: 4.23s remaining: 20.4s 172: learn: 0.4808926 total: 4.25s remaining: 20.3s 173: learn: 0.4807381 total: 4.28s remaining: 20.3s 174: learn: 0.4805700 total: 4.3s remaining: 20.3s 175: learn: 0.4804400 total: 4.32s remaining: 20.2s 176: learn: 0.4802812 total: 4.35s remaining: 20.2s 177: learn: 0.4801280 total: 4.37s remaining: 20.2s 178: learn: 0.4799842 total: 4.39s remaining: 20.2s 179: learn: 0.4798398 total: 4.42s remaining: 20.1s 180: learn: 0.4797125 total: 4.44s remaining: 20.1s 181: learn: 0.4795689 total: 4.47s remaining: 20.1s 182: learn: 0.4794479 total: 4.49s remaining: 20s 183: learn: 0.4792997 total: 4.51s remaining: 20s 184: learn: 0.4791614 total: 4.54s remaining: 20s 185: learn: 0.4790325 total: 4.56s remaining: 20s 186: learn: 0.4789103 total: 4.59s remaining: 19.9s 187: learn: 0.4787746 total: 4.61s remaining: 19.9s 188: learn: 0.4786240 total: 4.64s remaining: 19.9s 189: learn: 0.4784893 total: 4.66s remaining: 19.9s 190: learn: 0.4783515 total: 4.69s remaining: 19.9s 191: learn: 0.4782205 total: 4.71s remaining: 19.8s 192: learn: 0.4780959 total: 4.74s remaining: 19.8s 193: learn: 0.4779926 total: 4.76s remaining: 19.8s 194: learn: 0.4778871 total: 4.78s remaining: 19.7s 195: learn: 0.4777654 total: 4.81s remaining: 19.7s 196: learn: 0.4776578 total: 4.83s remaining: 19.7s 197: learn: 0.4775341 total: 4.85s remaining: 19.7s 198: learn: 0.4773985 total: 4.88s remaining: 19.6s 199: learn: 0.4772849 total: 4.91s remaining: 19.6s 200: learn: 0.4771735 total: 4.93s remaining: 19.6s 201: learn: 0.4770525 total: 4.95s remaining: 19.6s 202: learn: 0.4769315 total: 4.98s remaining: 19.5s 203: learn: 0.4768213 total: 5s remaining: 19.5s 204: learn: 0.4767196 total: 5.03s remaining: 19.5s 205: learn: 0.4766273 total: 5.05s remaining: 19.5s 206: learn: 0.4765335 total: 5.07s remaining: 19.4s 207: learn: 0.4764282 total: 5.1s remaining: 19.4s 208: learn: 0.4763482 total: 5.12s remaining: 19.4s 209: learn: 0.4762623 total: 5.15s remaining: 19.4s 210: learn: 0.4761735 total: 5.17s remaining: 19.3s 211: learn: 0.4760733 total: 5.19s remaining: 19.3s 212: learn: 0.4759979 total: 5.22s remaining: 19.3s 213: learn: 0.4758779 total: 5.24s remaining: 19.3s 214: learn: 0.4757827 total: 5.26s remaining: 19.2s 215: learn: 0.4757053 total: 5.29s remaining: 19.2s 216: learn: 0.4756008 total: 5.32s remaining: 19.2s 217: learn: 0.4755150 total: 5.34s remaining: 19.2s 218: learn: 0.4754208 total: 5.36s remaining: 19.1s 219: learn: 0.4753210 total: 5.39s remaining: 19.1s 220: learn: 0.4752463 total: 5.41s remaining: 19.1s 221: learn: 0.4751665 total: 5.44s remaining: 19.1s 222: learn: 0.4750788 total: 5.46s remaining: 19s 223: learn: 0.4749864 total: 5.49s remaining: 19s 224: learn: 0.4748725 total: 5.51s remaining: 19s 225: learn: 0.4747832 total: 5.54s remaining: 19s 226: learn: 0.4747065 total: 5.56s remaining: 18.9s 227: learn: 0.4746132 total: 5.59s remaining: 18.9s 228: learn: 0.4745160 total: 5.61s remaining: 18.9s 229: learn: 0.4744390 total: 5.64s remaining: 18.9s 230: learn: 0.4743604 total: 5.66s remaining: 18.9s 231: learn: 0.4742583 total: 5.69s remaining: 18.8s 232: learn: 0.4741847 total: 5.71s remaining: 18.8s 233: learn: 0.4740959 total: 5.74s remaining: 18.8s 234: learn: 0.4740094 total: 5.76s remaining: 18.8s 235: learn: 0.4739407 total: 5.79s remaining: 18.7s 236: learn: 0.4738660 total: 5.81s remaining: 18.7s 237: learn: 0.4737837 total: 5.84s remaining: 18.7s 238: learn: 0.4737104 total: 5.86s remaining: 18.7s 239: learn: 0.4736311 total: 5.89s remaining: 18.6s 240: learn: 0.4735539 total: 5.91s remaining: 18.6s 241: learn: 0.4734807 total: 5.94s remaining: 18.6s 242: learn: 0.4734141 total: 5.96s remaining: 18.6s 243: learn: 0.4733445 total: 5.99s remaining: 18.6s 244: learn: 0.4732736 total: 6.01s remaining: 18.5s 245: learn: 0.4731923 total: 6.04s remaining: 18.5s 246: learn: 0.4731118 total: 6.06s remaining: 18.5s 247: learn: 0.4730604 total: 6.08s remaining: 18.5s 248: learn: 0.4729942 total: 6.13s remaining: 18.5s 249: learn: 0.4729268 total: 6.23s remaining: 18.7s 250: learn: 0.4728592 total: 6.29s remaining: 18.8s 251: learn: 0.4727932 total: 6.32s remaining: 18.7s 252: learn: 0.4727464 total: 6.34s remaining: 18.7s 253: learn: 0.4726803 total: 6.37s remaining: 18.7s 254: learn: 0.4726170 total: 6.39s remaining: 18.7s 255: learn: 0.4725995 total: 6.4s remaining: 18.6s 256: learn: 0.4725542 total: 6.43s remaining: 18.6s 257: learn: 0.4724986 total: 6.45s remaining: 18.6s 258: learn: 0.4724363 total: 6.48s remaining: 18.5s 259: learn: 0.4723640 total: 6.5s remaining: 18.5s 260: learn: 0.4723009 total: 6.53s remaining: 18.5s 261: learn: 0.4722426 total: 6.55s remaining: 18.5s 262: learn: 0.4721912 total: 6.58s remaining: 18.4s 263: learn: 0.4721549 total: 6.6s remaining: 18.4s 264: learn: 0.4720826 total: 6.63s remaining: 18.4s 265: learn: 0.4720172 total: 6.65s remaining: 18.3s 266: learn: 0.4719535 total: 6.67s remaining: 18.3s 267: learn: 0.4719023 total: 6.7s remaining: 18.3s 268: learn: 0.4718534 total: 6.72s remaining: 18.3s 269: learn: 0.4717885 total: 6.74s remaining: 18.2s 270: learn: 0.4717176 total: 6.77s remaining: 18.2s 271: learn: 0.4716617 total: 6.79s remaining: 18.2s 272: learn: 0.4715830 total: 6.82s remaining: 18.2s 273: learn: 0.4715357 total: 6.84s remaining: 18.1s 274: learn: 0.4714868 total: 6.87s remaining: 18.1s 275: learn: 0.4714388 total: 6.89s remaining: 18.1s 276: learn: 0.4713894 total: 6.91s remaining: 18s 277: learn: 0.4713289 total: 6.94s remaining: 18s 278: learn: 0.4712782 total: 6.96s remaining: 18s 279: learn: 0.4712199 total: 7s remaining: 18s 280: learn: 0.4711798 total: 7.03s remaining: 18s 281: learn: 0.4711284 total: 7.06s remaining: 18s 282: learn: 0.4710726 total: 7.09s remaining: 18s 283: learn: 0.4710250 total: 7.13s remaining: 18s 284: learn: 0.4709614 total: 7.16s remaining: 18s 285: learn: 0.4709017 total: 7.19s remaining: 18s 286: learn: 0.4708453 total: 7.22s remaining: 17.9s 287: learn: 0.4707988 total: 7.25s remaining: 17.9s 288: learn: 0.4707513 total: 7.28s remaining: 17.9s 289: learn: 0.4706960 total: 7.32s remaining: 17.9s 290: learn: 0.4706629 total: 7.35s remaining: 17.9s 291: learn: 0.4706212 total: 7.38s remaining: 17.9s 292: learn: 0.4705679 total: 7.41s remaining: 17.9s 293: learn: 0.4705116 total: 7.45s remaining: 17.9s 294: learn: 0.4704656 total: 7.48s remaining: 17.9s 295: learn: 0.4704126 total: 7.51s remaining: 17.9s 296: learn: 0.4703708 total: 7.54s remaining: 17.9s 297: learn: 0.4703217 total: 7.57s remaining: 17.8s 298: learn: 0.4702804 total: 7.6s remaining: 17.8s 299: learn: 0.4702280 total: 7.63s remaining: 17.8s 300: learn: 0.4701886 total: 7.66s remaining: 17.8s 301: learn: 0.4701525 total: 7.69s remaining: 17.8s 302: learn: 0.4701154 total: 7.71s remaining: 17.7s 303: learn: 0.4700790 total: 7.75s remaining: 17.7s 304: learn: 0.4700324 total: 7.78s remaining: 17.7s 305: learn: 0.4699747 total: 7.81s remaining: 17.7s 306: learn: 0.4699302 total: 7.83s remaining: 17.7s 307: learn: 0.4698923 total: 7.86s remaining: 17.7s 308: learn: 0.4698518 total: 7.89s remaining: 17.6s 309: learn: 0.4698233 total: 7.92s remaining: 17.6s 310: learn: 0.4697744 total: 7.95s remaining: 17.6s 311: learn: 0.4697332 total: 7.98s remaining: 17.6s 312: learn: 0.4696965 total: 8.01s remaining: 17.6s 313: learn: 0.4696554 total: 8.03s remaining: 17.6s 314: learn: 0.4696063 total: 8.06s remaining: 17.5s 315: learn: 0.4695670 total: 8.09s remaining: 17.5s 316: learn: 0.4695291 total: 8.12s remaining: 17.5s 317: learn: 0.4694878 total: 8.15s remaining: 17.5s 318: learn: 0.4694437 total: 8.17s remaining: 17.4s 319: learn: 0.4694075 total: 8.2s remaining: 17.4s 320: learn: 0.4693653 total: 8.22s remaining: 17.4s 321: learn: 0.4693186 total: 8.24s remaining: 17.4s 322: learn: 0.4692856 total: 8.27s remaining: 17.3s 323: learn: 0.4692345 total: 8.29s remaining: 17.3s 324: learn: 0.4692009 total: 8.31s remaining: 17.3s 325: learn: 0.4691615 total: 8.34s remaining: 17.2s 326: learn: 0.4691261 total: 8.36s remaining: 17.2s 327: learn: 0.4690797 total: 8.38s remaining: 17.2s 328: learn: 0.4690424 total: 8.41s remaining: 17.1s 329: learn: 0.4689856 total: 8.43s remaining: 17.1s 330: learn: 0.4689386 total: 8.45s remaining: 17.1s 331: learn: 0.4689094 total: 8.47s remaining: 17.1s 332: learn: 0.4688729 total: 8.5s remaining: 17s 333: learn: 0.4688315 total: 8.52s remaining: 17s 334: learn: 0.4687920 total: 8.55s remaining: 17s 335: learn: 0.4687552 total: 8.57s remaining: 16.9s 336: learn: 0.4687154 total: 8.6s remaining: 16.9s 337: learn: 0.4686667 total: 8.62s remaining: 16.9s 338: learn: 0.4686390 total: 8.65s remaining: 16.9s 339: learn: 0.4685975 total: 8.67s remaining: 16.8s 340: learn: 0.4685549 total: 8.69s remaining: 16.8s 341: learn: 0.4685488 total: 8.7s remaining: 16.7s 342: learn: 0.4685116 total: 8.73s remaining: 16.7s 343: learn: 0.4684789 total: 8.75s remaining: 16.7s 344: learn: 0.4684390 total: 8.77s remaining: 16.7s 345: learn: 0.4683998 total: 8.8s remaining: 16.6s 346: learn: 0.4683660 total: 8.82s remaining: 16.6s 347: learn: 0.4683321 total: 8.85s remaining: 16.6s 348: learn: 0.4682982 total: 8.87s remaining: 16.5s 349: learn: 0.4682504 total: 8.89s remaining: 16.5s 350: learn: 0.4682185 total: 8.91s remaining: 16.5s 351: learn: 0.4681818 total: 8.94s remaining: 16.5s 352: learn: 0.4681308 total: 8.96s remaining: 16.4s 353: learn: 0.4680877 total: 8.99s remaining: 16.4s 354: learn: 0.4680679 total: 9.01s remaining: 16.4s 355: learn: 0.4680274 total: 9.03s remaining: 16.3s 356: learn: 0.4679898 total: 9.05s remaining: 16.3s 357: learn: 0.4679634 total: 9.08s remaining: 16.3s 358: learn: 0.4679364 total: 9.1s remaining: 16.3s 359: learn: 0.4679083 total: 9.13s remaining: 16.2s 360: learn: 0.4678759 total: 9.15s remaining: 16.2s 361: learn: 0.4678355 total: 9.17s remaining: 16.2s 362: learn: 0.4677858 total: 9.2s remaining: 16.1s 363: learn: 0.4677275 total: 9.22s remaining: 16.1s 364: learn: 0.4676922 total: 9.25s remaining: 16.1s 365: learn: 0.4676488 total: 9.27s remaining: 16.1s 366: learn: 0.4676148 total: 9.29s remaining: 16s 367: learn: 0.4675839 total: 9.32s remaining: 16s 368: learn: 0.4675372 total: 9.34s remaining: 16s 369: learn: 0.4675135 total: 9.36s remaining: 15.9s 370: learn: 0.4674817 total: 9.39s remaining: 15.9s 371: learn: 0.4674287 total: 9.41s remaining: 15.9s 372: learn: 0.4674151 total: 9.43s remaining: 15.9s 373: learn: 0.4673884 total: 9.45s remaining: 15.8s 374: learn: 0.4673525 total: 9.48s remaining: 15.8s 375: learn: 0.4673205 total: 9.5s remaining: 15.8s 376: learn: 0.4672887 total: 9.52s remaining: 15.7s 377: learn: 0.4672546 total: 9.55s remaining: 15.7s 378: learn: 0.4672294 total: 9.57s remaining: 15.7s 379: learn: 0.4671776 total: 9.6s remaining: 15.7s 380: learn: 0.4671438 total: 9.62s remaining: 15.6s 381: learn: 0.4670930 total: 9.65s remaining: 15.6s 382: learn: 0.4670640 total: 9.67s remaining: 15.6s 383: learn: 0.4670421 total: 9.69s remaining: 15.5s 384: learn: 0.4670226 total: 9.71s remaining: 15.5s 385: learn: 0.4669924 total: 9.74s remaining: 15.5s 386: learn: 0.4669590 total: 9.76s remaining: 15.5s 387: learn: 0.4669249 total: 9.78s remaining: 15.4s 388: learn: 0.4668914 total: 9.81s remaining: 15.4s 389: learn: 0.4668551 total: 9.84s remaining: 15.4s 390: learn: 0.4667978 total: 9.86s remaining: 15.4s 391: learn: 0.4667640 total: 9.88s remaining: 15.3s 392: learn: 0.4667328 total: 9.91s remaining: 15.3s 393: learn: 0.4667057 total: 9.93s remaining: 15.3s 394: learn: 0.4666729 total: 9.96s remaining: 15.2s 395: learn: 0.4666403 total: 9.98s remaining: 15.2s 396: learn: 0.4666114 total: 10s remaining: 15.2s 397: learn: 0.4665753 total: 10s remaining: 15.2s 398: learn: 0.4665430 total: 10.1s remaining: 15.1s 399: learn: 0.4665105 total: 10.1s remaining: 15.1s 400: learn: 0.4664574 total: 10.1s remaining: 15.1s 401: learn: 0.4664287 total: 10.1s remaining: 15.1s 402: learn: 0.4664050 total: 10.1s remaining: 15s 403: learn: 0.4663821 total: 10.2s remaining: 15s 404: learn: 0.4663643 total: 10.2s remaining: 15s 405: learn: 0.4663232 total: 10.2s remaining: 14.9s 406: learn: 0.4662958 total: 10.2s remaining: 14.9s 407: learn: 0.4662706 total: 10.3s remaining: 14.9s 408: learn: 0.4662425 total: 10.3s remaining: 14.9s 409: learn: 0.4661981 total: 10.3s remaining: 14.8s 410: learn: 0.4661695 total: 10.3s remaining: 14.8s 411: learn: 0.4661311 total: 10.4s remaining: 14.8s 412: learn: 0.4661092 total: 10.4s remaining: 14.7s 413: learn: 0.4660792 total: 10.4s remaining: 14.7s 414: learn: 0.4660495 total: 10.4s remaining: 14.7s 415: learn: 0.4660280 total: 10.4s remaining: 14.7s 416: learn: 0.4659799 total: 10.5s remaining: 14.6s 417: learn: 0.4659521 total: 10.5s remaining: 14.6s 418: learn: 0.4659136 total: 10.5s remaining: 14.6s 419: learn: 0.4658956 total: 10.5s remaining: 14.6s 420: learn: 0.4658665 total: 10.6s remaining: 14.5s 421: learn: 0.4658280 total: 10.6s remaining: 14.5s 422: learn: 0.4657990 total: 10.6s remaining: 14.5s 423: learn: 0.4657658 total: 10.6s remaining: 14.4s 424: learn: 0.4657396 total: 10.7s remaining: 14.4s 425: learn: 0.4657056 total: 10.7s remaining: 14.4s 426: learn: 0.4656819 total: 10.7s remaining: 14.4s 427: learn: 0.4656492 total: 10.7s remaining: 14.3s 428: learn: 0.4656146 total: 10.8s remaining: 14.3s 429: learn: 0.4656093 total: 10.8s remaining: 14.3s 430: learn: 0.4655778 total: 10.8s remaining: 14.2s 431: learn: 0.4655458 total: 10.8s remaining: 14.2s 432: learn: 0.4655261 total: 10.8s remaining: 14.2s 433: learn: 0.4655002 total: 10.9s remaining: 14.2s 434: learn: 0.4654702 total: 10.9s remaining: 14.1s 435: learn: 0.4654481 total: 10.9s remaining: 14.1s 436: learn: 0.4654259 total: 10.9s remaining: 14.1s 437: learn: 0.4654023 total: 10.9s remaining: 14s 438: learn: 0.4653630 total: 11s remaining: 14s 439: learn: 0.4653424 total: 11s remaining: 14s 440: learn: 0.4653026 total: 11s remaining: 14s 441: learn: 0.4652672 total: 11s remaining: 13.9s 442: learn: 0.4652472 total: 11.1s remaining: 13.9s 443: learn: 0.4652149 total: 11.1s remaining: 13.9s 444: learn: 0.4651717 total: 11.1s remaining: 13.9s 445: learn: 0.4651446 total: 11.1s remaining: 13.8s 446: learn: 0.4651118 total: 11.2s remaining: 13.8s 447: learn: 0.4650817 total: 11.2s remaining: 13.8s 448: learn: 0.4650518 total: 11.2s remaining: 13.7s 449: learn: 0.4650259 total: 11.2s remaining: 13.7s 450: learn: 0.4649924 total: 11.3s remaining: 13.7s 451: learn: 0.4649556 total: 11.3s remaining: 13.7s 452: learn: 0.4649371 total: 11.3s remaining: 13.6s 453: learn: 0.4649141 total: 11.3s remaining: 13.6s 454: learn: 0.4648867 total: 11.3s remaining: 13.6s 455: learn: 0.4648588 total: 11.4s remaining: 13.6s 456: learn: 0.4648250 total: 11.4s remaining: 13.5s 457: learn: 0.4647940 total: 11.4s remaining: 13.5s 458: learn: 0.4647546 total: 11.4s remaining: 13.5s 459: learn: 0.4647189 total: 11.5s remaining: 13.5s 460: learn: 0.4646901 total: 11.5s remaining: 13.4s 461: learn: 0.4646688 total: 11.5s remaining: 13.4s 462: learn: 0.4646478 total: 11.5s remaining: 13.4s 463: learn: 0.4646157 total: 11.6s remaining: 13.3s 464: learn: 0.4645898 total: 11.6s remaining: 13.3s 465: learn: 0.4645564 total: 11.6s remaining: 13.3s 466: learn: 0.4645328 total: 11.6s remaining: 13.3s 467: learn: 0.4645155 total: 11.6s remaining: 13.2s 468: learn: 0.4644996 total: 11.7s remaining: 13.2s 469: learn: 0.4644666 total: 11.7s remaining: 13.2s 470: learn: 0.4644513 total: 11.7s remaining: 13.2s 471: learn: 0.4644258 total: 11.7s remaining: 13.1s 472: learn: 0.4644093 total: 11.8s remaining: 13.1s 473: learn: 0.4643853 total: 11.8s remaining: 13.1s 474: learn: 0.4643599 total: 11.8s remaining: 13.1s 475: learn: 0.4643203 total: 11.8s remaining: 13s 476: learn: 0.4642956 total: 11.9s remaining: 13s 477: learn: 0.4642900 total: 11.9s remaining: 13s 478: learn: 0.4642673 total: 11.9s remaining: 12.9s 479: learn: 0.4642508 total: 11.9s remaining: 12.9s 480: learn: 0.4642142 total: 11.9s remaining: 12.9s 481: learn: 0.4641946 total: 12s remaining: 12.9s 482: learn: 0.4641754 total: 12s remaining: 12.8s 483: learn: 0.4641545 total: 12s remaining: 12.8s 484: learn: 0.4641318 total: 12s remaining: 12.8s 485: learn: 0.4641079 total: 12.1s remaining: 12.7s 486: learn: 0.4640660 total: 12.1s remaining: 12.7s 487: learn: 0.4640376 total: 12.1s remaining: 12.7s 488: learn: 0.4640081 total: 12.1s remaining: 12.7s 489: learn: 0.4639916 total: 12.1s remaining: 12.6s 490: learn: 0.4639589 total: 12.2s remaining: 12.6s 491: learn: 0.4639303 total: 12.2s remaining: 12.6s 492: learn: 0.4639081 total: 12.2s remaining: 12.6s 493: learn: 0.4638912 total: 12.2s remaining: 12.5s 494: learn: 0.4638710 total: 12.3s remaining: 12.5s 495: learn: 0.4638476 total: 12.3s remaining: 12.5s 496: learn: 0.4638259 total: 12.3s remaining: 12.5s 497: learn: 0.4637943 total: 12.3s remaining: 12.4s 498: learn: 0.4637668 total: 12.4s remaining: 12.4s 499: learn: 0.4637471 total: 12.4s remaining: 12.4s 500: learn: 0.4637232 total: 12.4s remaining: 12.4s 501: learn: 0.4637067 total: 12.4s remaining: 12.3s 502: learn: 0.4636740 total: 12.4s remaining: 12.3s 503: learn: 0.4636452 total: 12.5s remaining: 12.3s 504: learn: 0.4636246 total: 12.5s remaining: 12.2s 505: learn: 0.4636054 total: 12.5s remaining: 12.2s 506: learn: 0.4635891 total: 12.5s remaining: 12.2s 507: learn: 0.4635682 total: 12.6s remaining: 12.2s 508: learn: 0.4635449 total: 12.6s remaining: 12.1s 509: learn: 0.4635268 total: 12.6s remaining: 12.1s 510: learn: 0.4635089 total: 12.6s remaining: 12.1s 511: learn: 0.4634711 total: 12.6s remaining: 12.1s 512: learn: 0.4634321 total: 12.7s remaining: 12s 513: learn: 0.4633932 total: 12.7s remaining: 12s 514: learn: 0.4633642 total: 12.7s remaining: 12s 515: learn: 0.4633316 total: 12.7s remaining: 12s 516: learn: 0.4633065 total: 12.8s remaining: 11.9s 517: learn: 0.4632862 total: 12.8s remaining: 11.9s 518: learn: 0.4632698 total: 12.8s remaining: 11.9s 519: learn: 0.4632225 total: 12.8s remaining: 11.8s 520: learn: 0.4632012 total: 12.9s remaining: 11.8s 521: learn: 0.4631620 total: 12.9s remaining: 11.8s 522: learn: 0.4631414 total: 12.9s remaining: 11.8s 523: learn: 0.4631194 total: 12.9s remaining: 11.7s 524: learn: 0.4630930 total: 12.9s remaining: 11.7s 525: learn: 0.4630583 total: 13s remaining: 11.7s 526: learn: 0.4630285 total: 13s remaining: 11.7s 527: learn: 0.4630087 total: 13s remaining: 11.6s 528: learn: 0.4629701 total: 13s remaining: 11.6s 529: learn: 0.4629498 total: 13.1s remaining: 11.6s 530: learn: 0.4629261 total: 13.1s remaining: 11.6s 531: learn: 0.4629016 total: 13.1s remaining: 11.5s 532: learn: 0.4628997 total: 13.1s remaining: 11.5s 533: learn: 0.4628804 total: 13.1s remaining: 11.5s 534: learn: 0.4628440 total: 13.2s remaining: 11.4s 535: learn: 0.4628268 total: 13.2s remaining: 11.4s 536: learn: 0.4628064 total: 13.2s remaining: 11.4s 537: learn: 0.4627887 total: 13.2s remaining: 11.4s 538: learn: 0.4627678 total: 13.3s remaining: 11.3s 539: learn: 0.4627416 total: 13.3s remaining: 11.3s 540: learn: 0.4627175 total: 13.3s remaining: 11.3s 541: learn: 0.4626954 total: 13.3s remaining: 11.3s 542: learn: 0.4626756 total: 13.3s remaining: 11.2s 543: learn: 0.4626457 total: 13.4s remaining: 11.2s 544: learn: 0.4626263 total: 13.4s remaining: 11.2s 545: learn: 0.4625989 total: 13.4s remaining: 11.2s 546: learn: 0.4625737 total: 13.5s remaining: 11.1s 547: learn: 0.4625547 total: 13.5s remaining: 11.1s 548: learn: 0.4625313 total: 13.5s remaining: 11.1s 549: learn: 0.4625298 total: 13.5s remaining: 11.1s 550: learn: 0.4625013 total: 13.5s remaining: 11s 551: learn: 0.4624840 total: 13.6s remaining: 11s 552: learn: 0.4624610 total: 13.6s remaining: 11s 553: learn: 0.4624321 total: 13.6s remaining: 11s 554: learn: 0.4623955 total: 13.6s remaining: 10.9s 555: learn: 0.4623654 total: 13.7s remaining: 10.9s 556: learn: 0.4623306 total: 13.7s remaining: 10.9s 557: learn: 0.4623076 total: 13.7s remaining: 10.9s 558: learn: 0.4622805 total: 13.7s remaining: 10.8s 559: learn: 0.4622622 total: 13.8s remaining: 10.8s 560: learn: 0.4622486 total: 13.8s remaining: 10.8s 561: learn: 0.4622117 total: 13.8s remaining: 10.8s 562: learn: 0.4621923 total: 13.8s remaining: 10.7s 563: learn: 0.4621611 total: 13.8s remaining: 10.7s 564: learn: 0.4621252 total: 13.9s remaining: 10.7s 565: learn: 0.4621024 total: 13.9s remaining: 10.7s 566: learn: 0.4620826 total: 13.9s remaining: 10.6s 567: learn: 0.4620599 total: 13.9s remaining: 10.6s 568: learn: 0.4620290 total: 14s remaining: 10.6s 569: learn: 0.4620077 total: 14s remaining: 10.6s 570: learn: 0.4619892 total: 14s remaining: 10.5s 571: learn: 0.4619696 total: 14s remaining: 10.5s 572: learn: 0.4619295 total: 14.1s remaining: 10.5s 573: learn: 0.4619020 total: 14.1s remaining: 10.5s 574: learn: 0.4618889 total: 14.1s remaining: 10.4s 575: learn: 0.4618631 total: 14.1s remaining: 10.4s 576: learn: 0.4618511 total: 14.1s remaining: 10.4s 577: learn: 0.4618254 total: 14.2s remaining: 10.3s 578: learn: 0.4617986 total: 14.2s remaining: 10.3s 579: learn: 0.4617769 total: 14.2s remaining: 10.3s 580: learn: 0.4617501 total: 14.2s remaining: 10.3s 581: learn: 0.4617394 total: 14.3s remaining: 10.2s 582: learn: 0.4617182 total: 14.3s remaining: 10.2s 583: learn: 0.4616982 total: 14.3s remaining: 10.2s 584: learn: 0.4616772 total: 14.3s remaining: 10.2s 585: learn: 0.4616512 total: 14.4s remaining: 10.1s 586: learn: 0.4616288 total: 14.4s remaining: 10.1s 587: learn: 0.4616152 total: 14.4s remaining: 10.1s 588: learn: 0.4615934 total: 14.5s remaining: 10.1s 589: learn: 0.4615703 total: 14.5s remaining: 10.1s 590: learn: 0.4615505 total: 14.5s remaining: 10s 591: learn: 0.4615333 total: 14.5s remaining: 10s 592: learn: 0.4615128 total: 14.6s remaining: 10s 593: learn: 0.4614878 total: 14.6s remaining: 9.97s 594: learn: 0.4614633 total: 14.6s remaining: 9.95s 595: learn: 0.4614457 total: 14.6s remaining: 9.92s 596: learn: 0.4614247 total: 14.7s remaining: 9.89s 597: learn: 0.4614077 total: 14.7s remaining: 9.87s 598: learn: 0.4613902 total: 14.7s remaining: 9.84s 599: learn: 0.4613635 total: 14.7s remaining: 9.82s 600: learn: 0.4613463 total: 14.7s remaining: 9.79s 601: learn: 0.4613203 total: 14.8s remaining: 9.77s 602: learn: 0.4613012 total: 14.8s remaining: 9.74s 603: learn: 0.4612828 total: 14.8s remaining: 9.71s 604: learn: 0.4612645 total: 14.8s remaining: 9.69s 605: learn: 0.4612359 total: 14.9s remaining: 9.66s 606: learn: 0.4612195 total: 14.9s remaining: 9.63s 607: learn: 0.4612010 total: 14.9s remaining: 9.61s 608: learn: 0.4611876 total: 14.9s remaining: 9.58s 609: learn: 0.4611520 total: 15s remaining: 9.56s 610: learn: 0.4611268 total: 15s remaining: 9.53s 611: learn: 0.4611067 total: 15s remaining: 9.51s 612: learn: 0.4610767 total: 15s remaining: 9.48s 613: learn: 0.4610560 total: 15s remaining: 9.46s 614: learn: 0.4610269 total: 15.1s remaining: 9.43s 615: learn: 0.4610179 total: 15.1s remaining: 9.4s 616: learn: 0.4609943 total: 15.1s remaining: 9.38s 617: learn: 0.4609806 total: 15.1s remaining: 9.35s 618: learn: 0.4609726 total: 15.1s remaining: 9.32s 619: learn: 0.4609495 total: 15.2s remaining: 9.3s 620: learn: 0.4609341 total: 15.2s remaining: 9.27s 621: learn: 0.4609171 total: 15.2s remaining: 9.24s 622: learn: 0.4608912 total: 15.2s remaining: 9.22s 623: learn: 0.4608739 total: 15.3s remaining: 9.2s 624: learn: 0.4608477 total: 15.3s remaining: 9.17s 625: learn: 0.4608232 total: 15.3s remaining: 9.14s 626: learn: 0.4607958 total: 15.3s remaining: 9.12s 627: learn: 0.4607737 total: 15.4s remaining: 9.1s 628: learn: 0.4607536 total: 15.4s remaining: 9.07s 629: learn: 0.4607321 total: 15.4s remaining: 9.04s 630: learn: 0.4606962 total: 15.4s remaining: 9.02s 631: learn: 0.4606798 total: 15.4s remaining: 8.99s 632: learn: 0.4606626 total: 15.5s remaining: 8.97s 633: learn: 0.4606421 total: 15.5s remaining: 8.94s 634: learn: 0.4606160 total: 15.5s remaining: 8.92s 635: learn: 0.4606055 total: 15.5s remaining: 8.89s 636: learn: 0.4605804 total: 15.6s remaining: 8.87s 637: learn: 0.4605600 total: 15.6s remaining: 8.84s 638: learn: 0.4605430 total: 15.6s remaining: 8.81s 639: learn: 0.4604852 total: 15.6s remaining: 8.79s 640: learn: 0.4604722 total: 15.7s remaining: 8.77s 641: learn: 0.4604579 total: 15.7s remaining: 8.74s 642: learn: 0.4604315 total: 15.7s remaining: 8.72s 643: learn: 0.4604011 total: 15.7s remaining: 8.69s 644: learn: 0.4603772 total: 15.7s remaining: 8.67s 645: learn: 0.4603554 total: 15.8s remaining: 8.64s 646: learn: 0.4603449 total: 15.8s remaining: 8.62s 647: learn: 0.4603269 total: 15.8s remaining: 8.59s 648: learn: 0.4603107 total: 15.8s remaining: 8.56s 649: learn: 0.4602941 total: 15.9s remaining: 8.54s 650: learn: 0.4602784 total: 15.9s remaining: 8.51s 651: learn: 0.4602536 total: 15.9s remaining: 8.49s 652: learn: 0.4602419 total: 15.9s remaining: 8.46s 653: learn: 0.4602243 total: 15.9s remaining: 8.44s 654: learn: 0.4602097 total: 16s remaining: 8.41s 655: learn: 0.4601953 total: 16s remaining: 8.39s 656: learn: 0.4601696 total: 16s remaining: 8.36s 657: learn: 0.4601489 total: 16s remaining: 8.34s 658: learn: 0.4601240 total: 16.1s remaining: 8.31s 659: learn: 0.4601054 total: 16.1s remaining: 8.29s 660: learn: 0.4600876 total: 16.1s remaining: 8.26s 661: learn: 0.4600683 total: 16.1s remaining: 8.23s 662: learn: 0.4600450 total: 16.2s remaining: 8.21s 663: learn: 0.4600040 total: 16.2s remaining: 8.19s 664: learn: 0.4599875 total: 16.2s remaining: 8.16s 665: learn: 0.4599516 total: 16.2s remaining: 8.14s 666: learn: 0.4599331 total: 16.2s remaining: 8.11s 667: learn: 0.4599103 total: 16.3s remaining: 8.09s 668: learn: 0.4598994 total: 16.3s remaining: 8.06s 669: learn: 0.4598961 total: 16.3s remaining: 8.03s 670: learn: 0.4598790 total: 16.3s remaining: 8.01s 671: learn: 0.4598607 total: 16.4s remaining: 7.98s 672: learn: 0.4598472 total: 16.4s remaining: 7.96s 673: learn: 0.4598206 total: 16.4s remaining: 7.93s 674: learn: 0.4597874 total: 16.4s remaining: 7.91s 675: learn: 0.4597532 total: 16.4s remaining: 7.88s 676: learn: 0.4597350 total: 16.5s remaining: 7.86s 677: learn: 0.4597119 total: 16.5s remaining: 7.83s 678: learn: 0.4596926 total: 16.5s remaining: 7.81s 679: learn: 0.4596621 total: 16.5s remaining: 7.78s 680: learn: 0.4596408 total: 16.6s remaining: 7.76s 681: learn: 0.4596258 total: 16.6s remaining: 7.73s 682: learn: 0.4595994 total: 16.6s remaining: 7.71s 683: learn: 0.4595768 total: 16.6s remaining: 7.68s 684: learn: 0.4595568 total: 16.7s remaining: 7.66s 685: learn: 0.4595370 total: 16.7s remaining: 7.63s 686: learn: 0.4595153 total: 16.7s remaining: 7.61s 687: learn: 0.4594958 total: 16.7s remaining: 7.58s 688: learn: 0.4594774 total: 16.8s remaining: 7.56s 689: learn: 0.4594549 total: 16.8s remaining: 7.54s 690: learn: 0.4594402 total: 16.8s remaining: 7.51s 691: learn: 0.4594128 total: 16.8s remaining: 7.49s 692: learn: 0.4593982 total: 16.8s remaining: 7.46s 693: learn: 0.4593871 total: 16.9s remaining: 7.43s 694: learn: 0.4593710 total: 16.9s remaining: 7.41s 695: learn: 0.4593510 total: 16.9s remaining: 7.38s 696: learn: 0.4593407 total: 16.9s remaining: 7.36s 697: learn: 0.4593223 total: 17s remaining: 7.34s 698: learn: 0.4593092 total: 17s remaining: 7.31s 699: learn: 0.4592780 total: 17s remaining: 7.29s 700: learn: 0.4592626 total: 17s remaining: 7.26s 701: learn: 0.4592435 total: 17s remaining: 7.24s 702: learn: 0.4592295 total: 17.1s remaining: 7.21s 703: learn: 0.4592147 total: 17.1s remaining: 7.19s 704: learn: 0.4592007 total: 17.1s remaining: 7.16s 705: learn: 0.4591784 total: 17.1s remaining: 7.14s 706: learn: 0.4591636 total: 17.2s remaining: 7.11s 707: learn: 0.4591430 total: 17.2s remaining: 7.09s 708: learn: 0.4591243 total: 17.2s remaining: 7.06s 709: learn: 0.4590984 total: 17.2s remaining: 7.04s 710: learn: 0.4590806 total: 17.3s remaining: 7.01s 711: learn: 0.4590781 total: 17.3s remaining: 6.98s 712: learn: 0.4590593 total: 17.3s remaining: 6.96s 713: learn: 0.4590411 total: 17.3s remaining: 6.93s 714: learn: 0.4590188 total: 17.3s remaining: 6.91s 715: learn: 0.4590011 total: 17.4s remaining: 6.88s 716: learn: 0.4589881 total: 17.4s remaining: 6.86s 717: learn: 0.4589662 total: 17.4s remaining: 6.83s 718: learn: 0.4589442 total: 17.4s remaining: 6.81s 719: learn: 0.4589192 total: 17.5s remaining: 6.79s 720: learn: 0.4588769 total: 17.5s remaining: 6.76s 721: learn: 0.4588527 total: 17.5s remaining: 6.74s 722: learn: 0.4588199 total: 17.5s remaining: 6.71s 723: learn: 0.4587977 total: 17.5s remaining: 6.69s 724: learn: 0.4587864 total: 17.6s remaining: 6.67s 725: learn: 0.4587536 total: 17.6s remaining: 6.64s 726: learn: 0.4587309 total: 17.6s remaining: 6.62s 727: learn: 0.4587084 total: 17.6s remaining: 6.59s 728: learn: 0.4586970 total: 17.7s remaining: 6.57s 729: learn: 0.4586717 total: 17.7s remaining: 6.54s 730: learn: 0.4586615 total: 17.7s remaining: 6.51s 731: learn: 0.4586393 total: 17.7s remaining: 6.49s 732: learn: 0.4586023 total: 17.8s remaining: 6.47s 733: learn: 0.4585754 total: 17.8s remaining: 6.44s 734: learn: 0.4585648 total: 17.8s remaining: 6.42s 735: learn: 0.4585352 total: 17.8s remaining: 6.39s 736: learn: 0.4585120 total: 17.8s remaining: 6.37s 737: learn: 0.4584882 total: 17.9s remaining: 6.35s 738: learn: 0.4584717 total: 17.9s remaining: 6.32s 739: learn: 0.4584506 total: 17.9s remaining: 6.3s 740: learn: 0.4584400 total: 17.9s remaining: 6.27s 741: learn: 0.4584231 total: 18s remaining: 6.25s 742: learn: 0.4583861 total: 18s remaining: 6.22s 743: learn: 0.4583646 total: 18s remaining: 6.2s 744: learn: 0.4583315 total: 18s remaining: 6.17s 745: learn: 0.4583167 total: 18.1s remaining: 6.15s 746: learn: 0.4582962 total: 18.1s remaining: 6.13s 747: learn: 0.4582765 total: 18.1s remaining: 6.1s 748: learn: 0.4582646 total: 18.1s remaining: 6.08s 749: learn: 0.4582608 total: 18.1s remaining: 6.05s 750: learn: 0.4582458 total: 18.2s remaining: 6.02s 751: learn: 0.4582240 total: 18.2s remaining: 6s 752: learn: 0.4582099 total: 18.2s remaining: 5.97s 753: learn: 0.4581939 total: 18.2s remaining: 5.95s 754: learn: 0.4581842 total: 18.3s remaining: 5.92s 755: learn: 0.4581604 total: 18.3s remaining: 5.9s 756: learn: 0.4581397 total: 18.3s remaining: 5.87s 757: learn: 0.4581280 total: 18.3s remaining: 5.85s 758: learn: 0.4581143 total: 18.3s remaining: 5.82s 759: learn: 0.4580891 total: 18.4s remaining: 5.8s 760: learn: 0.4580627 total: 18.4s remaining: 5.77s 761: learn: 0.4580526 total: 18.4s remaining: 5.75s 762: learn: 0.4580375 total: 18.4s remaining: 5.72s 763: learn: 0.4580195 total: 18.5s remaining: 5.7s 764: learn: 0.4580057 total: 18.5s remaining: 5.67s 765: learn: 0.4579887 total: 18.5s remaining: 5.65s 766: learn: 0.4579696 total: 18.5s remaining: 5.63s 767: learn: 0.4579509 total: 18.5s remaining: 5.6s 768: learn: 0.4579495 total: 18.6s remaining: 5.57s 769: learn: 0.4579257 total: 18.6s remaining: 5.55s 770: learn: 0.4579099 total: 18.6s remaining: 5.53s 771: learn: 0.4578826 total: 18.6s remaining: 5.5s 772: learn: 0.4578682 total: 18.6s remaining: 5.48s 773: learn: 0.4578556 total: 18.7s remaining: 5.45s 774: learn: 0.4578421 total: 18.7s remaining: 5.43s 775: learn: 0.4578141 total: 18.7s remaining: 5.4s 776: learn: 0.4577988 total: 18.7s remaining: 5.38s 777: learn: 0.4577819 total: 18.8s remaining: 5.35s 778: learn: 0.4577701 total: 18.8s remaining: 5.33s 779: learn: 0.4577531 total: 18.8s remaining: 5.3s 780: learn: 0.4577352 total: 18.8s remaining: 5.28s 781: learn: 0.4577211 total: 18.8s remaining: 5.25s 782: learn: 0.4577111 total: 18.9s remaining: 5.23s 783: learn: 0.4576968 total: 18.9s remaining: 5.21s 784: learn: 0.4576787 total: 18.9s remaining: 5.18s 785: learn: 0.4576650 total: 18.9s remaining: 5.16s 786: learn: 0.4576469 total: 19s remaining: 5.13s 787: learn: 0.4576377 total: 19s remaining: 5.11s 788: learn: 0.4576243 total: 19s remaining: 5.08s 789: learn: 0.4576076 total: 19s remaining: 5.06s 790: learn: 0.4575942 total: 19s remaining: 5.03s 791: learn: 0.4575747 total: 19.1s remaining: 5.01s 792: learn: 0.4575623 total: 19.1s remaining: 4.98s 793: learn: 0.4575533 total: 19.1s remaining: 4.96s 794: learn: 0.4575277 total: 19.1s remaining: 4.94s 795: learn: 0.4575093 total: 19.2s remaining: 4.91s 796: learn: 0.4574888 total: 19.2s remaining: 4.89s 797: learn: 0.4574696 total: 19.2s remaining: 4.86s 798: learn: 0.4574404 total: 19.2s remaining: 4.84s 799: learn: 0.4574324 total: 19.3s remaining: 4.81s 800: learn: 0.4574140 total: 19.3s remaining: 4.79s 801: learn: 0.4573900 total: 19.3s remaining: 4.77s 802: learn: 0.4573717 total: 19.3s remaining: 4.74s 803: learn: 0.4573504 total: 19.4s remaining: 4.72s 804: learn: 0.4573377 total: 19.4s remaining: 4.69s 805: learn: 0.4573195 total: 19.4s remaining: 4.67s 806: learn: 0.4573076 total: 19.4s remaining: 4.64s 807: learn: 0.4572822 total: 19.4s remaining: 4.62s 808: learn: 0.4572714 total: 19.5s remaining: 4.6s 809: learn: 0.4572412 total: 19.5s remaining: 4.57s 810: learn: 0.4572213 total: 19.5s remaining: 4.55s 811: learn: 0.4572136 total: 19.5s remaining: 4.52s 812: learn: 0.4571862 total: 19.6s remaining: 4.5s 813: learn: 0.4571635 total: 19.6s remaining: 4.47s 814: learn: 0.4571452 total: 19.6s remaining: 4.45s 815: learn: 0.4571295 total: 19.6s remaining: 4.43s 816: learn: 0.4571178 total: 19.7s remaining: 4.4s 817: learn: 0.4571013 total: 19.7s remaining: 4.38s 818: learn: 0.4570851 total: 19.7s remaining: 4.35s 819: learn: 0.4570771 total: 19.7s remaining: 4.33s 820: learn: 0.4570603 total: 19.7s remaining: 4.3s 821: learn: 0.4570368 total: 19.8s remaining: 4.28s 822: learn: 0.4570364 total: 19.8s remaining: 4.25s 823: learn: 0.4570189 total: 19.8s remaining: 4.23s 824: learn: 0.4569980 total: 19.8s remaining: 4.21s 825: learn: 0.4569802 total: 19.9s remaining: 4.18s 826: learn: 0.4569542 total: 19.9s remaining: 4.16s 827: learn: 0.4569360 total: 19.9s remaining: 4.13s 828: learn: 0.4569153 total: 19.9s remaining: 4.11s 829: learn: 0.4568951 total: 19.9s remaining: 4.09s 830: learn: 0.4568824 total: 20s remaining: 4.06s 831: learn: 0.4568659 total: 20s remaining: 4.04s 832: learn: 0.4568395 total: 20s remaining: 4.01s 833: learn: 0.4568130 total: 20s remaining: 3.99s 834: learn: 0.4567972 total: 20.1s remaining: 3.96s 835: learn: 0.4567794 total: 20.1s remaining: 3.94s 836: learn: 0.4567642 total: 20.1s remaining: 3.92s 837: learn: 0.4567562 total: 20.1s remaining: 3.89s 838: learn: 0.4567364 total: 20.2s remaining: 3.87s 839: learn: 0.4567156 total: 20.2s remaining: 3.84s 840: learn: 0.4566981 total: 20.2s remaining: 3.82s 841: learn: 0.4566868 total: 20.2s remaining: 3.79s 842: learn: 0.4566664 total: 20.2s remaining: 3.77s 843: learn: 0.4566496 total: 20.3s remaining: 3.75s 844: learn: 0.4566306 total: 20.3s remaining: 3.72s 845: learn: 0.4566150 total: 20.3s remaining: 3.7s 846: learn: 0.4565976 total: 20.3s remaining: 3.67s 847: learn: 0.4565772 total: 20.4s remaining: 3.65s 848: learn: 0.4565593 total: 20.4s remaining: 3.63s 849: learn: 0.4565394 total: 20.4s remaining: 3.6s 850: learn: 0.4565181 total: 20.4s remaining: 3.58s 851: learn: 0.4565031 total: 20.5s remaining: 3.56s 852: learn: 0.4564851 total: 20.5s remaining: 3.53s 853: learn: 0.4564641 total: 20.5s remaining: 3.51s 854: learn: 0.4564482 total: 20.5s remaining: 3.48s 855: learn: 0.4564319 total: 20.6s remaining: 3.46s 856: learn: 0.4564187 total: 20.6s remaining: 3.44s 857: learn: 0.4564000 total: 20.6s remaining: 3.41s 858: learn: 0.4563870 total: 20.6s remaining: 3.39s 859: learn: 0.4563727 total: 20.7s remaining: 3.36s 860: learn: 0.4563622 total: 20.7s remaining: 3.34s 861: learn: 0.4563425 total: 20.7s remaining: 3.31s 862: learn: 0.4563246 total: 20.7s remaining: 3.29s 863: learn: 0.4562975 total: 20.7s remaining: 3.27s 864: learn: 0.4562817 total: 20.8s remaining: 3.24s 865: learn: 0.4562610 total: 20.8s remaining: 3.22s 866: learn: 0.4562399 total: 20.8s remaining: 3.19s 867: learn: 0.4562167 total: 20.8s remaining: 3.17s 868: learn: 0.4562029 total: 20.9s remaining: 3.15s 869: learn: 0.4561811 total: 20.9s remaining: 3.12s 870: learn: 0.4561683 total: 20.9s remaining: 3.1s 871: learn: 0.4561510 total: 20.9s remaining: 3.07s 872: learn: 0.4561413 total: 20.9s remaining: 3.05s 873: learn: 0.4561190 total: 21s remaining: 3.02s 874: learn: 0.4560982 total: 21s remaining: 3s 875: learn: 0.4560580 total: 21s remaining: 2.98s 876: learn: 0.4560453 total: 21s remaining: 2.95s 877: learn: 0.4560332 total: 21.1s remaining: 2.93s 878: learn: 0.4560155 total: 21.1s remaining: 2.9s 879: learn: 0.4559969 total: 21.1s remaining: 2.88s 880: learn: 0.4559668 total: 21.1s remaining: 2.85s 881: learn: 0.4559490 total: 21.2s remaining: 2.83s 882: learn: 0.4559280 total: 21.2s remaining: 2.81s 883: learn: 0.4559081 total: 21.2s remaining: 2.78s 884: learn: 0.4558973 total: 21.2s remaining: 2.76s 885: learn: 0.4558706 total: 21.3s remaining: 2.73s 886: learn: 0.4558539 total: 21.3s remaining: 2.71s 887: learn: 0.4558312 total: 21.3s remaining: 2.69s 888: learn: 0.4558055 total: 21.3s remaining: 2.66s 889: learn: 0.4557857 total: 21.3s remaining: 2.64s 890: learn: 0.4557686 total: 21.4s remaining: 2.61s 891: learn: 0.4557392 total: 21.4s remaining: 2.59s 892: learn: 0.4557197 total: 21.4s remaining: 2.57s 893: learn: 0.4557035 total: 21.4s remaining: 2.54s 894: learn: 0.4556835 total: 21.5s remaining: 2.52s 895: learn: 0.4556707 total: 21.5s remaining: 2.49s 896: learn: 0.4556560 total: 21.5s remaining: 2.47s 897: learn: 0.4556452 total: 21.5s remaining: 2.45s 898: learn: 0.4556277 total: 21.6s remaining: 2.42s 899: learn: 0.4556122 total: 21.6s remaining: 2.4s 900: learn: 0.4555786 total: 21.6s remaining: 2.37s 901: learn: 0.4555640 total: 21.6s remaining: 2.35s 902: learn: 0.4555446 total: 21.7s remaining: 2.33s 903: learn: 0.4555261 total: 21.7s remaining: 2.3s 904: learn: 0.4555105 total: 21.7s remaining: 2.28s 905: learn: 0.4554856 total: 21.7s remaining: 2.25s 906: learn: 0.4554663 total: 21.7s remaining: 2.23s 907: learn: 0.4554458 total: 21.8s remaining: 2.21s 908: learn: 0.4554283 total: 21.8s remaining: 2.18s 909: learn: 0.4554108 total: 21.8s remaining: 2.16s 910: learn: 0.4553899 total: 21.8s remaining: 2.13s 911: learn: 0.4553896 total: 21.9s remaining: 2.11s 912: learn: 0.4553777 total: 21.9s remaining: 2.08s 913: learn: 0.4553589 total: 21.9s remaining: 2.06s 914: learn: 0.4553303 total: 21.9s remaining: 2.04s 915: learn: 0.4553188 total: 21.9s remaining: 2.01s 916: learn: 0.4553021 total: 22s remaining: 1.99s 917: learn: 0.4552775 total: 22s remaining: 1.96s 918: learn: 0.4552582 total: 22s remaining: 1.94s 919: learn: 0.4552456 total: 22s remaining: 1.92s 920: learn: 0.4552353 total: 22.1s remaining: 1.89s 921: learn: 0.4552037 total: 22.1s remaining: 1.87s 922: learn: 0.4551853 total: 22.1s remaining: 1.84s 923: learn: 0.4551758 total: 22.1s remaining: 1.82s 924: learn: 0.4551646 total: 22.2s remaining: 1.8s 925: learn: 0.4551315 total: 22.2s remaining: 1.77s 926: learn: 0.4551300 total: 22.2s remaining: 1.75s 927: learn: 0.4551201 total: 22.2s remaining: 1.72s 928: learn: 0.4551077 total: 22.2s remaining: 1.7s 929: learn: 0.4550858 total: 22.3s remaining: 1.68s 930: learn: 0.4550616 total: 22.3s remaining: 1.65s 931: learn: 0.4550351 total: 22.3s remaining: 1.63s 932: learn: 0.4550163 total: 22.3s remaining: 1.6s 933: learn: 0.4550073 total: 22.4s remaining: 1.58s 934: learn: 0.4549719 total: 22.4s remaining: 1.55s 935: learn: 0.4549516 total: 22.4s remaining: 1.53s 936: learn: 0.4549308 total: 22.4s remaining: 1.51s 937: learn: 0.4549130 total: 22.4s remaining: 1.48s 938: learn: 0.4548999 total: 22.5s remaining: 1.46s 939: learn: 0.4548913 total: 22.5s remaining: 1.44s 940: learn: 0.4548726 total: 22.5s remaining: 1.41s 941: learn: 0.4548671 total: 22.5s remaining: 1.39s 942: learn: 0.4548387 total: 22.6s remaining: 1.36s 943: learn: 0.4548268 total: 22.6s remaining: 1.34s 944: learn: 0.4547866 total: 22.6s remaining: 1.31s 945: learn: 0.4547752 total: 22.6s remaining: 1.29s 946: learn: 0.4547542 total: 22.7s remaining: 1.27s 947: learn: 0.4547439 total: 22.7s remaining: 1.24s 948: learn: 0.4547325 total: 22.7s remaining: 1.22s 949: learn: 0.4547208 total: 22.7s remaining: 1.2s 950: learn: 0.4547003 total: 22.7s remaining: 1.17s 951: learn: 0.4546867 total: 22.8s remaining: 1.15s 952: learn: 0.4546711 total: 22.8s remaining: 1.12s 953: learn: 0.4546565 total: 22.8s remaining: 1.1s 954: learn: 0.4546364 total: 22.8s remaining: 1.07s 955: learn: 0.4546195 total: 22.9s remaining: 1.05s 956: learn: 0.4546043 total: 22.9s remaining: 1.03s 957: learn: 0.4545783 total: 22.9s remaining: 1s 958: learn: 0.4545592 total: 22.9s remaining: 980ms 959: learn: 0.4545341 total: 22.9s remaining: 956ms 960: learn: 0.4545170 total: 23s remaining: 932ms 961: learn: 0.4544988 total: 23s remaining: 908ms 962: learn: 0.4544688 total: 23s remaining: 884ms 963: learn: 0.4544563 total: 23s remaining: 860ms 964: learn: 0.4544383 total: 23.1s remaining: 836ms 965: learn: 0.4544221 total: 23.1s remaining: 813ms 966: learn: 0.4544077 total: 23.1s remaining: 789ms 967: learn: 0.4543857 total: 23.1s remaining: 765ms 968: learn: 0.4543678 total: 23.2s remaining: 741ms 969: learn: 0.4543516 total: 23.2s remaining: 717ms 970: learn: 0.4543406 total: 23.2s remaining: 693ms 971: learn: 0.4543111 total: 23.2s remaining: 669ms 972: learn: 0.4542974 total: 23.2s remaining: 645ms 973: learn: 0.4542779 total: 23.3s remaining: 621ms 974: learn: 0.4542674 total: 23.3s remaining: 597ms 975: learn: 0.4542575 total: 23.3s remaining: 573ms 976: learn: 0.4542426 total: 23.3s remaining: 549ms 977: learn: 0.4542294 total: 23.4s remaining: 526ms 978: learn: 0.4542146 total: 23.4s remaining: 502ms 979: learn: 0.4541863 total: 23.4s remaining: 478ms 980: learn: 0.4541696 total: 23.4s remaining: 454ms 981: learn: 0.4541501 total: 23.5s remaining: 430ms 982: learn: 0.4541334 total: 23.5s remaining: 406ms 983: learn: 0.4541171 total: 23.5s remaining: 382ms 984: learn: 0.4540999 total: 23.5s remaining: 358ms 985: learn: 0.4540737 total: 23.5s remaining: 334ms 986: learn: 0.4540537 total: 23.6s remaining: 310ms 987: learn: 0.4540358 total: 23.6s remaining: 287ms 988: learn: 0.4539991 total: 23.6s remaining: 263ms 989: learn: 0.4539805 total: 23.6s remaining: 239ms 990: learn: 0.4539649 total: 23.7s remaining: 215ms 991: learn: 0.4539472 total: 23.7s remaining: 191ms 992: learn: 0.4539314 total: 23.7s remaining: 167ms 993: learn: 0.4539172 total: 23.7s remaining: 143ms 994: learn: 0.4539031 total: 23.7s remaining: 119ms 995: learn: 0.4538919 total: 23.8s remaining: 95.5ms 996: learn: 0.4538690 total: 23.8s remaining: 71.6ms 997: learn: 0.4538524 total: 23.8s remaining: 47.7ms 998: learn: 0.4538285 total: 23.8s remaining: 23.9ms 999: learn: 0.4538154 total: 23.9s remaining: 0us model_name: CatBoostClassifier Confusion Matrix [[13973 3262] [ 3145 9620]] Model AUC: 0.858, Model Accuracy: 0.786 CPU times: user 1min 13s, sys: 8.63 s, total: 1min 22s Wall time: 24.8 s 1234567891011121314import numpy as npfrom datetime import datetimeversion = datetime.now().strftime(&quot;%d-%m-%Y %H-%M-%S&quot;)def final_submission(model, data, version): final_preds = model.predict(data) binarizer = np.vectorize(lambda x: 1 if x &gt;= .5 else 0) prediction_binarized = binarizer(final_preds) submission = pd.concat([sample_submission,pd.DataFrame(prediction_binarized)], axis=1).drop(columns=[&#x27;Survived&#x27;]) submission.columns = [&#x27;PassengerId&#x27;, &#x27;Survived&#x27;] submission.to_csv(&#x27;Sklearn of Submission.csv&#x27;.format(version), index=False) final_submission(cb_model, test_data, version) 1","categories":[],"tags":[]},{"title":"[ML]텍스트 마아닝","slug":"Python/textmining","date":"2021-04-19T15:00:00.000Z","updated":"2021-04-26T08:13:14.037Z","comments":true,"path":"2021/04/20/Python/textmining/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/20/Python/textmining/","excerpt":"","text":"[ML] 텍스트 마이닝(Text Mining)이란?1. 개념 텍스트 마이닝은 비정형 및 반 정형 데이터에 대하여 자연어 처리 기술과 문서 처리 기술을 적용하여 유용한 정보를 추출, 가공하는 목적으로 하는 기술이다. 대부분의 텍스트 데이터베이스에 저장된 데이터는 반구조적 데이터이다. 이때, 반구조적 데이터란? 완전히 구조적이지도 않고 완전히 비구조적이지도 않은 데이터를 의미 2. 데이터 마이닝과 텍스트 마이닝의 비교!()[/hueman_images/python/ml/tm1.png] 3. 텍스트 마이닝의 문제점 자연어에 영향을 많이 받는다. 분석 결과물 자체로 어떤 성과를 보기가 어렵다. 4. 텍스트 마이닝이 요구하는 기법 텍스트 마아닝은 “데이터 마이닝 기법” 이외에도 자연어 처리기술, 문서처리 기술을 추가로 요구한다. 5. 불용어의 정의 분석하는데 있어서 크게 의미가 없는 단어들, 예를 들면 조사, 접미사 등이 있다. References https://iamdaisy.tistory.com/29 https://wikidocs.net/book/2155","categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://eunjin-jun717.github.io/tags/ML/"}]},{"title":"[ML] Catboost","slug":"Python/catboost","date":"2021-04-14T15:00:00.000Z","updated":"2021-04-16T00:05:29.944Z","comments":true,"path":"2021/04/15/Python/catboost/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/15/Python/catboost/","excerpt":"","text":"[Machine Learning] CatboostCatboost의 개념 Catboost란 Gradient boosting를 기반으로한 부스팅이다. 간단하게 Gradient boosting을 설명하자면, 첫번째 정답의 오차에 대해 학습시키고 또 두번째 정답의 오차에 대해 학습시키면서 점진적으로 weaker learner을 strong learner로 만드는 방법이다. Catboost의 특징1) Level-wise tree 는 Best First Tree 같이 트리를 만들어나가는 형태 XGB, LightBGM과 다르게 대칭 트리의 구조를 사용한다. 이렇게 균형잡힌 depth-wise trees의 장점은 오버피팅을 방지할 수 있음(그러나 단점으로는 leaf wise보다는 느림) Root node에서 leaf node까지 만들어진 조건들을 binary된 백터로 변환하여 각 index에 값을 저장함으로써 메모리에 저장할 필요가 없기 때문에 효율적인 테스트 가능! 2) 기존의 부스팅 모델인 Gradient boosting같은 경우에는 모든 훈련 데이터를 대상으로 잔차 계산을 했다면, Catboost는 일부만가지고 잔차를 계산한 뒤, 이것을 모델로 만든다. X1의 잔차만 계산하고, 이를 기반으로 모델을 만든다. 이 모델을 이용하여 x2의 잔차를 예측한다. X1, x2 의 잔차를 가지고 모델을 만들고, 이를 기반으로 x3,x4의 잔차를 예측한다. 반복한다. 이를 Ordered Boosting이라 한다. 3) Random Permutation Ordered 부스팅할 때, 순서를 섞어 주지않으면 매번 같은 순서대로 잔차를 예측하는 모델을 만들 가능성이 있음 따라서 Catboost에서는 데이터를 셔플링해서 뽑아내야한다. 4) Ordered Target Encoding 범주형 변수를 수로 인코딩 시키는 방법 Time과 feature1을 가지고 class_labels를 예측할 것이다. Mean encoding을 할 것인데 , 우리가 예측해야하는 값이 훈련 셋 피쳐에 들어가는 문제(Data Leakage)를 일으키지 않는 해결책으로 아래처럼 인코딩 할 것이다. 예를 들면, Friday에 cloudy는 (15+14)/2 =15.5로 인코딩 Saturday에 cloudy는 (15+14+20)/3=16.3 으로 인코딩 즉, 현재 데이터의 target값을 사용하지 않고, 이전 데이터들의 타겟값만 사용하지 data leakage를 일으키지 않는 것! 5) Categorical Feature Combinations Country와 hair color로 class label을 예측하려는데, 위와 같이 두 feature를 하나의 feature로 묶을 수 있는 경우가 생긴다. catboost에서는 하나로 묶어버린다. 6) One-hot Encoding Caldinality가 3이하인 범주형 변수들은 Target encoding이 아닌 one-hot encoding으로된다. Target 인코딩보다 더 효율적이다. 3. Catboost의 한계 데이터 대부분이 수치형 변수일 경우에는 Light GBM보다 학습 속도가 느리다. 따라서 범주형 변수에 더 적합하다 4. Hyper Parameter Learning_rate Random_strength L2_regulariser Catboost는 기본 파라미터로도 충분히 최적화가 잘 되어있기 때문에, 굳이 다른 파라미터를 더 사용하지 않아도 된다. References https://dailyheumsi.tistory.com/136 https://hanishrohit.medium.com/whats-so-special-about-catboost-335d64d754ae","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[ML] Gradient Boosting","slug":"Python/ml-gradient","date":"2021-04-14T15:00:00.000Z","updated":"2021-04-16T00:05:29.946Z","comments":true,"path":"2021/04/15/Python/ml-gradient/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/15/Python/ml-gradient/","excerpt":"","text":"[Machine Learning] Gradient Boosting Catboost, XGBoost, LightGBM의 기반이 되는 부스팅이다.1. 과정1) 최초, 평균 값으로 예측 Height, favorite color, gender로 weight을 ‘일반적’으로 예측하는 방법은, 먼저 weight의 평균값으로 예측하는 것이다. (avg weight = 71.2) 2) 예측 값과 실제 값의 오차 구하기 Residual(오차)들을 계산해보자. 당연히 클 수밖에 없다. 3) 오차 값을 예측하는 Tree 만들기 위에서 계산한 오차를, dataset의 feature들(height, favorite color, gender)을 가지고 트리를 만들어 예측한다. Leaf node에 분류되 값들이 여러 개 있는 경우에는 평균을 내서 하나의 값으로 만들자 4) Learning rate를 적용하여, 기존 예측 값 업데이트하기 오차 값(16.8)에 원래의 예측값(71.2)을 더해주어 업데이트한다. 그러나 이때 값은 88이다. 이 값은 원래의 weight와 동일하다. 그냥 더해주기만 하면 기존 dataset에는 완벽히 fitted하지만, new dataset에는 안맞을 가능성이 높다 따라서 그냥 더하지 않고 learning rate만큼 더해준다. 여기서는 learning rate를 0.1로 잡아준다. 0.1을 곱해서 더해주니 예측값은 72.9가 나왔다. 처음 예측값(71.2) -&gt; 현재 예측값(72.9) 원래의 weight는 88이니 이전보다 오차가 줄어들었다. 또 다시 원래의 데이터 값에 현재 예측값을 뺀 오차에 대한 트리를 만들며 일정 loop를 반복하다보면 점점 원래의 값인 88에 가까워 진다. 즉, High Variance를 피하면서 점진적으로 학습시키는 것이다. 5) 전체적인 구조 References https://www.youtube.com/watch?v=2xudPOBz-vs https://dailyheumsi.tistory.com/116?category=815369","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[ML] Decision Tree","slug":"Python/ml-dtree","date":"2021-04-13T15:00:00.000Z","updated":"2021-04-16T00:05:29.945Z","comments":true,"path":"2021/04/14/Python/ml-dtree/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/14/Python/ml-dtree/","excerpt":"","text":"[Machine Learning] Decision Tree1. Decision Tree(결정트리) 개념 분류와 회귀 모두 가능한 학습 지도 모델 스무고개 하듯이 예/아니오 질문을 이어가며 학습 이렇게 특정 기준(질문)을 따라 데이터를 구분하는 모델 첫 질문: Root node / 맨 마지막 노드: Terminal node(Leaf node) 데이터를 지나치게 나누다 보면 과대적합(Overfitting) 됨 여기서 과대적합이란, 모든 데이터 셋이 모두 잘 분류되어 불순도가 0이 될 때 까지 분기해 나간다. 또한, Root에서부터 하위 노드가 많이 만들어질수록 모델이 복잡해져 과대적합이 발생할 수 있다. 따라서, 결정트리에 하이퍼파라미터를 주고 모델링하기 그렇다면 오버피팅 막는 전략은 무엇이 있을까? 1) 가지치기(Pruning): 가지치기란? 하부트리를 제거하여 일반화 성능을 높이는 것. 첫번째, 속성을 제한 한다. Max_depth를 통해 트리의 깊이를 제한한다. 두번째, 한 노드에 들어있는 최소 데이터 수를 정한다. Min_samples_split을 조정하면 된다. 왼쪽은 default, 오른쪽은 min_samples_split=4라는 규제를 주어 훈련 시킴. 리프 노드의 최소 샘플이 4개 이상이니, 과대적합을 피하였고 좀 더 일반화가 가능해진 것을 확인 할 수 있다. 2) 그리드서치(Grid Search): 격자탐색이라 하며, 모델 하이퍼 파라미터에 넣을 수 있는 값들을 순차적으로 입력한 뒤에 가장 높은 성능을 보이는 하이퍼 파라미터들을 찾는 탐색 방법이다. 즉, 쉽게 말하면 세부적인 규율인 하이퍼 파라미터를 일일히 다 적용해가면서 어떤 규율이 모델에 적합한지 판단하는 것! EX) grid_dtree = GridSearchCV(dtree, param_grid= parameters, cv=3, refit=True) parameters에는 하이퍼 파라미터 정보담기 딕셔너리 cv는 cross validation 교차 검증이다. 이 함수를 수행한 뒤 이후에 grid_dtree.best_estimator을 활용하면, 학습과정에서 하이퍼 파라미터 조정으로 가장 좋은 성능을 보인 모델이 반환된다. 3) 랜덤 서치(Random Search): 그리드서치는 말그대로 모든 경우를 테이블로 만든 뒤 격자로 탐색하는 방식이지만 랜덤 서치는 하이퍼 파라미터 값을 랜덤하게 넣어보고 그 중 우수한 값을 모인 하이퍼 파라미터를 활용해 모델을 생성하는 것! 2. 하이퍼 파라미터(Hyper Parameter) Max_depth: 최대 깊이 값이 클수록 모델의 복잡도는 증가 Min_samples_split: 분할되기 위해 노드가 가져야 하는 최소 샘플 수 (default: 2) Min_samples_leaf: 리프 노드가 가지고 있어야 할 최소 샘플 수(default: 1) Max_leaf_nodes: 리프 노드의 최대 수 Min_samples_leaf: 가지를 칠 최소 sample 수 Ex) min_samples_leaf=4 이면, 4보다 자식노드 개수가 작아지면 더 이상 분류하지 않고 끝낸다. 3. Criterion(분할 품질을 측정하는 기능)1) 불순도- Gini 계수(결정트리의 기본 criterion) 우선, 불순도가 무엇인지 간단하게 설명해보자. 위쪽: 순도가 높다 아래쪽: 불순도가 높음, 즉 순도가 낮다. 불순도 최대: 한 범주에 두 데이터가 딱 반반 그렇다면 불순도는 어떻게 구해지는가? 아래의 식을 Gini라고 한다. 6장의 예제를 보면서 간단하게 설명해보자. gini = 1- (i번째 노드에 들어있는 k의 비율) Versicolor의 Gini(불순도)를 위의 gini 식에 대입하면 아래와 같다. 1- ((0/54)^2 +(49/54)^2 +(5/54)^2 ) = 0.168 Gini 계수의 값이 0에 가까울수록 좋다. 2) 불순도- 엔트로피(Entropy) (Pi는 한 영역에 존재하는 데이터 가운데 범주 i에 속하는 데이터 비율) 위의 그림을 보며 엔트로피를 설명해보자. 전체 16개 중 빨간색(범주=1)는 10개, 파란색(범주=0)은 6개이다. 이때의 엔트로피는 다음과 같다. 주황색 영역의 엔트로피는 0.95이다. 그렇다면, 그림에서 빨간 선을 그은 뒤 두개의 영역으로 나눴을 때의 엔트로피는? 엔트로피가 0.95에서 0.75로 내려갔다. 즉, 불확실성 감소 -&gt; 순도 증가 -&gt; 정보 획득 이런식으로 엔트로피가 낮아지는 방향으로 분할하면 된다. 3) 정보 획득량(information gain) 정보획득량이란? 엔트로피(불확실성)의 감소량 각 특성들이 훈련 예제들을 얼만큼 잘 분류할 수 있는가를 측정 트리 구축과정에서 테스트할 후보 특성의 순서를 결정할 때 사용 Information gain = (분류 전) 처음 엔트로피 – (분류 후) 나중 엔트로피 4. DecisionTreeRegressor결정트리 문제는 회귀 문제에도 사용 가능하다. DecisionTreeClassifer 와 비슷하지만 DecisionTreeRegressor은 클래스를 예측하는것이 아니라 어떤 값을 예측한다. 예를 들면, x1=0.6인 샘플의 target값을 예측한다 하자 루트 노드부터 순회하면 결국 value=0.111인 리프 노드에 도달한다. 이 리프노드에 있는 110개 훈련 샘플의 평균 target값이 예측된다. 이 예측값을 사용해 110개 sample에 대한 MSE를 계산하면 0.015가 나온다. 즉, 각 영역의 예측값 = 그 영역에 있는 target 값의 평균 알고리즘은 예측값과 가능한 많은 샘플이 가까이 있도록 영역을 분할한다. References https://wikidocs.net/43627 https://ysyblog.tistory.com/76 https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html https://huidea.tistory.com/32 핸즈온 머신러닝","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Python] Series & Dataframe","slug":"Python/series_df","date":"2021-04-07T15:00:00.000Z","updated":"2021-04-26T08:13:14.034Z","comments":true,"path":"2021/04/08/Python/series_df/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/08/Python/series_df/","excerpt":"","text":"[Python] Series &amp; Dataframe Pandas의 데이터 구조: Series, Dataframe, Panel 3가지가 있다. 1. Series pandas의 1차원 자료구조 1234import pandas as pdser= pd.Series([10,20,30,40],index=list(&#x27;abcd&#x27;)) # 10,20,30,40의 리스트를 만드는데 이때의 index는 a,b,c,d로 지정ser a 10 b 20 c 30 d 40 dtype: int64 12print(ser[3],ser[2],ser[1],ser[0])print(ser[&#x27;d&#x27;],ser[&#x27;c&#x27;],ser[&#x27;b&#x27;],ser[&#x27;a&#x27;]) 40 30 20 10 40 30 20 10 1print(ser[&#x27;a&#x27;:&#x27;d&#x27;]) #index 문자로 slicing 하면 마지막까지 포함됨 a 10 b 20 c 30 d 40 dtype: int64 1print(ser[0:3]) #index 숫자로 slicing 했을 때는 마지막이 포함되지 않음 a 10 b 20 c 30 dtype: int64 2. Dataframe pandas의 2차원 자료구조 행과 열 1234567data=&#123;&#x27;name&#x27;:[&#x27;bella&#x27;,&#x27;charlie&#x27;], &#x27;age&#x27;:[25,27], &#x27;birthyear&#x27;:[97,95] &#125;df1 = pd.DataFrame(data, index=list(&#x27;bc&#x27;))print(df1)print(df1.info()) name age birthyear b bella 25 97 c charlie 27 95 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 2 entries, b to c Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 name 2 non-null object 1 age 2 non-null int64 2 birthyear 2 non-null int64 dtypes: int64(2), object(1) memory usage: 64.0+ bytes None 1print(df1.dtypes) # column의 datatype을 반환 name object age int64 birthyear int64 dtype: object 12print(df1.T) # dataframe을 치환함, 행과 열의 데이터를 치환한다. # 2*3 -&gt; 3*2 b c name bella charlie age 25 27 birthyear 97 95 1print(df1.values) # 값들을 반환 [[&#39;bella&#39; 25 97] [&#39;charlie&#39; 27 95]] 1print(df1.index) # index를 반환 Index([&#39;b&#39;, &#39;c&#39;], dtype=&#39;object&#39;) 1print(df1.columns) # column 명을 반환 Index([&#39;name&#39;, &#39;age&#39;, &#39;birthyear&#39;], dtype=&#39;object&#39;) 1234567data2=&#123;&#x27;color&#x27;:[&#x27;red&#x27;,&#x27;orange&#x27;,&#x27;yellow&#x27;,&#x27;green&#x27;,&#x27;blue&#x27;], &#x27;flower&#x27;:[&#x27;rose&#x27;,&#x27;daisy&#x27;,&#x27;lotus&#x27;,&#x27;tulips&#x27;,&#x27;calendula&#x27;], &#x27;season&#x27;:[&#x27;spring&#x27;,&#x27;spring&#x27;,&#x27;winter&#x27;,&#x27;summer&#x27;,&#x27;fall&#x27;], &#x27;month&#x27;:[10,5,6,7,11] &#125;df2=pd.DataFrame(data2,index=list(&#x27;abcde&#x27;))print(df2) color flower season month a red rose spring 10 b orange daisy spring 5 c yellow lotus winter 6 d green tulips summer 7 e blue calendula fall 11 1df2.head(3) # 상위데이터 3개 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower season month a red rose spring 10 b orange daisy spring 5 c yellow lotus winter 6 1df2.tail(2) # 하위데이터 2개 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower season month d green tulips summer 7 e blue calendula fall 11 1df2.sort_index(axis=0, ascending=False) #axis=0일 경우, index 이름 기준 정렬, 내림차순 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower season month e blue calendula fall 11 d green tulips summer 7 c yellow lotus winter 6 b orange daisy spring 5 a red rose spring 10 1df2.sort_index(axis=1) #axis=1일 경우, index의 column 이름 기준 정렬, 오름차순 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower month season a red rose 10 spring b orange daisy 5 spring c yellow lotus 6 winter d green tulips 7 summer e blue calendula 11 fall 1df2.sort_values(&#x27;color&#x27;) # index명이나 column명 입력하면 키값이 정렬됨 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; color flower season month e blue calendula fall 11 d green tulips summer 7 b orange daisy spring 5 a red rose spring 10 c yellow lotus winter 6 참조 https://harryp.tistory.com/868","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Python]시각화 코드 실습","slug":"Python/Visualization_Python/spines_grids","date":"2021-04-06T15:00:00.000Z","updated":"2021-04-26T08:13:14.032Z","comments":true,"path":"2021/04/07/Python/Visualization_Python/spines_grids/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/07/Python/Visualization_Python/spines_grids/","excerpt":"","text":"블로그 실습 따라하기 Spines&amp;Grids practice 시각화 코드를 함수로 만들기1234567891011121314151617181920212223def plot_example(ax, zorder=0): # zorder(레이어의 위치) ax.bar(tips_day[&quot;day&quot;],tips_day[&quot;tip&quot;],color=&quot;lightgray&quot;, zorder=zorder) #x축: day, y축: tip ax.set_title(&quot;tip (mean)&quot;, fontsize=16, pad=12) # values h_pad=0.1 for i in range(len(ax.patches)): # axes에 들어오는 객체: 막대기 4개 즉, ax.patches 길이는 4 fontweight=&quot;normal&quot; #폰트 굵기 color=&quot;k&quot; # 폰트 색깔 if i == 3: # 일요일 fontweight=&quot;bold&quot; color=&quot;darkred&quot; ax.text(i, tips_day[&quot;tip&quot;].loc[i]+h_pad, f&quot;&#123;tips_day[&#x27;tip&#x27;].loc[i]:0.2f&#125;&quot;, # loc[i]: 요일별높이 설정해줌, f&quot;: 글자 출력 horizontalalignment=&#x27;center&#x27;,fontsize=12, fontweight=fontweight, color=color) #Sunday # 4번째가 sunday ax.patches[3].set_facecolor(&quot;darkred&quot;) # bar채우기색 ax.patches[3].set_edgecolor(&quot;black&quot;) # bar edge 색 # set Range ax.set_ylim(0,4) return ax seaborn의 tips 데이터 가져오기 12345import matplotlib.pyplot as pltimport seaborn as snstips = sns.load_dataset(&quot;tips&quot;)tips.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 12tips_day=tips.groupby(&quot;day&quot;).mean().reset_index() # 요일별 평균 데이터 tips_day .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; day total_bill tip size 0 Thur 17.682742 2.771452 2.451613 1 Fri 17.151579 2.734737 2.105263 2 Sat 20.441379 2.993103 2.517241 3 Sun 21.410000 3.255132 2.842105 12fig, ax =plt.subplots() # figure, axes 객체 생성ax=plot_example(ax) 1type(ax.spines) #dictionary의 일종 collections.OrderedDict 12for k,v in ax.spines.items(): print(f&quot;spines[&#123;k&#125;]=&#123;v&#125;&quot;) spines[left]=Spine spines[right]=Spine spines[bottom]=Spine spines[top]=Spine 1ax.spines.values() # spine은 patch의 subclass odict_values([&lt;matplotlib.spines.Spine object at 0x7f0fd2d9f890&gt;, &lt;matplotlib.spines.Spine object at 0x7f0fd2d9f050&gt;, &lt;matplotlib.spines.Spine object at 0x7f0fd2d9f1d0&gt;, &lt;matplotlib.spines.Spine object at 0x7f0fd2d9fcd0&gt;]) 1234567fig, ax=plt.subplots()ax = plot_example(ax)# set_visible(False)를 함으로써 bottom 축만 남기고 다른 축들은 숨김ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False) 1234567fig,ax = plt.subplots()ax = plot_example(ax)# spince의 일부 영역만 보고싶을 때ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_bounds(1,3) # set_bounds로 최소, 최대 영역 정해줌 1ax.spines[&#x27;left&#x27;].get_position() # left spine이 밖으로 0만큼 나가있음 (&#39;outward&#39;, 0.0) 123456fig, ax = plt.subplots()ax = plot_example(ax)ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_position((&#x27;outward&#x27;,10)) # left spine을 바깥쪽으로 10만큼 움직임 그래프 3개 한꺼번에 그리기123456789101112131415fig, ax = plt.subplots(ncols=3, figsize=(15,3))for i in range(3): #3개의 그래프 ax[i]=plot_example(ax[i]) ax[i].spines[&#x27;top&#x27;].set_visible(False) ax[i].spines[&#x27;right&#x27;].set_visible(False)# ax[0]: spine을 data영역에서 -50만큼 이동ax[0].spines[&#x27;left&#x27;].set_position((&quot;outward&quot;,-50))# ax[1]: spine을 axes의 0.3의 위치에 설정ax[1].spines[&#x27;left&#x27;].set_position((&#x27;axes&#x27;,0.3))# ax[2]: spine을 data의 2.5의 위치에 설정ax[2].spines[&#x27;left&#x27;].set_position((&#x27;data&#x27;,2.5)) sin 그래프 그리기123456789101112131415161718192021222324252627282930313233import numpy as np## datax= np.linspace(-np.pi, np.pi,100)y=2*np.sin(x)fig, ax = plt.subplots(ncols=3, figsize=(12,4)) # 그래프 3개 만듦## normal plotax[0].plot(x,y)ax[0].set_title(&quot;normal plot&quot;, pad=12)## textbook(1)ax[1].plot(x,y)ax[1].set_title(&quot;textbook (1)&quot;, pad=12)ax[1].spines[&#x27;left&#x27;].set_visible(False)ax[1].spines[&#x27;top&#x27;].set_visible(False)ax[1].spines[&#x27;right&#x27;].set_position((&#x27;data&#x27;,0)) # (0,0) 지나가게 함ax[1].spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;,0))## textbook(2)ax[2].plot(x,y)ax[2].set_title(&quot;textbook (2)&quot;, pad=12)ax[2].spines[&#x27;left&#x27;].set_visible(False)ax[2].spines[&#x27;top&#x27;].set_visible(False)ax[2].spines[&#x27;right&#x27;].set_position((&#x27;data&#x27;,0))ax[2].spines[&#x27;bottom&#x27;].set_position((&#x27;data&#x27;,0))ax[2].plot(1,0,&#x27;&gt;k&#x27;,transform=ax[2].get_yaxis_transform(),clip_on=False) # &gt;k 화살표 모양# get_yaxis_transform이면 x축 기준으로 1만큼 y기준 움직이지 않음ax[2].plot(0,1,&#x27;^k&#x27;,transform=ax[2].get_xaxis_transform(),clip_on=False) # ^k 위쪽 화살표 # get_xaxis_transform이면 y축 기준으로 x는 움직이지 않고 y는 1만큼 움직임plt.show() set_position((‘axes’,0.5)) = set_position(‘center’) set_position((‘data’,0)) = set_position(‘zero’) 12345678910111213141516fig, ax = plt.subplots(ncols=3, figsize=(15,3))for i in range(3): ax[i]=plot_example(ax[i]) ax[i].spines[&#x27;top&#x27;].set_visible(False) ax[i].spines[&#x27;right&#x27;].set_visible(False) ax[i].spines[&#x27;left&#x27;].set_position((&#x27;outward&#x27;,10))# ax[0]: x,y 둘다ax[0].grid(axis=&#x27;both&#x27;)# ax[1]: x축만ax[1].grid(axis=&#x27;x&#x27;)# ax[2]: y축만ax[2].grid(axis=&#x27;y&#x27;) 12345678910111213141516# major, minor tick 설정from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, StrMethodFormatter)fig, ax = plt.subplots()ax=plot_example(ax)ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_visible(False)# y축 tick 설정ax.yaxis.set_major_locator(MultipleLocator(1)) # y축 major tick을 1단위로 설정ax.yaxis.set_major_formatter(&#x27;&#123;x:0.5f&#125;&#x27;)ax.yaxis.set_minor_locator(MultipleLocator(0.5)) # 0.5마다 minor tick 그림plt.plot() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-180-84d5866b2f60&gt; in &lt;module&gt;() 11 12 ax.yaxis.set_major_locator(MultipleLocator(1)) # y축 major tick을 1단위로 설정 ---&gt; 13 ax.yaxis.set_major_formatter(&#39;&#123;x:0.5f&#125;&#39;) 14 ax.yaxis.set_minor_locator(MultipleLocator(0.5)) # 0.5마다 minor tick 그림 15 /usr/local/lib/python3.7/dist-packages/matplotlib/axis.py in set_major_formatter(self, formatter) 1626 formatter : `~matplotlib.ticker.Formatter` 1627 &quot;&quot;&quot; -&gt; 1628 cbook._check_isinstance(mticker.Formatter, formatter=formatter) 1629 self.isDefault_majfmt = False 1630 self.major.formatter = formatter /usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py in _check_isinstance(_types, **kwargs) 2126 &quot;, &quot;.join(names[:-1]) + &quot; or &quot; + names[-1] 2127 if len(names) &gt; 1 else names[0], -&gt; 2128 type_name(type(v)))) 2129 2130 TypeError: &#39;formatter&#39; must be an instance of matplotlib.ticker.Formatter, not a str 오류 상황: ax.yaxis.set_major_formatter(‘{x:0.5f}’) 에서 Type Error가 발생하였다. TypeError: ‘formatter’ must be an instance of matplotlib.ticker.Formatter, not a str 해결 방법: string을 입력했을 때, 내부적으로 StrMethodFormatter이 autogenerate되서 str로 교체된다.그러나 Error를 보면 자동으로 동작하지 않았기 때문에 Type Error가 뜬 것으로 보인다.따라서, StrMethodFormatter 의 함수를 입력하면 된다.from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, StrMethodFormatter) 처럼 StrMethodFormatter를 추가한 뒤,*ax.yaxis.set_major_formatter(StrMethodFormatter(‘{x:0.2f}’))*위의 코드로 변경하면 된다 . 12345678910111213141516# major, minor tick 설정from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, StrMethodFormatter)fig, ax = plt.subplots()ax=plot_example(ax)ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_visible(False)# y축 tick 설정ax.yaxis.set_major_locator(MultipleLocator(1)) # y축 major tick을 1단위로 설정ax.yaxis.set_major_formatter(StrMethodFormatter(&#x27;&#123;x:0.2f&#125;&#x27;)) # y축 단위 소수점 2자리까지 보임ax.yaxis.set_minor_locator(MultipleLocator(0.5)) # 0.5마다 minor tick 그림plt.plot() [] 1234567891011121314151617181920212223fig, ax = plt.subplots(ncols=3, figsize=(15,3))for i in range(3): ax[i]= plot_example(ax[i], zorder=2) # bar를 grid 앞으로 위치시킴 ax[i].spines[&#x27;top&#x27;].set_visible(False) ax[i].spines[&#x27;right&#x27;].set_visible(False) ax[i].spines[&#x27;left&#x27;].set_position((&#x27;outward&#x27;,10)) ax[i].yaxis.set_major_locator(MultipleLocator(1)) ax[i].yaxis.set_major_formatter(StrMethodFormatter(&#x27;&#123;x:0.2f&#125;&#x27;)) ax[i].yaxis.set_minor_locator(MultipleLocator(0.5))# ax[0]: major, minor 둘다ax[0].grid(axis=&#x27;y&#x27;,which=&#x27;both&#x27;)# ax[1]: major 만ax[1].grid(axis=&#x27;y&#x27;, which=&#x27;major&#x27;)# ax[2]: major만 &amp; optionax[2].grid(axis=&#x27;y&#x27;, which=&#x27;major&#x27;, color=&#x27;r&#x27;, lw=0.5, alpha=0.5)plt.show() 12345678910111213fig, ax = plt.subplots()ax=plot_example(ax, zorder=2)ax.spines[&#x27;top&#x27;].set_visible(False)ax.spines[&#x27;right&#x27;].set_visible(False)ax.spines[&#x27;left&#x27;].set_visible(False)ax.yaxis.set_major_locator(MultipleLocator(1))ax.yaxis.set_major_formatter(StrMethodFormatter(&#x27;&#123;x:0.2f&#125;&#x27;))ax.yaxis.set_minor_locator(MultipleLocator(0.5))ax.grid(axis=&#x27;y&#x27;,which=&#x27;major&#x27;, color=&#x27;lightgray&#x27;)ax.grid(axis=&#x27;y&#x27;, which=&#x27;minor&#x27;, ls=&#x27;:&#x27;) # 점선으로 minor 부분 그리기 출처 Jehyunlee: spines and grids Formatter type error solution 참고","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Python] List vs Tuple","slug":"Python/speed_list_tuple","date":"2021-04-05T15:00:00.000Z","updated":"2021-04-26T08:13:14.035Z","comments":true,"path":"2021/04/06/Python/speed_list_tuple/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/06/Python/speed_list_tuple/","excerpt":"","text":"List vs Tuple in python차이점 list는 mutable 즉, 생성된 후에 변경 가능하다.tuple은 immutable 즉, 생성된 후에 변경이 불가능하다. list는 dictionary의 key값으로 쓸 수 없지만 tuple은 가능하다.\\ why? key값은 불변한 값만 올 수 있기 때문!만약, tuple에 list가 들어있다면?=&gt; key 값으로 사용 불가능함! 참고) 문자열 또한 불변한 값이므로 dictionary의 key값으로 사용 가능하다. list 대신 tuple을 사용하는 이유는?메모리 크기 비교12345list_1 = [&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;]tuple_1 = &#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;import sysprint(sys.getsizeof(list_1))print(sys.getsizeof(tuple_1)) 104 88 tuple이 메모리를 차이하는 것이 더 작은 것을 볼 수 있다. 생성 속도12%timeit list_2=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;]%timeit tuple_2=&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27; 10000000 loops, best of 5: 57.5 ns per loop 100000000 loops, best of 5: 16.4 ns per loop tuple의 생성속도가 더 빠른 것을 확인할 수 있다. 인덱싱 속도12345list_3=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;]%timeit list_3[0]tuple_3=&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;,&#x27;D&#x27;%timeit tuple_3[0] 10000000 loops, best of 5: 43 ns per loop 10000000 loops, best of 5: 42.8 ns per loop tuple이 list보다 indexing 으로 데이터에 접근하는 속도가 더 빠르다. 결과 메모리 크기비교, 생성속도, 인덱싱 속도에서 모두 tuple의 결과가 더 빠른것을 확인 할 수 있었다. 출처 https://codacoding.tistory.com/36 https://itholic.github.io/python-list-tuple/","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Python]시각화 블로그 연습","slug":"Python/Visualization_Python/hexo_python","date":"2021-04-05T15:00:00.000Z","updated":"2021-04-26T08:13:14.030Z","comments":true,"path":"2021/04/06/Python/Visualization_Python/hexo_python/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/06/Python/Visualization_Python/hexo_python/","excerpt":"","text":"파이썬 시각화 블로그 연습1234567891011121314import matplotlib.pyplot as pltdates = [ &#x27;2021-01-01&#x27;, &#x27;2021-01-02&#x27;, &#x27;2021-01-03&#x27;, &#x27;2021-01-04&#x27;, &#x27;2021-01-05&#x27;, &#x27;2021-01-06&#x27;, &#x27;2021-01-07&#x27;, &#x27;2021-01-08&#x27;, &#x27;2021-01-09&#x27;, &#x27;2021-01-10&#x27;]min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0]fig,axes = plt.subplots(nrows=1, ncols=1, figsize=(10,6))axes.plot(dates, min_temperature, label = &#x27;Min Temperature&#x27;)axes.plot(dates, max_temperature, label = &#x27;Max Temperature&#x27;)axes.legend()plt.show()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"}]},{"title":"[Statistic]기초 통계-상관분석","slug":"Python/statistic","date":"2021-04-05T06:08:52.000Z","updated":"2021-04-26T08:13:14.036Z","comments":true,"path":"2021/04/05/Python/statistic/","link":"","permalink":"https://eunjin-jun717.github.io/2021/04/05/Python/statistic/","excerpt":"","text":"상관분석과 로지스틱회귀분석상관분석개념(정의) 연속 변수로 측정된 두 변수간의 선형 관계를 분석하는 기법 X가 증가함에 따라 B도 증가되는지 혹은 감소하는지를 분석기본 가정사항 두 변수 중 적어도 하나의 변수는 정규분포일 것 정규성 검사는 “shapiro.test()” 사용 만약, 두 변수 모두 정규성을 만족하지 못한다면? Spearman, Kendall 상관계수 사용! 정규성 검정에서 정규분포를 따르지 않거나 표본의 개수가 10개 미만일 때 사용 연속형 두 변수 간에는 선형적인 관계가 있어야함 분석을 실시하기 전, 반드시 두 변수의 산점도를 통해 확인해야함 공분산(Corvariance) 2개의 확률 변수의 상관정도를 나타내는 값 만약 하나의 값이 상승하는 경향을 보이면서 다른 값도 상승하면, 이때 공분산의 값은 양수! 하나의 값이 상승하는데 반대로 다른 값은 하강하면, 이때 공분산 값은 음수! 공분산 값만으로는 상승 or 하강의 경향을 알 수는 있으나 어느정도의 상관관계인지는 파악할 수 없음 따라서 공분산을 표준화 시킨 “상관계수”를 통해 파악하자! 상관관계 &amp; 상관계수(Correlation Coefficient) 상관관계 두 변수의 선형적인 관계 정도를 나타냄 일반적으로, 피어슨 상관계수를 의미 상관계수 피어슨 상관계수(Pearson) 상관계수(r)의 값: -1.0 &lt;= r &lt;= -0.7 (강한 음적 선형관계)-0.7&lt;= r &lt;= -0.3 (뚜렷한 음적 선형관계)-0.3&lt;= r &lt;= -0.1 (약한 음적 선형관계)-0.1&lt;= r &lt;= +0.1 (거의 무시될 수 있는 선형관계)+0.1&lt;= r &lt;= +0.3 (약한 양적 선형관계)+0.3&lt;= r &lt;= +0.73 (뚜렷한 양적 선형관계)+0.7&lt;= r &lt;= +1.0 (강한 양적 선형관계) 로지스틱 회귀분석로지스틱 회귀분석이 필요한 이유는?GLM(Generalized Linear Model) 비선형을 선형적으로 일반화 시킨 모형 이유는? 선형모델에서만 사용할 수 있는 모형의 해석, 확장, 수정 등을 이용하기 위해! 선형화 시키는 대상은? 관심 범주에 속할 확률 알코올 섭취량과 비만일 확률은 높아지지만 완전한 선형이라기 보다는 약간의 커브가 존재한다. 여기에 일반 선형회귀 라인을 넣으면 위와 같이 선형회귀선이 얼추 맞는다고 생각이 들 수 있다. 그렇지만 이 경우에는 음의 값이나 1을 초과하는 예측값을 제시할 수도 있는 가능성이 있다. 이러한 구조적 문제로 추가분석이 불가능하게 된다 그렇다면 비선형적인 모형을 넣어보면 어떨까? 훨씬 데이터에 잘 적용되는 것을 알 수 있다. 그렇지만 비선형모델은 여러 가지 추가 분석 제약이 있다! 따라서 이런 제약을 극복하기 위해 이를 선형화하는 것이 바로!! 로지스틱 회귀분석!! 이러한 특징을 만족하는 함수가 “시그모이드 함수”이다. 시그모이드 함수(로지스틱 반응함수)를 Odds에 넣는다. 확률p의 범위(0,1) -&gt; Odds(p)의 범위는(0,∞) 이다. Odds가 클수록 데이터가 해당 집단에 속할 확률이 큼 Odds에 log를 취하는데 이유는 선형으로 만들기 위해! 입력값(독립변수)의 범위가 –무한대에서 +무한대일 때, 출력값(종속변수)의 범위를 0에서 1로 변환시켜줌 X1, x2,…,xq가 어느 값을 가지더라도 항상 0과 1의 값을 가지게 된다. 1) 정의: 분류모델(기법) 일반적인 회귀분석과 동일하게 종속변수와 독립변수간의 관계를 함수로 나타내어 예측모델로 사용 특정기준(정답)에 의해 분석 대상을 특정개수의 집단으로 분류하는 예측모형 학습된 모델을 통해, 입력된 값을 미리 정해진 결과를 분류해주는 모델 이항형 로지스틱 회귀(Binary Logistic Regression): 2개의 범주Ex) 주택소유(있다/없다) 다항형 로지스틱 회귀(Multinomial Logistic Regression): 3개 이상의 범주Ex) 소유주택현황(아파트/단독주택/연립) 2) Odds(승산비) Odds란? 예측변수 1단위 변경 이후의 승산/ 원래 승산= 데이터가 어떤 집단에 속할 확률/ 속하지 않을 확률\\ 예를 들면, 나이의 Odds비가 0.969라면 나이가 한 단위 증가할수록 사망률이 0.969배 증가한다는 의미이다. 다시 말해, 생존율이 0.041배 감소한다. 3) 최대우도법 검정(Likelihood Ratio Test) 일반적인 회귀분석과 다르게 최대우도법 검정으로 추정을 한다!! 선형 회귀분석: 최소제곱법 로지스틱 회귀분석: 최대우도법 원하는 값이 나올 확률을 최대로 만드는 모수를 선택하는 방법 즉, 주어진 현상이 있을 때, 이 현상이 추출될 가능성을 가장 높게한다. 4) 이탈도(Deviance) 모형의 적합도의 측도 두 모형의 로그가능도 함수 값의 차이가 유의한지 보는 것 이때 포화모형이란 추정해야할 모수의 수가 데이터의 수와 동일한 상태 이탈도가 크면 그 모형은 적합하지 않다는 뜻 만약, p-value(&gt;0.05)이 클 때 모형 M이 의미있다는 뜻 5) AIC 이탈도와 다르게 로그가능도 함수 값과 모형에 사용된 모수의 수를 동시에 고려 로그가능도함수 값이 높으면 가산점을 준다!사용된 모수의 수가 많으면 패널티를 준다!따라서 AIC가 작을수록 바람직한 모형 Ex) 2개의 모형 모수를 이용한 모형의 로그가능도함수 값(LL)이 -16,4개의 모수만을 이용한 모형의 로그가능도함수 값(LL)이 -15 일 때,각 모형의 AIC =&gt; 36, 38 이다.비록 두번째 모형의 로그가능도함수 값이 -15로 더 컸지만,모형에 사용된 모수의 개수가 첫번째 모형보다 2개가 더 사용되었기 때문에 바람직하지 않다고 볼 수 있다. 6) 추가 Residual deviance(잔차이탈도) : 작을수록 좋고, 카이제곱분포를 따르기 때문에 카이제곱 적합도 검정을 통해 모형이 적합한지 확인가능 null deviance(0이탈도) : 아무런 변수 없이 상수항만 있을 때의 이탈도, 데이터가 전혀없는 최악의 상황 staturated model(포화모형) : 추정해야할 모수의 수가 데이터의 수와 동일한 상태 즉, 데이터가 10000개이면 추정할 모수의 수도 10000개 Null model(영 모델): 절편항만 가지는 모형으로 추정할 모수가 1개임 Proposed Model(제안모형): p개의 모수+ 절편항을 포함하는 모형으로, 추정할 모수가 p+1 개임. 참조 상관분석 상관분석 R 로지스틱회귀 이탈도 로그가능도, 이탈도","categories":[],"tags":[{"name":"statistic","slug":"statistic","permalink":"https://eunjin-jun717.github.io/tags/statistic/"}]},{"title":"[R] Encoding error in R","slug":"TroubleShooting/error2","date":"2021-03-30T12:06:12.000Z","updated":"2021-04-26T08:13:14.049Z","comments":true,"path":"2021/03/30/TroubleShooting/error2/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/30/TroubleShooting/error2/","excerpt":"","text":"오류 상황 12Not all characters in C:&#x2F;Users&#x2F;study&#x2F;Desktop&#x2F;analysis&#x2F;logistic_hd.R could be decoded using CP949. To try a different encoding, choose &quot;File | Reopen with Encoding...&quot; from the main menu. R 파일을 여는데 위와 같은 오류문구가 발생하면서 글자가 거의 모두 깨져있었다. 해결방안 File -&gt; Reopen with encoding -&gt; UTF-8로 설정 설정을 완료하면 아래와 같이 해결된 것을 볼 수 있다. 오류가 난 이유 Mac OS나 Linux에서의 UTF-8로 인코딩되어 있는 상태에서 자료가 Windows로 넘어올 때 글자가 깨지는 현상이 발생할 수도 있다. 참조 https://nittaku.tistory.com/341","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://eunjin-jun717.github.io/tags/hexo/"}]},{"title":"[Hexo] troubleshooting in Hexo","slug":"TroubleShooting/error","date":"2021-03-29T12:23:23.000Z","updated":"2021-04-26T08:13:14.048Z","comments":true,"path":"2021/03/29/TroubleShooting/error/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/29/TroubleShooting/error/","excerpt":"","text":"오류 상황: npm을 통해서 설치를 완료하였는데도 아래와 같은 오류가 발생하였다. 1bash: hexo: command not found 해결 방안 우선 node와 npm이 제대로 설치되었는지 확인할 것 12$ node -v$ npm -v 정상적으로 설치되어 있다면, 다음 step을 따를 것step 1) 자신의 blog 폴더 (ex. myblog) -&gt; node_modules -&gt; .bin경로를 복사함 step 2) 시스템 속성 -&gt; 고급 탭 -&gt; 환경 변수 step 3) 시스템 변수에서 Path 클릭 -&gt; 편집 클릭 -&gt; 새로만들기 -&gt; 경로 붙여넣기 -&gt; 확인 step 4) 다시 열어서 hexo server로 테스트해보기 123$ hexo serverINFO Hexo is running at http:&#x2F;&#x2F;localhost:4000 . Press Ctrl+C to stop. bash: hexo: command not found가 나오지 않는다면 해결!! 참조 https://www.programmersought.com/article/45443350618/","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://eunjin-jun717.github.io/tags/hexo/"}]},{"title":"[R] apply함수와 for문의 속도 비교","slug":"R/for_apply","date":"2021-03-27T15:00:00.000Z","updated":"2021-04-26T08:13:14.047Z","comments":true,"path":"2021/03/28/R/for_apply/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/28/R/for_apply/","excerpt":"","text":"개념 정리apply 함수 apply(X, MARGIN, FUN, …) X: 배열, 매트릭스 Margin: 행(1), 열(2) Fun: 함수 12my.matrx &lt;- matrix(c(1:10, 11:20, 21:30), nrow = 10, ncol=3) # 행이 10개, 열이 3개인 매트릭스 생성my.matrx 1234567891011## [,1] [,2] [,3]## [1,] 1 11 21## [2,] 2 12 22## [3,] 3 13 23## [4,] 4 14 24## [5,] 5 15 25## [6,] 6 16 26## [7,] 7 17 27## [8,] 8 18 28## [9,] 9 19 29## [10,] 10 20 30 1apply(my.matrx, 1, sum) # 매트릭스를 행단위 합 계산 1## [1] 33 36 39 42 45 48 51 54 57 60 1apply(my.matrx, 2, sum) # 열단위 합 계산 1## [1] 55 155 255 1apply(my.matrx, 2, function(x) length(x)) # 직접 함수를 정의해서 사용 가능 1## [1] 10 10 10 lapply 함수 lapply(X,FUN, …) X: 벡터, 리스트 반환값: 리스트12vec &lt;- c(1:10)vec 1## [1] 1 2 3 4 5 6 7 8 9 10 1lapply(vec, sum) #list 형태로 나옴 1234567891011121314151617181920212223242526272829## [[1]]## [1] 1## ## [[2]]## [1] 2## ## [[3]]## [1] 3## ## [[4]]## [1] 4## ## [[5]]## [1] 5## ## [[6]]## [1] 6## ## [[7]]## [1] 7## ## [[8]]## [1] 8## ## [[9]]## [1] 9## ## [[10]]## [1] 10 12345A &lt;- c(1:9)B &lt;- c(1:12)C &lt;- c(1:15)my.lst &lt;- list(A,B,C)my.lst 12345678## [[1]]## [1] 1 2 3 4 5 6 7 8 9## ## [[2]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12## ## [[3]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1lapply(my.lst, sum) # 각 list 안의 값들의 합 12345678## [[1]]## [1] 45## ## [[2]]## [1] 78## ## [[3]]## [1] 120 sapply 함수 sapply(X,FUN,…,simplify=TRUE, USE.NAMES=TRUE) lapply와 같은 동작을 하지만, 가능하면 출력을 단순화 시키는 함수 simplify=TRUE : 출력을 단순화시킴, simplify=FALSE : 단순화 시키지 않음 USE.NAMES=TRUE : 이름 속성도 반환, USE.NAMES=FALSE : 이름 속성 없이 반환 1vec 1## [1] 1 2 3 4 5 6 7 8 9 10 1sapply(vec, sum, simplify=FALSE) # 출력을 단순화 하지 않으므로 리스트 형태로 출력 1234567891011121314151617181920212223242526272829## [[1]]## [1] 1## ## [[2]]## [1] 2## ## [[3]]## [1] 3## ## [[4]]## [1] 4## ## [[5]]## [1] 5## ## [[6]]## [1] 6## ## [[7]]## [1] 7## ## [[8]]## [1] 8## ## [[9]]## [1] 9## ## [[10]]## [1] 10 1sapply(vec, sum, simplify=TRUE) # 출력을 단순화시킴 1## [1] 1 2 3 4 5 6 7 8 9 10 1my.lst 12345678## [[1]]## [1] 1 2 3 4 5 6 7 8 9## ## [[2]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12## ## [[3]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1sapply(my.lst, sum) # 출력을 단순화시킴 # simplify=TRUE 생략된 모습 1## [1] 45 78 120 1lapply(my.lst, sum) # 리스트를 반환 12345678## [[1]]## [1] 45## ## [[2]]## [1] 78## ## [[3]]## [1] 120 vapply 함수 vapply(X,FUN,FUN.VALUE,…,USE.NAMES=TRUE) sapply함수와 비슷함. 차이점: value로 예상되는 데이터 유형을 지정해야함 FUN.VALUE: 자료형 지정 1vec 1## [1] 1 2 3 4 5 6 7 8 9 10 1vapply(vec, sum, numeric(1)) # 1개의 숫자데이터로 나오게함 1## [1] 1 2 3 4 5 6 7 8 9 10 1my.lst 12345678## [[1]]## [1] 1 2 3 4 5 6 7 8 9## ## [[2]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12## ## [[3]]## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1vapply(my.lst, sum, numeric(1)) # 1개의 숫자데이터로 나오게함 1## [1] 45 78 120 12#vapply(my.lst, function(x) x, numeric(1)) # error 뜸#1개의 데이터만 들어갈 수 있는데 함수의 결과는 9개가 나오므로 error뜸 tapply 함수 tapply(X, INDEX, FUN=NULL, …, DEFAULT=NA,SIMPLIFY=TRUE) 그룹별로 처리함 factor형으로 인자를 줌 12tdata &lt;- as.data.frame(cbind(c(1,1,1,1,1,2,2,2,2,2), my.matrx)) # 열단위로 결합시킴tdata 1234567891011## V1 V2 V3 V4## 1 1 1 11 21## 2 1 2 12 22## 3 1 3 13 23## 4 1 4 14 24## 5 1 5 15 25## 6 2 6 16 26## 7 2 7 17 27## 8 2 8 18 28## 9 2 9 19 29## 10 2 10 20 30 1colnames(tdata) # tdata의 열 이름 1## [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; 1tapply(tdata$V2, tdata$V1, sum) # V1: index이며, V2가 함수의 인자로 전달됨 12## 1 2 ## 15 40 1# index가 1인 그룹으로 sum되고, index가 2인 그룹을 sum함 mapply 함수 mapply(FUN,…,MOREARGS=NULL, SIMPLIFY=TRUE, USE.NAMES=TRUE) sapply()와 유사하지만 mapply()는 다수의 인자를 함수에 넘길 수 있음 1mapply(rep, 1:10, 10:1) # 반복함수, 반복할 숫자, 반복되는 갯수 1234567891011121314151617181920212223242526272829## [[1]]## [1] 1 1 1 1 1 1 1 1 1 1## ## [[2]]## [1] 2 2 2 2 2 2 2 2 2## ## [[3]]## [1] 3 3 3 3 3 3 3 3## ## [[4]]## [1] 4 4 4 4 4 4 4## ## [[5]]## [1] 5 5 5 5 5 5## ## [[6]]## [1] 6 6 6 6 6## ## [[7]]## [1] 7 7 7 7## ## [[8]]## [1] 8 8 8## ## [[9]]## [1] 9 9## ## [[10]]## [1] 10 1tdata 1234567891011## V1 V2 V3 V4## 1 1 1 11 21## 2 1 2 12 22## 3 1 3 13 23## 4 1 4 14 24## 5 1 5 15 25## 6 2 6 16 26## 7 2 7 17 27## 8 2 8 18 28## 9 2 9 19 29## 10 2 10 20 30 12tdata$V5 &lt;- mapply(function(x,y) x*y, tdata$V1, tdata$V2) # V1과 V2의 값들에 대한 곱tdata 1234567891011## V1 V2 V3 V4 V5## 1 1 1 11 21 1## 2 1 2 12 22 2## 3 1 3 13 23 3## 4 1 4 14 24 4## 5 1 5 15 25 5## 6 2 6 16 26 12## 7 2 7 17 27 14## 8 2 8 18 28 16## 9 2 9 19 29 18## 10 2 10 20 30 20 For문, apply함수 속도 비교(1)For문 사용 시 속도123456789101112# 랜덤한 10000개의 숫자를 x1, x2에 저장N &lt;- 10000x1 &lt;- runif(N) # runif() : 랜덤숫자 발생함수x2 &lt;- runif(N)# x1과 x2를 열단위로 묶어서 d에 data frame 형태로 저장d &lt;- as.data.frame(cbind(x1, x2))# for loop으로 시간 체크system.time(for(i in c(1:length(d[,1])))&#123; # 1부터 &#x27;d의 1열 길이&#x27;만큼 for 문이 반복함 d$mean2[i] &lt;- mean(c(d[i,1], d[i,2])) # x1, x2 각각 더해 평균을 구한것을 d의 데이터 프레임에 넣음&#125;) 12## user system elapsed ## 0.80 0.05 0.84 For문은 1부터 10000까지 한 명이 순차적으로 일을 처리한다. apply 함수사용 시 속도12# apply 함수를 사용하여 같은 데이터를 처리해보자.system.time(d$mean1 &lt;- apply(d, 1, mean)) # d의 행에 대한 평균 12## user system elapsed ## 0.06 0.00 0.06 For문, apply함수 속도 비교(2)1install.packages(&#x27;nycflights13&#x27;, repos=&quot;http://cran.us.r-project.org&quot;) 1234## package &#39;nycflights13&#39; successfully unpacked and MD5 sums checked## ## The downloaded binary packages are in## C:\\Users\\study\\AppData\\Local\\Temp\\Rtmp08X2ug\\downloaded_packages 1install.packages(&#x27;dplyr&#x27;, repos=&quot;http://cran.us.r-project.org&quot;) 1234## package &#39;dplyr&#39; successfully unpacked and MD5 sums checked## ## The downloaded binary packages are in## C:\\Users\\study\\AppData\\Local\\Temp\\Rtmp08X2ug\\downloaded_packages 12library(dplyr)library(nycflights13) apply 함수사용 시 속도1head(flights) 123456789101112## # A tibble: 6 x 19## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;## 1 2013 1 1 517 515 2 830 819## 2 2013 1 1 533 529 4 850 830## 3 2013 1 1 542 540 2 923 850## 4 2013 1 1 544 545 -1 1004 1022## 5 2013 1 1 554 600 -6 812 837## 6 2013 1 1 554 558 -4 740 728## # ... with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 1234567891011flights_selected &lt;- select(flights, flight, distance) # flight number와 distance를 뽑음data_flight &lt;- tapply(flights_selected$distance, flights_selected$flight, sum) # 데이터 양이 많아 우선적으로 각각의 flight마다 간 거리를 합함.df &lt;- data.frame(flight_num= c(1:length(data_flight)), flight_distance= data_flight)# data frame 형태로 만듦 (인덱스 길이= flight 갯수)# flight_num와 distance의 평균을 구함, 의미는 없으며 속도차이를 보기 위함!## apply문 사용 시 속도 # 행단위로 평균을 구함system.time(df$mean3 &lt;- apply(df, 1, mean)) 12## user system elapsed ## 0.03 0.00 0.03 1234## For문 사용 시 속도system.time(for(i in c(1:length(data_flight)))&#123; df$mean2[i] &lt;- mean(c(df[i,1], df[i,2]))&#125;) 12## user system elapsed ## 0.28 0.02 0.29 apply함수 사용 시 장점 For문과 비교해봤을 때, apply함수 사용 시 속도가 빠음 대용량 데이터에 대해 짧은 코드로 반복 연산 처리 가능 참조 https://ademos.people.uic.edu/Chapter4.html#:~:text=Apply%20functions%20are%20a%20family,and%20often%20require%20less%20code. http://rstudio-pubs-static.s3.amazonaws.com/5526_83e42f97a07141e88b75f642dbae8b1b.html","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}]},{"title":"[R] 주택 가격: 시각화 예제","slug":"R/Visualization_R/house_miniproj_eunjin","date":"2021-03-24T15:00:00.000Z","updated":"2021-04-26T08:13:14.042Z","comments":true,"path":"2021/03/25/R/Visualization_R/house_miniproj_eunjin/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/25/R/Visualization_R/house_miniproj_eunjin/","excerpt":"","text":"참조kaggle: house prices_ project r chunk에서 message=FALSE를 하게 되면 패키지를 불러올때의 관련 메세지가 안보임 12345678910111213library(knitr) # 마크다운 관련 패키지library(ggplot2) # 시각화 패키지library(plyr) # 데이터 분석, 가공 패키지library(dplyr) # 데이터 가공 패키지library(corrplot) # 상관행렬과 신뢰구간의 그래프library(caret) # 전처리, 모형 시각화library(gridExtra) # 시각화 이미지 분할library(scales) # ggplot2에 의해 사용된 scaling 인프라 제공library(Rmisc) # 데이터 분석, 유틸리티 동작library(ggrepel) # 겹치는 텍스트 레이블 제거library(randomForest) # 분류와 회귀를 위해 Breiman의 random forest algorithm 구현library(psych) # 기술통계량 구해주는 패키지#library(xgboost) # 머신러닝 패키지 R로 train.csv파일과 test.csv파일을 부름 12train &lt;-read.csv(&quot;train.csv&quot;, stringsAsFactors = F)test &lt;- read.csv(&quot;test.csv&quot;, stringsAsFactors = F) 1dim(train) # train의 데이터프레임 길이 1## [1] 1460 81 1str(train[,c(1:10, 81)]) # 처음 10개의 변수와 response variable 123456789101112## &#39;data.frame&#39;: 1460 obs. of 11 variables:## $ Id : int 1 2 3 4 5 6 7 8 9 10 ...## $ MSSubClass : int 60 20 60 70 60 50 20 60 50 190 ...## $ MSZoning : chr &quot;RL&quot; &quot;RL&quot; &quot;RL&quot; &quot;RL&quot; ...## $ LotFrontage: int 65 80 68 60 84 85 75 NA 51 50 ...## $ LotArea : int 8450 9600 11250 9550 14260 14115 10084 10382 6120 7420 ...## $ Street : chr &quot;Pave&quot; &quot;Pave&quot; &quot;Pave&quot; &quot;Pave&quot; ...## $ Alley : chr NA NA NA NA ...## $ LotShape : chr &quot;Reg&quot; &quot;Reg&quot; &quot;IR1&quot; &quot;IR1&quot; ...## $ LandContour: chr &quot;Lvl&quot; &quot;Lvl&quot; &quot;Lvl&quot; &quot;Lvl&quot; ...## $ Utilities : chr &quot;AllPub&quot; &quot;AllPub&quot; &quot;AllPub&quot; &quot;AllPub&quot; ...## $ SalePrice : int 208500 181500 223500 140000 250000 143000 307000 200000 129900 118000 ... 123test_labels &lt;- test$Id # 테스트 Id는 test_labels에 저장test$Id &lt;- NULLtrain$Id &lt;- NULL 123test$SalePrice &lt;- NA # Not Available 결측치, NULL은 출력되지 않지만 NA는 출력됨all &lt;- rbind(train, test) # train과 test 결합dim(all) # 결합된 것을 all이라하며 데이터 프레임 길이 구함 1## [1] 2919 80 1#Id가 없으므로 (데이터프레임 = 79개의 예측변수들 + SalePrice 인 response 변수) 123456ggplot(data = all[!is.na(all$SalePrice),], #SalePrice에 결측값인 NA가 포함되어있는지 확인함 # 이때 결측값이 아닌 SalePrice 만 선택 aes(x= SalePrice))+ # x축 : SalePrice geom_histogram(fill=&quot;blue&quot;, binwidth = 10000)+ # 히스토그램으로 표현, bar은 blue, bar의 두께는 10000으로 설정 scale_x_continuous(breaks=seq(0, 800000, by=100000), # x축의 범위는 0~800000, 100000단위로 끊어줌 labels= comma) # 숫자 3자리마다 &#x27;,&#x27; 넣음 히스토그램을 보면 좌측으로 치우쳐져 있다.이 말은 SalePrice가 낮은 집이 잘 팔린다는 뜻이고, SalePrice가 높은 집은 사는 사람이 거의 없다는 것을 의미한다. 1summary(all$SalePrice) 12## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 34900 129975 163000 180921 214000 755000 1459 SalePrice의 최저가격: 34,900, 최고가격: 755,000,1사 분위수(1st Qu): 129,975 =&gt; 오름차순으로 정렬했을 때, 하위 25%의 SalePrice중앙값(Median): 163,000 =&gt; 오름차순으로 정렬했을 때, 중앙에 있는 값인 SalePrice평균값(Mean): 180,921 =&gt; SalePrice의 평균1사 분위수(3rd Qu): 214,000 =&gt; 오름차순으로 정렬했을 때, 상위 25%의 SalePriceNA’s : 결측값 123numericVars &lt;- which(sapply(all, is.numeric)) # numeric 변수들의 위치를 찾는데 그 결과를 벡터 또는 행렬로 반환numericVarNames &lt;- names(numericVars) # 저장된 numeric 변수들인 numericVars로 변수명 변경된 것을 Name벡터인 numericVarNames로 저장cat(&#x27;There are&#x27;, length(numericVars), &#x27;numeric variables&#x27;) #cat함수는 행을 바꾸지 않음 1## There are 37 numeric variables 123456789101112131415#numericVars의 갯수를 출력함all_numVar &lt;- all[, numericVars] # numericVars의 데이터들을 &#x27;all_numVar&#x27;에 저장 (numericVars는 그냥 벡터일뿐, 데이터프레임이 아님)cor_numVar &lt;- cor(all_numVar, use=&quot;pairwise.complete.obs&quot;) # all_numVar들의 상관관계를 저장# use=&quot;pairwise.complete.obs&quot; =&gt; 결측치가 포함된 데이터에서 상관관계를 구하기 위해 사용cor_sorted &lt;- as.matrix(sort(cor_numVar[, &#x27;SalePrice&#x27;], decreasing = TRUE)) # SalePrice와의 상관관계만을 내림차순으로 정렬한 뒤 행렬로 변환한 것을 cor_sorted 행렬에 저장CorHigh &lt;- names(which(apply(cor_sorted, 1, function(x) abs(x)&gt;0.5))) # 1:행, 2:열, function(x) &#123; abs (x)&#125;# 행단위로 SalePrice의 상관관계를 절댓값 형태로 취한 뒤 0.5 이상의 값만 추출함# 추출된 것들의 변수명을 변경하고 CorHigh에 저장 =&gt; 0.5이상의 상관관계는 &quot;관련이 높다&quot;라는 의미cor_numVar &lt;- cor_numVar[CorHigh, CorHigh] # 0.5이상의 상관관계를 가진 값들로만 cor_numVar에 다시 저장# tl.col(text legend color은 black), tl.pos(text legend position은 left와 top)# corrplot.mixed 함수: 시각화 방법을 혼합할 때 사용corrplot.mixed(cor_numVar, tl.col= &quot;black&quot;, tl.pos=&quot;lt&quot;) #tl: text legend, cl: color legend 상관관계가 0.5이상인 데이터들을 봤을 때, SalePrice와 가장 높은 상관관계를 가지는 것은 “OverallQual”인 전반적인 품질이었다. =&gt; 0.791 독립변수들 간에 강한 상관관계가 나타나는 문제를 multicollinearity (다중공선성)라고 하는데 이것이 문제인 것으로 보인다. SalePrice와 가장 상관관계가 높은 OverallQual를 제외하고 다음으로 높은 GrLivArea 와 GarageCars 사이의 상관관계를 보았을 때 매우 높은 0.8897 이다. 즉, 독립변수들 간의 강한 상관관계를 나타낸다. 상관관계가 0.5 이상인 높은 상관관계를 가진 변수는 나머지 6개가 있다.TotalBsmtSF, 1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt, YearRemodAdd","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}],"author":"Eunjin"},{"title":"R코딩 기초함수","slug":"R/R-edu2","date":"2021-03-23T13:53:23.000Z","updated":"2021-03-26T08:00:44.237Z","comments":true,"path":"2021/03/23/R/R-edu2/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/23/R/R-edu2/","excerpt":"","text":"Visualization Practice using “counties”파일 불러오기 get working directory 라는 뜻으로 현재 위치를 알려준다.1getwd() set as working directory 라는 뜻으로 위치를 새로 설정할 수 있다.1setwd(&#x27;R_NCS_2020/1_day/&#x27;) dplyr 설치 방법 2가지 method1) tidyverse 설치 12install.packages(&quot;tidyverse&quot;)library(tidyverse) method2) dplyr만 설치 12install.packages(&quot;dplyr&quot;)library(dplyr) 데이터 가져오기 data 폴더에 있는 counties.xlsx 파일을 가져온다.만약 경로 error가 뜨면 getwd()를 하여 현재 파일 위치가 잘못되어있는지 확인해본다. 1counties &lt;- readxl::read_xlsx(&quot;data/counties.xslx&quot;, sheet = 1) 데이터 확인 glimpse 를 사용하여 counties에 무슨 변수들이 있는지 살펴볼 수 있다. 1glimpse(counties) Select 사용하여 원하는 변수들 추출 counties 변수들 중에서 state, region, men, women, population 변수들의 데이터만 고른다.select한 것을 counties_selected라는 변수 이름으로 저장한다. counties_selected 를 state 기준으로 내림차순 정렬한다. 12345counties_selected &lt;- counties %&gt;% select(state, region, men, women, population) counties_selected %&gt;% arrange(desc(state)) Filter 사용하여 불필요한 것 제거하고 보기 population이 10000명 이하인 것들만 보여준다.12counties_selected %&gt;% filter(population &lt; 10000) 조건이 두가지일 경우에는 아래와 같이 한다. state가 California이거나 population이 100000명 이상인 것을 추출한다. 12counties_selected %&gt;% filter(state == &quot;California&quot; | population &gt; 100000) Arange 함수 오름차순, 내림차순으로 정렬 가능하다.123counties_selected %&gt;% filter(state == &quot;California&quot; | population &gt; 100000) %&gt;% arrange(desc(public_work)) Mutate 함수 새로 변수를 추가한다는 것 보다는 의미있는 데이터 발견하고자 할 때 사용한다. public workers를 구해서 배열하는 코드이다. 123456counties_seletec &lt;- counties %&gt;% select(state, county, private_work, public_work, population)counties_seleteced %&gt;% mutate(public_workers = population * private_work /100) %&gt;% arrange(public_workers) Count 함수 각 행의 갯수를 셀 수 있고, 정렬까지 가능하다. sort=TRUE이면 자동으로 내림차순12counties_selected %&gt;% count(state, sort =TRUE) 가중치를 두어 주별로 걸어서 출퇴근하는 사람의 수를 카운트할 수 있다. 12counties_selected %&gt;% count(state, wt= walkers_pop) 참조 https://github.com/dschloe/R_edu","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}]},{"title":"R코딩 기초1","slug":"R/R-edu1","date":"2021-03-23T13:06:56.000Z","updated":"2021-03-26T08:00:44.235Z","comments":true,"path":"2021/03/23/R/R-edu1/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/23/R/R-edu1/","excerpt":"","text":"R 코딩 기초실행방법 Windows: Ctrl + Enter Mac: Command + Enter 변수 저장 시1a &lt;- 3 Alt + - key 누르면 된다. 변수 호출 시123print(x) #혹은x 변수 종류변수 type 숫자형 변수 1my_numeric &lt;- 17 문자형 변수 1my_character &lt;- &quot;eunjin&quot; 논리형 변수 12my_logical &lt;- FALSE my_logical &lt;- TRUE 벡터 생성12number_vector &lt;- c(1,2,3)number_vertor &lt;- c(1,&quot;2&quot;,3) 첫번째 라인은 숫자로만 이루어져있기 때문에 변수 type을 알아보면 “numeric” 으로 나온다. 두번째 라인은 숫자, 문자가 섞여져 있다.이때 컴퓨터는 문자형 -&gt; 숫자형 -&gt; 논리형 으로 저장된다.따라서 두번째 라인의 변수 type은 문자형이 나온다. =&gt; “character” 1class(number_vector) 변수 type을 알아볼때는 class()를 사용한다. 범주형 변수의 순서 Levels: 낮음 높음123height_vector &lt;- c(&quot;낮음&quot;, &quot;높음&quot;, &quot;낮음&quot;)factor_height_vector &lt;- factor(height_vector)factor_height_vector Levels 순서 바꾸기 =&gt; 높음 낮음12levels(factor_sex_vector) &lt;- c(&quot;남성&quot;, &quot;여성&quot;)factor_sex_vector Levels 이름 변경 =&gt; Levels: 매우높음 매우낮음1234factor_height_vector &lt;- factor(factor_height_vector, levels = c(&quot;매우높음&quot;, &quot;매우낮음&quot;))factor_height_vector 참조 https://github.com/dschloe/R_edu","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}]},{"title":"[Hexo] What is the main and branch in Github","slug":"Hexo/meaning","date":"2021-03-23T12:40:58.000Z","updated":"2021-04-26T08:13:14.029Z","comments":true,"path":"2021/03/23/Hexo/meaning/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/23/Hexo/meaning/","excerpt":"","text":"Main 과 Branch 그림에서 보이는 main과 branch의 역할이 무엇인지 궁금하여 찾아보았다. Branch란? 여러 개발자들이 동시에 다양한 작업을 할 수 있게 만들어 주는 기능 각자 독립적인 저장소안에서 소스코드를 원하는대로 작업할 수 있다. 분리된 작업들을 나중에 하나의 새로운 버전으로 만들어낼 수 있다. 장점: 여러 작업들을 동시에 진행할 수 있다. 문제가 발생했을 경우 원인이 되는 작업을 찾아내 대책을 세우기 쉽다.Main이란? default branch 참조https://backlog.com/git-tutorial/kr/stepup/stepup1_1.html","categories":[],"tags":[{"name":"github","slug":"github","permalink":"https://eunjin-jun717.github.io/tags/github/"}]},{"title":"[R]Simple visualization","slug":"R/Visualization_R/visualization","date":"2021-03-22T13:09:02.000Z","updated":"2021-04-26T08:13:14.044Z","comments":true,"path":"2021/03/22/R/Visualization_R/visualization/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/22/R/Visualization_R/visualization/","excerpt":"","text":"간단하게 시각화 하는 과정 step1) 패키지 설치1install.packages(&quot;ggplot2&quot;) step2) 패키지 불러오기1library(ggplot2) step3) 데이터 불러오기1data(&quot;iris&quot;) step4) 데이터 확인하기1str(iris) step5) 가공되지 않은 Raw data 가공하기 step6) 시각화하기12345ggplot(data=iris, mapping = aes(x = Petal.Length, y = Petal.Width, colour = Species)) + geom_point(size=3) *tap 누르면 자동입력기능 있음.*help에서 ggplot sample 확인 가능. 참조 https://github.com/dschloe/R_edu","categories":[],"tags":[{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"}]},{"title":"[Hexo]How to create a blog with Hexo","slug":"R/firstday","date":"2021-03-22T12:38:36.000Z","updated":"2021-04-26T08:13:14.045Z","comments":true,"path":"2021/03/22/R/firstday/","link":"","permalink":"https://eunjin-jun717.github.io/2021/03/22/R/firstday/","excerpt":"","text":"github에 2개의 Repository 생성 포스트 버전관리: user_name 포스트 배포용 관리: user_name.github.io Blog 초기화$ hexo init myblog Hexo 모듈 설치$ npm install -g hexo-cli myblog로 들어간 뒤 아래와 같이 입력$ cd myblog$ npm install$ npm install hexo-server –save$ npm install hexo-deployer-git –save 참고로, 복사+붙여넣기 하면 오류를 줄일 수 있음 Local server로 테스트$ hexo server Pycharm pycharm을 열어서 myblog 폴더를 연다. config.yml 파일을 열어 title, author을 수정한다.title: Eunjin’s Blogauthor: Eunjin Jun URL도 아래와 같이 수정한다. url: https://user_name.github.io Deployment의 deploy를 아래와 같이 입력한다.deploy:type: gitrepo: https://github.com/user_name/user_name.github.io.gitbranch: main Hexo generate &amp; deploy 활성화시킨 뒤 배포한다.$ hexo generate$ hexo deploy 한꺼번에 명령을 할 수도 있다.$ hexo deploy –generate Themes 설치ex) icarus 설치 icarus 설치$ npm install hexo-theme-icarus config.yml의 Extensions의 theme을 icarus로 변경theme: icarus $ hexo server이때, Error에서 뜨는 설명대로 그대로 복사한 뒤 붙여넣기하여 설치하기$ npm install –save &#98;&#x75;&#x6c;&#x6d;&#x61;&#x2d;&#x73;&#116;&#121;&#108;&#117;&#x73;&#x40;&#48;&#46;&#x38;&#46;&#x30; hexo-renderer-inferno@^0.1.3 다시 local server 테스트$ hexo server$ hexo deploy –generate 참조 https://dschloe.github.io/settings/hexo_blog/","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://eunjin-jun717.github.io/tags/hexo/"}]}],"categories":[],"tags":[{"name":"ML","slug":"ML","permalink":"https://eunjin-jun717.github.io/tags/ML/"},{"name":"python","slug":"python","permalink":"https://eunjin-jun717.github.io/tags/python/"},{"name":"statistic","slug":"statistic","permalink":"https://eunjin-jun717.github.io/tags/statistic/"},{"name":"hexo","slug":"hexo","permalink":"https://eunjin-jun717.github.io/tags/hexo/"},{"name":"r","slug":"r","permalink":"https://eunjin-jun717.github.io/tags/r/"},{"name":"github","slug":"github","permalink":"https://eunjin-jun717.github.io/tags/github/"}]}